{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CleanRL - Overview CleanRL is a Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features. The implementation is clean and simple, yet we can scale it to run thousands of experiments using AWS Batch. The highlight features of CleanRL are: \ud83d\udcdc Single-file implementation Every detail about an algorithm variant is put into a single standalone file. For example, our ppo_atari.py only has 340 lines of code but contains all implementation details on how PPO works with Atari games, so it is a great reference implementation to read for folks who do not wish to read an entire modular library . \ud83d\udcca Benchmarked Implementation (7+ algorithms and 34+ games at https://benchmark.cleanrl.dev) \ud83d\udcc8 Tensorboard Logging \ud83e\ude9b Local Reproducibility via Seeding \ud83c\udfae Videos of Gameplay Capturing \ud83e\uddeb Experiment Management with Weights and Biases \ud83d\udcb8 Cloud Integration with docker and AWS You can read more about CleanRL in our technical paper and documentation . Good luck have fun \u26a0\ufe0f NOTE : CleanRL is not a modular library and therefore it is not meant to be imported. At the cost of duplicate code, we make all implementation details of a DRL algorithm variant easy to understand, so CleanRL comes with its own pros and cons. You should consider using CleanRL if you want to 1) understand all implementation details of an algorithm's varaint or 2) prototype advanced features that other modular DRL libraries do not support (CleanRL has minimal lines of code so it gives you great debugging experience and you don't have do a lot of subclassing like sometimes in modular DRL libraries). Citing CleanRL If you use CleanRL in your work, please cite our technical paper : @article { huang2021cleanrl , title = {CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms} , author = {Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga} , year = {2021} , eprint = {2111.08819} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"Overview"},{"location":"#cleanrl-overview","text":"CleanRL is a Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features. The implementation is clean and simple, yet we can scale it to run thousands of experiments using AWS Batch. The highlight features of CleanRL are: \ud83d\udcdc Single-file implementation Every detail about an algorithm variant is put into a single standalone file. For example, our ppo_atari.py only has 340 lines of code but contains all implementation details on how PPO works with Atari games, so it is a great reference implementation to read for folks who do not wish to read an entire modular library . \ud83d\udcca Benchmarked Implementation (7+ algorithms and 34+ games at https://benchmark.cleanrl.dev) \ud83d\udcc8 Tensorboard Logging \ud83e\ude9b Local Reproducibility via Seeding \ud83c\udfae Videos of Gameplay Capturing \ud83e\uddeb Experiment Management with Weights and Biases \ud83d\udcb8 Cloud Integration with docker and AWS You can read more about CleanRL in our technical paper and documentation . Good luck have fun \u26a0\ufe0f NOTE : CleanRL is not a modular library and therefore it is not meant to be imported. At the cost of duplicate code, we make all implementation details of a DRL algorithm variant easy to understand, so CleanRL comes with its own pros and cons. You should consider using CleanRL if you want to 1) understand all implementation details of an algorithm's varaint or 2) prototype advanced features that other modular DRL libraries do not support (CleanRL has minimal lines of code so it gives you great debugging experience and you don't have do a lot of subclassing like sometimes in modular DRL libraries).","title":"CleanRL - Overview"},{"location":"#citing-cleanrl","text":"If you use CleanRL in your work, please cite our technical paper : @article { huang2021cleanrl , title = {CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms} , author = {Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga} , year = {2021} , eprint = {2111.08819} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"Citing CleanRL"},{"location":"contribution/","text":"Contributing to CleanRL \ud83d\udc4d\ud83c\udf89 Thank you for taking the time to contribute! \ud83c\udf89\ud83d\udc4d Feel free to open an issue or a Pull Request if you have any questions or suggestions. You can also join our Discord and ask questions there. If you plan to work on an issue, let us know in the issue thread to avoid duplicate work. Good luck and have fun! Dev Setup poetry install poetry install -E atari poetry install -E pybullet Then you can run the scripts under the poetry environment in two ways: poetry run or poetry shell . poetry run : By prefixing poetry run , your command will run in poetry's virtual environment. For example, try running poetry run python ppo.py poetry shell : First, activate the poetry's virtual environment by executing poetry shell . Then, the name of the poetry's virtual environment (e.g. (cleanrl-ghSZGHE3-py3.9) ) should appear in the left side of your shell. Afterwards, you can directly run ( cleanrl-ghSZGHE3-py3.9 ) python ppo.py Pre-commit utilities We use pre-commit to helps us automate a sequence of short tasks (called pre-commit \"hooks\") such as code formatting. In particular, we always use the following hooks when submitting code to the main repository. pyupgrade : pyupgrade upgrades syntax for newer versions of the language. isort : isort sorts imported dependencies according to their type (e.g, standard library vs third-party library) and name. black : black enforces an uniform code style across the codebase. autoflake : autoflake helps remove unused imports and variables. codespell : codespell helps avoid common incorrect spelling. You can run the following command to run the following hooks: poetry run pre-commit run --all-files which in most cases should automatically fix things as shown below: Contributing new algorithms We welcome the contributions of new algorithms. Before opening a pull request , please open an issue first to discuss with us since this is likely a sizable effort. Once we agree on the plan, feel free to make a PR to include the new algorithm. To help ease the review process, here is a checklist template when contributing a new algorithm. See https://github.com/vwxyzjn/cleanrl/pull/137 as an example. [ ] I've read the CONTRIBUTION guide ( required ). [ ] I have ensured pre-commit run --all-files passes ( required ). [ ] I have contacted @vwxyzjn to obtain access to the openrlbenchmark W&B team ( required ). [ ] I have tracked applicable experiments in openrlbenchmark/cleanrl with --capture-video flag toggled on ( required ). [ ] I have updated the documentation and previewed the changes via mkdocs serve . [ ] I have explained note-worthy implementation details. [ ] I have explained the logged metrics. [ ] I have added links to the original paper and related papers (if applicable). [ ] I have added links to the PR related to the algorithm. [ ] I have created a table comparing my results against those from reputable sources (i.e., the original paper or other reference implementation). [ ] I have added the learning curves (in PNG format with width=500 and height=300 ). [ ] I have added links to the tracked experiments. [ ] I have updated the tests accordingly (if applicable).","title":"Contribution"},{"location":"contribution/#contributing-to-cleanrl","text":"\ud83d\udc4d\ud83c\udf89 Thank you for taking the time to contribute! \ud83c\udf89\ud83d\udc4d Feel free to open an issue or a Pull Request if you have any questions or suggestions. You can also join our Discord and ask questions there. If you plan to work on an issue, let us know in the issue thread to avoid duplicate work. Good luck and have fun!","title":"Contributing to CleanRL"},{"location":"contribution/#dev-setup","text":"poetry install poetry install -E atari poetry install -E pybullet Then you can run the scripts under the poetry environment in two ways: poetry run or poetry shell . poetry run : By prefixing poetry run , your command will run in poetry's virtual environment. For example, try running poetry run python ppo.py poetry shell : First, activate the poetry's virtual environment by executing poetry shell . Then, the name of the poetry's virtual environment (e.g. (cleanrl-ghSZGHE3-py3.9) ) should appear in the left side of your shell. Afterwards, you can directly run ( cleanrl-ghSZGHE3-py3.9 ) python ppo.py","title":"Dev Setup"},{"location":"contribution/#pre-commit-utilities","text":"We use pre-commit to helps us automate a sequence of short tasks (called pre-commit \"hooks\") such as code formatting. In particular, we always use the following hooks when submitting code to the main repository. pyupgrade : pyupgrade upgrades syntax for newer versions of the language. isort : isort sorts imported dependencies according to their type (e.g, standard library vs third-party library) and name. black : black enforces an uniform code style across the codebase. autoflake : autoflake helps remove unused imports and variables. codespell : codespell helps avoid common incorrect spelling. You can run the following command to run the following hooks: poetry run pre-commit run --all-files which in most cases should automatically fix things as shown below:","title":"Pre-commit utilities"},{"location":"contribution/#contributing-new-algorithms","text":"We welcome the contributions of new algorithms. Before opening a pull request , please open an issue first to discuss with us since this is likely a sizable effort. Once we agree on the plan, feel free to make a PR to include the new algorithm. To help ease the review process, here is a checklist template when contributing a new algorithm. See https://github.com/vwxyzjn/cleanrl/pull/137 as an example. [ ] I've read the CONTRIBUTION guide ( required ). [ ] I have ensured pre-commit run --all-files passes ( required ). [ ] I have contacted @vwxyzjn to obtain access to the openrlbenchmark W&B team ( required ). [ ] I have tracked applicable experiments in openrlbenchmark/cleanrl with --capture-video flag toggled on ( required ). [ ] I have updated the documentation and previewed the changes via mkdocs serve . [ ] I have explained note-worthy implementation details. [ ] I have explained the logged metrics. [ ] I have added links to the original paper and related papers (if applicable). [ ] I have added links to the PR related to the algorithm. [ ] I have created a table comparing my results against those from reputable sources (i.e., the original paper or other reference implementation). [ ] I have added the learning curves (in PNG format with width=500 and height=300 ). [ ] I have added links to the tracked experiments. [ ] I have updated the tests accordingly (if applicable).","title":"Contributing new algorithms"},{"location":"made-with-cleanrl/","text":"Made with CleanRL CleanRL has become an increasingly popular deep reinforcement learning library, especially among practitioners who prefer more customizable code. Since its debut in July 2019, CleanRL has supported many open source projects and publications. Below are some highlight projects and publications made with CleanRL. Feel free to edit this list if your project or paper has used CleanRL. Publications The 37 Implementation Details of Proximal Policy Optimization , Huang, S., Dossa, R., Raffin, A., Kanervisto, A., & Wang, W. (2022). International Conference on Learning Representations 2022 Blog Post Track A Closer Look at Invalid Action Masking in Policy Gradient Algorithms , Huang, S., Onta\u00f1\u00f3n, S., (2022). The International FLAIRS Conference Proceedings, 35. Fast and Data-Efficient Training of Rainbow: an Experimental Study on Atari Schmidt, D., & Schmied, T. (2021). Deep Reinforcement Learning Workshop at the 35th Conference on Neural Information Processing Systems An Empirical Investigation of Early Stopping Optimizations in Proximal Policy Optimization , Dossa, R., Huang, S., Onta\u00f1\u00f3n, S., Matsubara, T., IEEE Access, 2021 Gym-\u03bcRTS: Toward Affordable Full Game Real-time Strategy Games Research with Deep Reinforcement Learning , Huang, S., Onta\u00f1\u00f3n, S., Bamford, C., Grela, L., IEEE Conference on Games 2021 Measuring Generalization of Deep Reinforcement Learning Applied to Real-time Strategy Games , Huang, S., Onta\u00f1\u00f3n, S., AAAI 2021 Reinforcement Learning in Games Workshop Griddly: A platform for AI research in games , Bamford, C., Huang, S., Lucas, S., AAAI 2021 Reinforcement Learning in Games Workshop Action Guidance: Getting the Best of Sparse Rewards and Shaped Rewards for Real-time Strategy Games , Huang, S., Onta\u00f1\u00f3n, S., AIIDE Workshop on Artificial Intelligence for Strategy Games , October 2020 Comparing Observation and Action Representations for Reinforcement Learning in \u00b5RTS , Huang, S., Onta\u00f1\u00f3n, S., AIIDE Workshop on Artificial Intelligence for Strategy Games , October 2019","title":"Made with CleanRL"},{"location":"made-with-cleanrl/#made-with-cleanrl","text":"CleanRL has become an increasingly popular deep reinforcement learning library, especially among practitioners who prefer more customizable code. Since its debut in July 2019, CleanRL has supported many open source projects and publications. Below are some highlight projects and publications made with CleanRL. Feel free to edit this list if your project or paper has used CleanRL.","title":"Made with CleanRL"},{"location":"made-with-cleanrl/#publications","text":"The 37 Implementation Details of Proximal Policy Optimization , Huang, S., Dossa, R., Raffin, A., Kanervisto, A., & Wang, W. (2022). International Conference on Learning Representations 2022 Blog Post Track A Closer Look at Invalid Action Masking in Policy Gradient Algorithms , Huang, S., Onta\u00f1\u00f3n, S., (2022). The International FLAIRS Conference Proceedings, 35. Fast and Data-Efficient Training of Rainbow: an Experimental Study on Atari Schmidt, D., & Schmied, T. (2021). Deep Reinforcement Learning Workshop at the 35th Conference on Neural Information Processing Systems An Empirical Investigation of Early Stopping Optimizations in Proximal Policy Optimization , Dossa, R., Huang, S., Onta\u00f1\u00f3n, S., Matsubara, T., IEEE Access, 2021 Gym-\u03bcRTS: Toward Affordable Full Game Real-time Strategy Games Research with Deep Reinforcement Learning , Huang, S., Onta\u00f1\u00f3n, S., Bamford, C., Grela, L., IEEE Conference on Games 2021 Measuring Generalization of Deep Reinforcement Learning Applied to Real-time Strategy Games , Huang, S., Onta\u00f1\u00f3n, S., AAAI 2021 Reinforcement Learning in Games Workshop Griddly: A platform for AI research in games , Bamford, C., Huang, S., Lucas, S., AAAI 2021 Reinforcement Learning in Games Workshop Action Guidance: Getting the Best of Sparse Rewards and Shaped Rewards for Real-time Strategy Games , Huang, S., Onta\u00f1\u00f3n, S., AIIDE Workshop on Artificial Intelligence for Strategy Games , October 2020 Comparing Observation and Action Representations for Reinforcement Learning in \u00b5RTS , Huang, S., Onta\u00f1\u00f3n, S., AIIDE Workshop on Artificial Intelligence for Strategy Games , October 2019","title":"Publications"},{"location":"open-rl-benchmark/","text":"","title":"Open rl benchmark"},{"location":"advanced/resume-training/","text":"Resume Training A common question we get asked is how to set up model checkpoints to continue training. In this document, we take this PPO example to explain that question. Save model checkpoints The first step is to save models periodically. By default, we save the model to wandb . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 num_updates = args . total_timesteps // args . batch_size CHECKPOINT_FREQUENCY = 50 starting_update = 1 for update in range ( starting_update , num_updates + 1 ): # ... do rollouts and train models if args . track : # make sure to tune `CHECKPOINT_FREQUENCY` # so models are not saved too frequently if update % CHECKPOINT_FREQUENCY == 0 : torch . save ( agent . state_dict (), f \" { wandb . run . dir } /agent.pt\" ) wandb . save ( f \" { wandb . run . dir } /agent.pt\" , policy = \"now\" ) Then we could run the following to train our agents python ppo_gridnet.py --prod-mode --capture-video If the training was terminated early, we can still see the last updated model agent.pt in W&B like in this URL https://wandb.ai/costa-huang/cleanRL/runs/21421tda/files or as follows Resume training The second step is to automatically download the agent.pt from the URL above and resume training as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 num_updates = args . total_timesteps // args . batch_size CHECKPOINT_FREQUENCY = 50 starting_update = 1 if args . track and wandb . run . resumed : starting_update = run . summary . get ( \"charts/update\" ) + 1 global_step = starting_update * args . batch_size api = wandb . Api () run = api . run ( f \" { run . entity } / { run . project } / { run . id } \" ) model = run . file ( \"agent.pt\" ) model . download ( f \"models/ { experiment_name } /\" ) agent . load_state_dict ( torch . load ( f \"models/ { experiment_name } /agent.pt\" , map_location = device )) agent . eval () print ( f \"resumed at update { starting_update } \" ) for update in range ( starting_update , num_updates + 1 ): # ... do rollouts and train models if args . track : # make sure to tune `CHECKPOINT_FREQUENCY` # so models are not saved too frequently if update % CHECKPOINT_FREQUENCY == 0 : torch . save ( agent . state_dict (), f \" { wandb . run . dir } /agent.pt\" ) wandb . save ( f \" { wandb . run . dir } /agent.pt\" , policy = \"now\" ) To resume training, note the ID of the experiment is 21421tda as in the URL https://wandb.ai/costa-huang/cleanRL/runs/21421tda , so we need to pass in the ID via environment variable to trigger the resume mode of W&B: WANDB_RUN_ID=21421tda WANDB_RESUME=must python ppo_gridnet.py --prod-mode --capture-video","title":"Resume Training"},{"location":"advanced/resume-training/#resume-training","text":"A common question we get asked is how to set up model checkpoints to continue training. In this document, we take this PPO example to explain that question.","title":"Resume Training"},{"location":"advanced/resume-training/#save-model-checkpoints","text":"The first step is to save models periodically. By default, we save the model to wandb . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 num_updates = args . total_timesteps // args . batch_size CHECKPOINT_FREQUENCY = 50 starting_update = 1 for update in range ( starting_update , num_updates + 1 ): # ... do rollouts and train models if args . track : # make sure to tune `CHECKPOINT_FREQUENCY` # so models are not saved too frequently if update % CHECKPOINT_FREQUENCY == 0 : torch . save ( agent . state_dict (), f \" { wandb . run . dir } /agent.pt\" ) wandb . save ( f \" { wandb . run . dir } /agent.pt\" , policy = \"now\" ) Then we could run the following to train our agents python ppo_gridnet.py --prod-mode --capture-video If the training was terminated early, we can still see the last updated model agent.pt in W&B like in this URL https://wandb.ai/costa-huang/cleanRL/runs/21421tda/files or as follows","title":"Save model checkpoints"},{"location":"advanced/resume-training/#resume-training_1","text":"The second step is to automatically download the agent.pt from the URL above and resume training as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 num_updates = args . total_timesteps // args . batch_size CHECKPOINT_FREQUENCY = 50 starting_update = 1 if args . track and wandb . run . resumed : starting_update = run . summary . get ( \"charts/update\" ) + 1 global_step = starting_update * args . batch_size api = wandb . Api () run = api . run ( f \" { run . entity } / { run . project } / { run . id } \" ) model = run . file ( \"agent.pt\" ) model . download ( f \"models/ { experiment_name } /\" ) agent . load_state_dict ( torch . load ( f \"models/ { experiment_name } /agent.pt\" , map_location = device )) agent . eval () print ( f \"resumed at update { starting_update } \" ) for update in range ( starting_update , num_updates + 1 ): # ... do rollouts and train models if args . track : # make sure to tune `CHECKPOINT_FREQUENCY` # so models are not saved too frequently if update % CHECKPOINT_FREQUENCY == 0 : torch . save ( agent . state_dict (), f \" { wandb . run . dir } /agent.pt\" ) wandb . save ( f \" { wandb . run . dir } /agent.pt\" , policy = \"now\" ) To resume training, note the ID of the experiment is 21421tda as in the URL https://wandb.ai/costa-huang/cleanRL/runs/21421tda , so we need to pass in the ID via environment variable to trigger the resume mode of W&B: WANDB_RUN_ID=21421tda WANDB_RESUME=must python ppo_gridnet.py --prod-mode --capture-video","title":"Resume training"},{"location":"cloud/installation/","text":"Installation The rough idea behind the cloud integration is to package our code into a docker container and use AWS Batch to run thousands of experiments concurrently. Prerequisites Terraform (see installation tutorial here ) We use Terraform to define our infrastructure with AWS Batch, which you can spin up as follows # assuming you are at the root of the CleanRL project poetry install -E cloud cd cloud python -m awscli configure terraform init export AWS_DEFAULT_REGION = $( aws configure get region --profile default ) terraform apply Note Don't worry about the cost of spining up these AWS Batch compute environments and job queues. They are completely free and you are only charged when you submit experiments. Then your AWS Batch console should look like Clean Up Uninstalling/Deleting the infrastructure is pretty straightforward: export AWS_DEFAULT_REGION=$(aws configure get region --profile default) terraform destroy","title":"Installation"},{"location":"cloud/installation/#installation","text":"The rough idea behind the cloud integration is to package our code into a docker container and use AWS Batch to run thousands of experiments concurrently.","title":"Installation"},{"location":"cloud/installation/#prerequisites","text":"Terraform (see installation tutorial here ) We use Terraform to define our infrastructure with AWS Batch, which you can spin up as follows # assuming you are at the root of the CleanRL project poetry install -E cloud cd cloud python -m awscli configure terraform init export AWS_DEFAULT_REGION = $( aws configure get region --profile default ) terraform apply Note Don't worry about the cost of spining up these AWS Batch compute environments and job queues. They are completely free and you are only charged when you submit experiments. Then your AWS Batch console should look like","title":"Prerequisites"},{"location":"cloud/installation/#clean-up","text":"Uninstalling/Deleting the infrastructure is pretty straightforward: export AWS_DEFAULT_REGION=$(aws configure get region --profile default) terraform destroy","title":"Clean Up"},{"location":"cloud/submit-experiments/","text":"Submit Experiments Inspection Dry run to inspect the generated docker command poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --num-seed 1 The generated docker command should look like docker run -d --cpuset-cpus=\"0\" -e WANDB_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx vwxyzjn/cleanrl:latest /bin/bash -c \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video --seed 1\" Run on AWS Submit a job using AWS's compute-optimized spot instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --job-queue c5a-large-spot \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-memory 2000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's accelerated-computing spot instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 --track --capture-video\" \\ --job-queue g4dn-xlarge-spot \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-gpu 1 \\ --num-memory 4000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's compute-optimized on-demand instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --job-queue c5a-large \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-memory 2000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's accelerated-computing on-demand instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 --track --capture-video\" \\ --job-queue g4dn-xlarge \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-gpu 1 \\ --num-memory 4000 \\ --num-hours 48.0 \\ --provider aws Then you should see: Customize the Docker Container Set up docker's buildx and login in to your preferred registry. docker buildx create --use docker login Then you could build a container using the --build flag based on the Dockerfile in the current directory. Also, --push will auto-push to the docker registry. poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --build --push To build a multi-arch image using --archs linux/arm64,linux/amd64 : poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --archs linux/arm64,linux/amd64 --build --push Note Building an multi-arch image is quite slow but will allow you to use ARM instances such as m6gd.medium that is 20-70% cheaper than X86 instances. However, note there is no cloud providers that give ARM instances with Nvidia's GPU (to my knowledge), so this effort might not be worth it. If you still wants to pursue multi-arch, you can speed things up by using a native ARM server and connect it to your buildx instance: docker -H ssh://costa@gpu info docker buildx create --name remote --use docker buildx create --name remote --append ssh://costa@gpu docker buildx inspect --bootstrap python -m cleanrl_utils.submit_exp -b --archs linux/arm64,linux/amd64","title":"Submit Experiments"},{"location":"cloud/submit-experiments/#submit-experiments","text":"","title":"Submit Experiments"},{"location":"cloud/submit-experiments/#inspection","text":"Dry run to inspect the generated docker command poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --num-seed 1 The generated docker command should look like docker run -d --cpuset-cpus=\"0\" -e WANDB_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx vwxyzjn/cleanrl:latest /bin/bash -c \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video --seed 1\"","title":"Inspection"},{"location":"cloud/submit-experiments/#run-on-aws","text":"Submit a job using AWS's compute-optimized spot instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --job-queue c5a-large-spot \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-memory 2000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's accelerated-computing spot instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 --track --capture-video\" \\ --job-queue g4dn-xlarge-spot \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-gpu 1 \\ --num-memory 4000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's compute-optimized on-demand instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --job-queue c5a-large \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-memory 2000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's accelerated-computing on-demand instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 --track --capture-video\" \\ --job-queue g4dn-xlarge \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-gpu 1 \\ --num-memory 4000 \\ --num-hours 48.0 \\ --provider aws Then you should see:","title":"Run on AWS"},{"location":"cloud/submit-experiments/#customize-the-docker-container","text":"Set up docker's buildx and login in to your preferred registry. docker buildx create --use docker login Then you could build a container using the --build flag based on the Dockerfile in the current directory. Also, --push will auto-push to the docker registry. poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --build --push To build a multi-arch image using --archs linux/arm64,linux/amd64 : poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --archs linux/arm64,linux/amd64 --build --push Note Building an multi-arch image is quite slow but will allow you to use ARM instances such as m6gd.medium that is 20-70% cheaper than X86 instances. However, note there is no cloud providers that give ARM instances with Nvidia's GPU (to my knowledge), so this effort might not be worth it. If you still wants to pursue multi-arch, you can speed things up by using a native ARM server and connect it to your buildx instance: docker -H ssh://costa@gpu info docker buildx create --name remote --use docker buildx create --name remote --append ssh://costa@gpu docker buildx inspect --bootstrap python -m cleanrl_utils.submit_exp -b --archs linux/arm64,linux/amd64","title":"Customize the Docker Container"},{"location":"get-started/basic-usage/","text":"Basic Usage Two Ways to Run After the dependencies have been installed, there are two ways to run the CleanRL script under the poetry virtual environments. Using poetry run : poetry run python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 Using poetry shell : We first activate the virtual environment by using poetry shell Then, run any desired CleanRL script Attention: Each step must be executed separately! poetry shell python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 Note We recommend poetry shell workflow for development. When the shell is activeated, you should be seeing a prefix like (cleanrl-iXg02GqF-py3.9) in your shell's prompt, which is the name of the poetry's virtual environment. We will assume to run other commands (e.g. tensorboard ) in the documentation within the poetry's shell. Visualize Training Metrics By default, the CleanRL scripts record all the training metrics via Tensorboard into the runs folder. So, after running the training script above, feel free to run tensorboard --logdir runs Visualize the Agent's Gameplay Videos CleanRL helps record the agent's gameplay videos with a --capture-video flag, which will save the videos in the videos/{$run_name} folder. 1 2 3 4 5 python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 \\ --capture-video Get Documentation You can directly obtained the documentation by using the --help flag. python cleanrl/ppo.py --help usage: ppo.py [ -h ] [ --exp-name EXP_NAME ] [ --env-id ENV_ID ] [ --learning-rate LEARNING_RATE ] [ --seed SEED ] [ --total-timesteps TOTAL_TIMESTEPS ] [ --torch-deterministic [ TORCH_DETERMINISTIC ]] [ --cuda [ CUDA ]] [ --track [ TRACK ]] [ --wandb-project-name WANDB_PROJECT_NAME ] [ --wandb-entity WANDB_ENTITY ] [ --capture-video [ CAPTURE_VIDEO ]] [ --num-envs NUM_ENVS ] [ --num-steps NUM_STEPS ] [ --anneal-lr [ ANNEAL_LR ]] [ --gae [ GAE ]] [ --gamma GAMMA ] [ --gae-lambda GAE_LAMBDA ] [ --num-minibatches NUM_MINIBATCHES ] [ --update-epochs UPDATE_EPOCHS ] [ --norm-adv [ NORM_ADV ]] [ --clip-coef CLIP_COEF ] [ --clip-vloss [ CLIP_VLOSS ]] [ --ent-coef ENT_COEF ] [ --vf-coef VF_COEF ] [ --max-grad-norm MAX_GRAD_NORM ] [ --target-kl TARGET_KL ] optional arguments: -h, --help show this help message and exit --exp-name EXP_NAME the name of this experiment --env-id ENV_ID the id of the environment --learning-rate LEARNING_RATE the learning rate of the optimizer --seed SEED seed of the experiment --total-timesteps TOTAL_TIMESTEPS total timesteps of the experiments --torch-deterministic [ TORCH_DETERMINISTIC ] if toggled, ` torch.backends.cudnn.deterministic = False ` --cuda [ CUDA ] if toggled, cuda will be enabled by default --track [ TRACK ] if toggled, this experiment will be tracked with Weights and Biases --wandb-project-name WANDB_PROJECT_NAME the wandb 's project name --wandb-entity WANDB_ENTITY the entity (team) of wandb' s project --capture-video [ CAPTURE_VIDEO ] weather to capture videos of the agent performances ( check out ` videos ` folder ) --num-envs NUM_ENVS the number of parallel game environments --num-steps NUM_STEPS the number of steps to run in each environment per policy rollout --anneal-lr [ ANNEAL_LR ] Toggle learning rate annealing for policy and value networks --gae [ GAE ] Use GAE for advantage computation --gamma GAMMA the discount factor gamma --gae-lambda GAE_LAMBDA the lambda for the general advantage estimation --num-minibatches NUM_MINIBATCHES the number of mini-batches --update-epochs UPDATE_EPOCHS the K epochs to update the policy --norm-adv [ NORM_ADV ] Toggles advantages normalization --clip-coef CLIP_COEF the surrogate clipping coefficient --clip-vloss [ CLIP_VLOSS ] Toggles whether or not to use a clipped loss for the value function , as per the paper. --ent-coef ENT_COEF coefficient of the entropy --vf-coef VF_COEF coefficient of the value function --max-grad-norm MAX_GRAD_NORM the maximum norm for the gradient clipping --target-kl TARGET_KL the target KL divergence threshold","title":"Basic Usage"},{"location":"get-started/basic-usage/#basic-usage","text":"","title":"Basic Usage"},{"location":"get-started/basic-usage/#two-ways-to-run","text":"After the dependencies have been installed, there are two ways to run the CleanRL script under the poetry virtual environments. Using poetry run : poetry run python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 Using poetry shell : We first activate the virtual environment by using poetry shell Then, run any desired CleanRL script Attention: Each step must be executed separately! poetry shell python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 Note We recommend poetry shell workflow for development. When the shell is activeated, you should be seeing a prefix like (cleanrl-iXg02GqF-py3.9) in your shell's prompt, which is the name of the poetry's virtual environment. We will assume to run other commands (e.g. tensorboard ) in the documentation within the poetry's shell.","title":"Two Ways to Run"},{"location":"get-started/basic-usage/#visualize-training-metrics","text":"By default, the CleanRL scripts record all the training metrics via Tensorboard into the runs folder. So, after running the training script above, feel free to run tensorboard --logdir runs","title":"Visualize Training Metrics"},{"location":"get-started/basic-usage/#visualize-the-agents-gameplay-videos","text":"CleanRL helps record the agent's gameplay videos with a --capture-video flag, which will save the videos in the videos/{$run_name} folder. 1 2 3 4 5 python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 \\ --capture-video","title":"Visualize the Agent's Gameplay Videos"},{"location":"get-started/basic-usage/#get-documentation","text":"You can directly obtained the documentation by using the --help flag. python cleanrl/ppo.py --help usage: ppo.py [ -h ] [ --exp-name EXP_NAME ] [ --env-id ENV_ID ] [ --learning-rate LEARNING_RATE ] [ --seed SEED ] [ --total-timesteps TOTAL_TIMESTEPS ] [ --torch-deterministic [ TORCH_DETERMINISTIC ]] [ --cuda [ CUDA ]] [ --track [ TRACK ]] [ --wandb-project-name WANDB_PROJECT_NAME ] [ --wandb-entity WANDB_ENTITY ] [ --capture-video [ CAPTURE_VIDEO ]] [ --num-envs NUM_ENVS ] [ --num-steps NUM_STEPS ] [ --anneal-lr [ ANNEAL_LR ]] [ --gae [ GAE ]] [ --gamma GAMMA ] [ --gae-lambda GAE_LAMBDA ] [ --num-minibatches NUM_MINIBATCHES ] [ --update-epochs UPDATE_EPOCHS ] [ --norm-adv [ NORM_ADV ]] [ --clip-coef CLIP_COEF ] [ --clip-vloss [ CLIP_VLOSS ]] [ --ent-coef ENT_COEF ] [ --vf-coef VF_COEF ] [ --max-grad-norm MAX_GRAD_NORM ] [ --target-kl TARGET_KL ] optional arguments: -h, --help show this help message and exit --exp-name EXP_NAME the name of this experiment --env-id ENV_ID the id of the environment --learning-rate LEARNING_RATE the learning rate of the optimizer --seed SEED seed of the experiment --total-timesteps TOTAL_TIMESTEPS total timesteps of the experiments --torch-deterministic [ TORCH_DETERMINISTIC ] if toggled, ` torch.backends.cudnn.deterministic = False ` --cuda [ CUDA ] if toggled, cuda will be enabled by default --track [ TRACK ] if toggled, this experiment will be tracked with Weights and Biases --wandb-project-name WANDB_PROJECT_NAME the wandb 's project name --wandb-entity WANDB_ENTITY the entity (team) of wandb' s project --capture-video [ CAPTURE_VIDEO ] weather to capture videos of the agent performances ( check out ` videos ` folder ) --num-envs NUM_ENVS the number of parallel game environments --num-steps NUM_STEPS the number of steps to run in each environment per policy rollout --anneal-lr [ ANNEAL_LR ] Toggle learning rate annealing for policy and value networks --gae [ GAE ] Use GAE for advantage computation --gamma GAMMA the discount factor gamma --gae-lambda GAE_LAMBDA the lambda for the general advantage estimation --num-minibatches NUM_MINIBATCHES the number of mini-batches --update-epochs UPDATE_EPOCHS the K epochs to update the policy --norm-adv [ NORM_ADV ] Toggles advantages normalization --clip-coef CLIP_COEF the surrogate clipping coefficient --clip-vloss [ CLIP_VLOSS ] Toggles whether or not to use a clipped loss for the value function , as per the paper. --ent-coef ENT_COEF coefficient of the entropy --vf-coef VF_COEF coefficient of the value function --max-grad-norm MAX_GRAD_NORM the maximum norm for the gradient clipping --target-kl TARGET_KL the target KL divergence threshold","title":"Get Documentation"},{"location":"get-started/benchmark-utility/","text":"Benchmark Utility CleanRL comes with a utility module cleanrl_utils.benchmark to help schedule and run benchmark experiments on your local machine. Usage Try running python -m cleanrl_utils.benchmark --help to get the help text. python -m cleanrl_utils.benchmark --help usage: benchmark.py [ -h ] [ --env-ids ENV_IDS [ ENV_IDS ... ]] [ --command COMMAND ] [ --num-seeds NUM_SEEDS ] [ --workers WORKERS ] optional arguments: -h, --help show this help message and exit --env-ids ENV_IDS [ ENV_IDS ... ] the ids of the environment to benchmark --command COMMAND the command to run --num-seeds NUM_SEEDS the number of random seeds --workers WORKERS the number of eval workers to run benchmark experimenets ( skips evaluation when set to 0 ) Examples The following example demonstrates how to run classic control benchmark experiments. OMP_NUM_THREADS = 1 xvfb-run -a python -m cleanrl_utils.benchmark \\ --env-ids CartPole-v1 Acrobot-v1 MountainCar-v0 \\ --command \"poetry run python cleanrl/ppo.py --cuda False --track --capture-video\" \\ --num-seeds 3 \\ --workers 5 What just happened here? In principle the helps run the following commands in 5 subprocesses: poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id CartPole-v1 --seed 1 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id Acrobot-v1 --seed 1 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id MountainCar-v0 --seed 1 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id CartPole-v1 --seed 2 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id Acrobot-v1 --seed 2 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id MountainCar-v0 --seed 2 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id CartPole-v1 --seed 3 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id Acrobot-v1 --seed 3 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id MountainCar-v0 --seed 3 More specifically: --env-ids CartPole-v1 Acrobot-v1 MountainCar-v0 specifies that running experiments against these three environments --command \"poetry run python cleanrl/ppo.py --cuda False --track --capture-video\" suggests running ppo.py with these settings: turn off GPU usage via --cuda False : because ppo.py has such as small neural network it often runs faster on CPU only track the experiments via --track render the agent gameplay videos via --capture-video ; these videos algo get saved to the tracked experiments xvfb-run -a virtualizes a display for video recording, enabling these commands on a headless linux system --num-seeds 3 suggests running the the command with 3 random seeds for each env-id --workers 5 suggests at maximum using 5 subprocesses to run the experiments OMP_NUM_THREADS=1 suggests torch to use only 1 thread for each subprocesses; this way we don't have processes fighting each other. Note that when you run with high-throughput environments such as envpool or procgen , it's recommended to set --workers 1 to maximuize SPS (steps per second), such as xvfb-run -a python -m cleanrl_utils.benchmark \\ --env-ids Pong-v5 BeamRider-v5 Breakout-v5 \\ --command \"poetry run python cleanrl/ppo_atari_envpool.py --track --capture-video\" \\ --num-seeds 3 \\ --workers 1 For more example usage, see https://github.com/vwxyzjn/cleanrl/blob/master/benchmark/ppo.sh","title":"Benchmark Utility"},{"location":"get-started/benchmark-utility/#benchmark-utility","text":"CleanRL comes with a utility module cleanrl_utils.benchmark to help schedule and run benchmark experiments on your local machine.","title":"Benchmark Utility"},{"location":"get-started/benchmark-utility/#usage","text":"Try running python -m cleanrl_utils.benchmark --help to get the help text. python -m cleanrl_utils.benchmark --help usage: benchmark.py [ -h ] [ --env-ids ENV_IDS [ ENV_IDS ... ]] [ --command COMMAND ] [ --num-seeds NUM_SEEDS ] [ --workers WORKERS ] optional arguments: -h, --help show this help message and exit --env-ids ENV_IDS [ ENV_IDS ... ] the ids of the environment to benchmark --command COMMAND the command to run --num-seeds NUM_SEEDS the number of random seeds --workers WORKERS the number of eval workers to run benchmark experimenets ( skips evaluation when set to 0 )","title":"Usage"},{"location":"get-started/benchmark-utility/#examples","text":"The following example demonstrates how to run classic control benchmark experiments. OMP_NUM_THREADS = 1 xvfb-run -a python -m cleanrl_utils.benchmark \\ --env-ids CartPole-v1 Acrobot-v1 MountainCar-v0 \\ --command \"poetry run python cleanrl/ppo.py --cuda False --track --capture-video\" \\ --num-seeds 3 \\ --workers 5 What just happened here? In principle the helps run the following commands in 5 subprocesses: poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id CartPole-v1 --seed 1 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id Acrobot-v1 --seed 1 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id MountainCar-v0 --seed 1 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id CartPole-v1 --seed 2 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id Acrobot-v1 --seed 2 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id MountainCar-v0 --seed 2 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id CartPole-v1 --seed 3 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id Acrobot-v1 --seed 3 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id MountainCar-v0 --seed 3 More specifically: --env-ids CartPole-v1 Acrobot-v1 MountainCar-v0 specifies that running experiments against these three environments --command \"poetry run python cleanrl/ppo.py --cuda False --track --capture-video\" suggests running ppo.py with these settings: turn off GPU usage via --cuda False : because ppo.py has such as small neural network it often runs faster on CPU only track the experiments via --track render the agent gameplay videos via --capture-video ; these videos algo get saved to the tracked experiments xvfb-run -a virtualizes a display for video recording, enabling these commands on a headless linux system --num-seeds 3 suggests running the the command with 3 random seeds for each env-id --workers 5 suggests at maximum using 5 subprocesses to run the experiments OMP_NUM_THREADS=1 suggests torch to use only 1 thread for each subprocesses; this way we don't have processes fighting each other. Note that when you run with high-throughput environments such as envpool or procgen , it's recommended to set --workers 1 to maximuize SPS (steps per second), such as xvfb-run -a python -m cleanrl_utils.benchmark \\ --env-ids Pong-v5 BeamRider-v5 Breakout-v5 \\ --command \"poetry run python cleanrl/ppo_atari_envpool.py --track --capture-video\" \\ --num-seeds 3 \\ --workers 1 For more example usage, see https://github.com/vwxyzjn/cleanrl/blob/master/benchmark/ppo.sh","title":"Examples"},{"location":"get-started/examples/","text":"Examples Atari poetry shell poetry install -E atari python cleanrl/dqn_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/c51_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 # NEW: 3-4x side-effects free speed up with envpool's atari (only available to linux) poetry install -E envpool python cleanrl/ppo_atari_envpool.py --env-id BreakoutNoFrameskip-v4 # Learn Pong-v5 in ~5-10 mins # Side effects such as lower sample efficiency might occur poetry run python ppo_atari_envpool.py --clip-coef=0.2 --num-envs=16 --num-minibatches=8 --num-steps=128 --update-epochs=3 Demo You can also run training scripts in other games, such as: Classic Control poetry shell python cleanrl/dqn.py --env-id CartPole-v1 python cleanrl/ppo.py --env-id CartPole-v1 python cleanrl/c51.py --env-id CartPole-v1 PyBullet poetry shell poetry install -E pybullet python cleanrl/td3_continuous_action.py --env-id MinitaurBulletDuckEnv-v0 python cleanrl/ddpg_continuous_action.py --env-id MinitaurBulletDuckEnv-v0 python cleanrl/sac_continuous_action.py --env-id MinitaurBulletDuckEnv-v0 Procgen poetry shell poetry install -E procgen python cleanrl/ppo_procgen.py --env-id starpilot python cleanrl/ppg_procgen.py --env-id starpilot PPO + LSTM poetry shell poetry install -E atari python cleanrl/ppo_atari_lstm.py --env-id BreakoutNoFrameskip-v4 python cleanrl/ppo_memory_env_lstm.py","title":"Examples"},{"location":"get-started/examples/#examples","text":"","title":"Examples"},{"location":"get-started/examples/#atari","text":"poetry shell poetry install -E atari python cleanrl/dqn_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/c51_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 # NEW: 3-4x side-effects free speed up with envpool's atari (only available to linux) poetry install -E envpool python cleanrl/ppo_atari_envpool.py --env-id BreakoutNoFrameskip-v4 # Learn Pong-v5 in ~5-10 mins # Side effects such as lower sample efficiency might occur poetry run python ppo_atari_envpool.py --clip-coef=0.2 --num-envs=16 --num-minibatches=8 --num-steps=128 --update-epochs=3","title":"Atari"},{"location":"get-started/examples/#demo","text":"You can also run training scripts in other games, such as:","title":"Demo"},{"location":"get-started/examples/#classic-control","text":"poetry shell python cleanrl/dqn.py --env-id CartPole-v1 python cleanrl/ppo.py --env-id CartPole-v1 python cleanrl/c51.py --env-id CartPole-v1","title":"Classic Control"},{"location":"get-started/examples/#pybullet","text":"poetry shell poetry install -E pybullet python cleanrl/td3_continuous_action.py --env-id MinitaurBulletDuckEnv-v0 python cleanrl/ddpg_continuous_action.py --env-id MinitaurBulletDuckEnv-v0 python cleanrl/sac_continuous_action.py --env-id MinitaurBulletDuckEnv-v0","title":"PyBullet"},{"location":"get-started/examples/#procgen","text":"poetry shell poetry install -E procgen python cleanrl/ppo_procgen.py --env-id starpilot python cleanrl/ppg_procgen.py --env-id starpilot","title":"Procgen"},{"location":"get-started/examples/#ppo-lstm","text":"poetry shell poetry install -E atari python cleanrl/ppo_atari_lstm.py --env-id BreakoutNoFrameskip-v4 python cleanrl/ppo_memory_env_lstm.py","title":"PPO + LSTM"},{"location":"get-started/experiment-tracking/","text":"Experiment tracking To use experiment tracking with wandb, run with the --track flag, which will also upload the videos recorded by the --capture-video flag. poetry shell wandb login # only required for the first time python cleanrl/ppo.py --track --capture-video The console will output the url for the tracked experiment like the following wandb: View project at https://wandb.ai/costa-huang/cleanRL wandb: View run at https://wandb.ai/costa-huang/cleanRL/runs/10dwbgeh When you open the URL, it's going to look like the following page:","title":"Experiment tracking"},{"location":"get-started/experiment-tracking/#experiment-tracking","text":"To use experiment tracking with wandb, run with the --track flag, which will also upload the videos recorded by the --capture-video flag. poetry shell wandb login # only required for the first time python cleanrl/ppo.py --track --capture-video The console will output the url for the tracked experiment like the following wandb: View project at https://wandb.ai/costa-huang/cleanRL wandb: View run at https://wandb.ai/costa-huang/cleanRL/runs/10dwbgeh When you open the URL, it's going to look like the following page:","title":"Experiment tracking"},{"location":"get-started/installation/","text":"Installation Prerequisites Python 3.8-3.9 (not yet 3.10) Poetry Simply run the following command for a quick start git clone https://github.com/vwxyzjn/cleanrl.git && cd cleanrl poetry install Working with PyPI mirrors Users in some countries (e.g., China) can usually speed up package installation via faster PyPI mirrors. If this helps you, try appending the following lines to the pyproject.toml at the root of this repository and run poetry install [[tool.poetry.source]] name = \"douban\" url = \"https://pypi.doubanio.com/simple/\" default = true Optional Dependencies CleanRL makes it easy to install optional dependencies for common RL environments and various development utilities. These optional dependencies are defined at pyproject.toml as shown below: atari = [ \"ale-py\" , \"AutoROM\" , \"stable-baselines3\" ] pybullet = [ \"pybullet\" ] procgen = [ \"procgen\" , \"stable-baselines3\" ] pettingzoo = [ \"pettingzoo\" , \"stable-baselines3\" , \"pygame\" , \"pymunk\" ] plot = [ \"pandas\" , \"seaborn\" ] cloud = [ \"boto3\" , \"awscli\" ] docs = [ \"mkdocs-material\" ] spyder = [ \"spyder\" ] You can install them using the following command poetry install -E atari poetry install -E pybullet poetry install -E mujoco poetry install -E procgen poetry install -E envpool poetry install -E pettingzoo Install via pip While we recommend using poetry to manage environments and dependencies, the traditional requirements.txt are available: pip install -r requirements/requirements.txt pip install -r requirements/requirements-atari.txt pip install -r requirements/requirements-pybullet.txt pip install -r requirements/requirements-mujoco.txt pip install -r requirements/requirements-procgen.txt pip install -r requirements/requirements-envpool.txt pip install -r requirements/requirements-pettingzoo.txt","title":"Installation"},{"location":"get-started/installation/#installation","text":"","title":"Installation"},{"location":"get-started/installation/#prerequisites","text":"Python 3.8-3.9 (not yet 3.10) Poetry Simply run the following command for a quick start git clone https://github.com/vwxyzjn/cleanrl.git && cd cleanrl poetry install Working with PyPI mirrors Users in some countries (e.g., China) can usually speed up package installation via faster PyPI mirrors. If this helps you, try appending the following lines to the pyproject.toml at the root of this repository and run poetry install [[tool.poetry.source]] name = \"douban\" url = \"https://pypi.doubanio.com/simple/\" default = true","title":"Prerequisites"},{"location":"get-started/installation/#optional-dependencies","text":"CleanRL makes it easy to install optional dependencies for common RL environments and various development utilities. These optional dependencies are defined at pyproject.toml as shown below: atari = [ \"ale-py\" , \"AutoROM\" , \"stable-baselines3\" ] pybullet = [ \"pybullet\" ] procgen = [ \"procgen\" , \"stable-baselines3\" ] pettingzoo = [ \"pettingzoo\" , \"stable-baselines3\" , \"pygame\" , \"pymunk\" ] plot = [ \"pandas\" , \"seaborn\" ] cloud = [ \"boto3\" , \"awscli\" ] docs = [ \"mkdocs-material\" ] spyder = [ \"spyder\" ] You can install them using the following command poetry install -E atari poetry install -E pybullet poetry install -E mujoco poetry install -E procgen poetry install -E envpool poetry install -E pettingzoo","title":"Optional Dependencies"},{"location":"get-started/installation/#install-via-pip","text":"While we recommend using poetry to manage environments and dependencies, the traditional requirements.txt are available: pip install -r requirements/requirements.txt pip install -r requirements/requirements-atari.txt pip install -r requirements/requirements-pybullet.txt pip install -r requirements/requirements-mujoco.txt pip install -r requirements/requirements-procgen.txt pip install -r requirements/requirements-envpool.txt pip install -r requirements/requirements-pettingzoo.txt","title":"Install via pip"},{"location":"rl-algorithms/c51/","text":"Categorical DQN (C51) Overview C51 introduces a distributional perspective for DQN: instead of learning a single value for an action, C51 learns to predict a distribution of values for the action. Empirically, C51 demonstrates impressive performance in ALE. Original papers: A Distributional Perspective on Reinforcement Learning Implemented Variants Variants Implemented Description c51_atari.py , docs For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. c51.py , docs For classic control tasks like CartPole-v1 . Below are our single-file implementations of C51: c51_atari.py The c51_atari.py has the following features: For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space Usage poetry install -E atari python cleanrl/c51_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/c51_atari.py --env-id PongNoFrameskip-v4 Explanation of the logged metrics Running python cleanrl/c51_atari.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/loss : the cross entropy loss between the \\(t\\) step state value distribution and the projected \\(t+1\\) step state value distribution losses/q_values : implemented as (old_pmfs * q_network.atoms).sum(1) , which is the sum of the probability of getting returns \\(x\\) ( old_pmfs ) multiplied by \\(x\\) ( q_network.atoms ), averaged over the sample obtained from the replay buffer; useful when gauging if under or over estimation happens Implementation details c51_atari.py is based on (Bellemare et al., 2017) 1 but presents a few implementation differences: (Bellemare et al., 2017) 1 injects stochaticity by doing \"on each frame the environment rejects the agent\u2019s selected action with probability \\(p = 0.25\\) \", but c51_atari.py does not do this c51_atari.py use a self-contained evaluation scheme: c51_atari.py reports the episodic returns obtained throughout training, whereas (Bellemare et al., 2017) 1 is trained with --end-e=0.01 but reported episodic returns using a separate evaluation process with --end-e=0.001 (See \"5.2. State-of-the-Art Results\" on page 7). Experiment results To run benchmark experiments, see benchmark/c51.sh . Specifically, execute the following command: Below are the average episodic returns for c51_atari.py . Environment c51_atari.py 10M steps (Bellemare et al., 2017, Figure 14) 1 50M steps (Hessel et al., 2017, Figure 5) 3 BreakoutNoFrameskip-v4 461.86 \u00b1 69.65 748 ~500 at 10M steps, ~600 at 50M steps PongNoFrameskip-v4 19.46 \u00b1 0.70 20.9 ~20 10M steps, ~20 at 50M steps BeamRiderNoFrameskip-v4 9592.90 \u00b1 2270.15 14,074 ~12000 10M steps, ~14000 at 50M steps Note that we save computational time by reducing timesteps from 50M to 10M, but our c51_atari.py scores the same or higher than (Mnih et al., 2015) 1 in 10M steps. Learning curves: Tracked experiments and game play videos: c51.py The c51.py has the following features: Works with the Box observation space of low-level features Works with the Discrete action space Works with envs like CartPole-v1 Usage python cleanrl/c51.py --env-id CartPole-v1 Explanation of the logged metrics See related docs for c51_atari.py . Implementation details The c51.py shares the same implementation details as c51_atari.py except the c51.py runs with different hyperparameters and neural network architecture. Specifically, c51.py uses a simpler neural network as follows: self . network = nn . Sequential ( nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 120 ), nn . ReLU (), nn . Linear ( 120 , 84 ), nn . ReLU (), nn . Linear ( 84 , env . single_action_space . n ), ) c51.py runs with different hyperparameters: python c51.py --total-timesteps 500000 \\ --learning-rate 2 .5e-4 \\ --buffer-size 10000 \\ --gamma 0 .99 \\ --target-network-frequency 500 \\ --max-grad-norm 0 .5 \\ --batch-size 128 \\ --start-e 1 \\ --end-e 0 .05 \\ --exploration-fraction 0 .5 \\ --learning-starts 10000 \\ --train-frequency 10 Experiment results To run benchmark experiments, see benchmark/c51.sh . Specifically, execute the following command: Below are the average episodic returns for c51.py . Environment c51.py CartPole-v1 481.20 \u00b1 20.53 Acrobot-v1 -87.70 \u00b1 5.52 MountainCar-v0 -166.38 \u00b1 27.94 Note that the C51 has no official benchmark on classic control environments, so we did not include a comparison. That said, our c51.py was able to achieve near perfect scores in CartPole-v1 and Acrobot-v1 ; further, it can obtain successful runs in the sparse environment MountainCar-v0 . Learning curves: Tracked experiments and game play videos: Bellemare, M.G., Dabney, W., & Munos, R. (2017). A Distributional Perspective on Reinforcement Learning. ICML. \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 [Proposal] Formal API handling of truncation vs termination. https://github.com/openai/gym/issues/2510 \u21a9 Hessel, M., Modayil, J., Hasselt, H.V., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M.G., & Silver, D. (2018). Rainbow: Combining Improvements in Deep Reinforcement Learning. AAAI. \u21a9","title":"Categorical DQN (C51)"},{"location":"rl-algorithms/c51/#categorical-dqn-c51","text":"","title":"Categorical DQN (C51)"},{"location":"rl-algorithms/c51/#overview","text":"C51 introduces a distributional perspective for DQN: instead of learning a single value for an action, C51 learns to predict a distribution of values for the action. Empirically, C51 demonstrates impressive performance in ALE. Original papers: A Distributional Perspective on Reinforcement Learning","title":"Overview"},{"location":"rl-algorithms/c51/#implemented-variants","text":"Variants Implemented Description c51_atari.py , docs For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. c51.py , docs For classic control tasks like CartPole-v1 . Below are our single-file implementations of C51:","title":"Implemented Variants"},{"location":"rl-algorithms/c51/#c51_ataripy","text":"The c51_atari.py has the following features: For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space","title":"c51_atari.py"},{"location":"rl-algorithms/c51/#usage","text":"poetry install -E atari python cleanrl/c51_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/c51_atari.py --env-id PongNoFrameskip-v4","title":"Usage"},{"location":"rl-algorithms/c51/#explanation-of-the-logged-metrics","text":"Running python cleanrl/c51_atari.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/loss : the cross entropy loss between the \\(t\\) step state value distribution and the projected \\(t+1\\) step state value distribution losses/q_values : implemented as (old_pmfs * q_network.atoms).sum(1) , which is the sum of the probability of getting returns \\(x\\) ( old_pmfs ) multiplied by \\(x\\) ( q_network.atoms ), averaged over the sample obtained from the replay buffer; useful when gauging if under or over estimation happens","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/c51/#implementation-details","text":"c51_atari.py is based on (Bellemare et al., 2017) 1 but presents a few implementation differences: (Bellemare et al., 2017) 1 injects stochaticity by doing \"on each frame the environment rejects the agent\u2019s selected action with probability \\(p = 0.25\\) \", but c51_atari.py does not do this c51_atari.py use a self-contained evaluation scheme: c51_atari.py reports the episodic returns obtained throughout training, whereas (Bellemare et al., 2017) 1 is trained with --end-e=0.01 but reported episodic returns using a separate evaluation process with --end-e=0.001 (See \"5.2. State-of-the-Art Results\" on page 7).","title":"Implementation details"},{"location":"rl-algorithms/c51/#experiment-results","text":"To run benchmark experiments, see benchmark/c51.sh . Specifically, execute the following command: Below are the average episodic returns for c51_atari.py . Environment c51_atari.py 10M steps (Bellemare et al., 2017, Figure 14) 1 50M steps (Hessel et al., 2017, Figure 5) 3 BreakoutNoFrameskip-v4 461.86 \u00b1 69.65 748 ~500 at 10M steps, ~600 at 50M steps PongNoFrameskip-v4 19.46 \u00b1 0.70 20.9 ~20 10M steps, ~20 at 50M steps BeamRiderNoFrameskip-v4 9592.90 \u00b1 2270.15 14,074 ~12000 10M steps, ~14000 at 50M steps Note that we save computational time by reducing timesteps from 50M to 10M, but our c51_atari.py scores the same or higher than (Mnih et al., 2015) 1 in 10M steps. Learning curves: Tracked experiments and game play videos:","title":"Experiment results"},{"location":"rl-algorithms/c51/#c51py","text":"The c51.py has the following features: Works with the Box observation space of low-level features Works with the Discrete action space Works with envs like CartPole-v1","title":"c51.py"},{"location":"rl-algorithms/c51/#usage_1","text":"python cleanrl/c51.py --env-id CartPole-v1","title":"Usage"},{"location":"rl-algorithms/c51/#explanation-of-the-logged-metrics_1","text":"See related docs for c51_atari.py .","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/c51/#implementation-details_1","text":"The c51.py shares the same implementation details as c51_atari.py except the c51.py runs with different hyperparameters and neural network architecture. Specifically, c51.py uses a simpler neural network as follows: self . network = nn . Sequential ( nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 120 ), nn . ReLU (), nn . Linear ( 120 , 84 ), nn . ReLU (), nn . Linear ( 84 , env . single_action_space . n ), ) c51.py runs with different hyperparameters: python c51.py --total-timesteps 500000 \\ --learning-rate 2 .5e-4 \\ --buffer-size 10000 \\ --gamma 0 .99 \\ --target-network-frequency 500 \\ --max-grad-norm 0 .5 \\ --batch-size 128 \\ --start-e 1 \\ --end-e 0 .05 \\ --exploration-fraction 0 .5 \\ --learning-starts 10000 \\ --train-frequency 10","title":"Implementation details"},{"location":"rl-algorithms/c51/#experiment-results_1","text":"To run benchmark experiments, see benchmark/c51.sh . Specifically, execute the following command: Below are the average episodic returns for c51.py . Environment c51.py CartPole-v1 481.20 \u00b1 20.53 Acrobot-v1 -87.70 \u00b1 5.52 MountainCar-v0 -166.38 \u00b1 27.94 Note that the C51 has no official benchmark on classic control environments, so we did not include a comparison. That said, our c51.py was able to achieve near perfect scores in CartPole-v1 and Acrobot-v1 ; further, it can obtain successful runs in the sparse environment MountainCar-v0 . Learning curves: Tracked experiments and game play videos: Bellemare, M.G., Dabney, W., & Munos, R. (2017). A Distributional Perspective on Reinforcement Learning. ICML. \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 [Proposal] Formal API handling of truncation vs termination. https://github.com/openai/gym/issues/2510 \u21a9 Hessel, M., Modayil, J., Hasselt, H.V., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M.G., & Silver, D. (2018). Rainbow: Combining Improvements in Deep Reinforcement Learning. AAAI. \u21a9","title":"Experiment results"},{"location":"rl-algorithms/ddpg/","text":"Deep Deterministic Policy Gradient (DDPG) Overview DDPG is a popular DRL algorithm for continuous control. It extends DQN to work with the continuous action space by introducing a deterministic actor that directly outputs continuous actions. DDPG also combines techniques from DQN, such as the replay buffer and target network. Original paper: Continuous control with deep reinforcement learning Reference resources: sfujim/TD3 Deep Deterministic Policy Gradient | Spinning Up in Deep RL ikostrikov/jaxrl (helpful reference when implemented ddpg_continuous_action_jax.py ) Implemented Variants Variants Implemented Description ddpg_continuous_action.py , docs For continuous action space Below is our single-file implementation of DDPG: ddpg_continuous_action.py The ddpg_continuous_action.py has the following features: For continuous action space Works with the Box observation space of low-level features Works with the Box (continuous) action space Usage poetry install poetry install -E pybullet python cleanrl/ddpg_continuous_action.py --help python cleanrl/ddpg_continuous_action.py --env-id HopperBulletEnv-v0 poetry install -E mujoco # only works in Linux python cleanrl/ddpg_continuous_action.py --env-id Hopper-v3 Explanation of the logged metrics Running python cleanrl/ddpg_continuous_action.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/qf1_loss : the mean squared error (MSE) between the Q values at timestep \\(t\\) and the Bellman update target estimated using the reward \\(r_t\\) and the Q values at timestep \\(t+1\\) , thus minimizing the one-step temporal difference. Formally, it can be expressed by the equation below. $$ J(\\theta^{Q}) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\big[ (Q(s, a) - y)^2 \\big], $$ with the Bellman update target \\(y = r + \\gamma \\, Q^{'}(s', a')\\) , where \\(a' \\sim \\mu^{'}(s')\\) , and the replay buffer \\(\\mathcal{D}\\) . losses/actor_loss : implemented as -qf1(data.observations, actor(data.observations)).mean() ; it is the negative average Q values calculated based on the 1) observations and the 2) actions computed by the actor based on these observations. By minimizing actor_loss , the optimizer updates the actors parameter using the following gradient (Lillicrap et al., 2016, Algorithm 1) 1 : \\[ \\nabla_{\\theta^{\\mu}} J \\approx \\frac{1}{N}\\sum_i\\left.\\left.\\nabla_{a} Q\\left(s, a \\mid \\theta^{Q}\\right)\\right|_{s=s_{i}, a=\\mu\\left(s_{i}\\right)} \\nabla_{\\theta^{\\mu}} \\mu\\left(s \\mid \\theta^{\\mu}\\right)\\right|_{s_{i}} \\] losses/qf1_values : implemented as qf1(data.observations, data.actions).view(-1) , it is the average Q values of the sampled data in the replay buffer; useful when gauging if under or over estimation happens. Implementation details Our ddpg_continuous_action.py is based on the OurDDPG.py from sfujim/TD3 , which presents the the following implementation difference from (Lillicrap et al., 2016) 1 : ddpg_continuous_action.py uses a gaussian exploration noise \\(\\mathcal{N}(0, 0.1)\\) , while (Lillicrap et al., 2016) 1 uses Ornstein-Uhlenbeck process with \\(\\theta=0.15\\) and \\(\\sigma=0.2\\) . ddpg_continuous_action.py runs the experiments using the openai/gym MuJoCo environments, while (Lillicrap et al., 2016) 1 uses their proprietary MuJoCo environments. ddpg_continuous_action.py uses the following architecture: class QNetwork ( nn . Module ): def __init__ ( self , env ): super ( QNetwork , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod () + np . prod ( env . single_action_space . shape ), 256 ) self . fc2 = nn . Linear ( 256 , 256 ) self . fc3 = nn . Linear ( 256 , 1 ) def forward ( self , x , a ): x = torch . cat ([ x , a ], 1 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x class Actor ( nn . Module ): def __init__ ( self , env ): super ( Actor , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 256 ) self . fc2 = nn . Linear ( 256 , 256 ) self . fc_mu = nn . Linear ( 256 , np . prod ( env . single_action_space . shape )) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) return torch . tanh ( self . fc_mu ( x )) while (Lillicrap et al., 2016, see Appendix 7 EXPERIMENT DETAILS) 1 uses the following architecture (difference highlighted): class QNetwork ( nn . Module ): def __init__ ( self , env ): super ( QNetwork , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 400 ) self . fc2 = nn . Linear ( 400 + np . prod ( env . single_action_space . shape ), 300 ) self . fc3 = nn . Linear ( 300 , 1 ) def forward ( self , x , a ): x = F . relu ( self . fc1 ( x )) x = torch . cat ([ x , a ], 1 ) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x class Actor ( nn . Module ): def __init__ ( self , env ): super ( Actor , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 400 ) self . fc2 = nn . Linear ( 400 , 300 ) self . fc_mu = nn . Linear ( 300 , np . prod ( env . single_action_space . shape )) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) return torch . tanh ( self . fc_mu ( x )) ddpg_continuous_action.py uses the following learning rates: q_optimizer = optim . Adam ( list ( qf1 . parameters ()), lr = 3e-4 ) actor_optimizer = optim . Adam ( list ( actor . parameters ()), lr = 3e-4 ) while (Lillicrap et al., 2016, see Appendix 7 EXPERIMENT DETAILS) 1 uses the following learning rates: q_optimizer = optim . Adam ( list ( qf1 . parameters ()), lr = 1e-4 ) actor_optimizer = optim . Adam ( list ( actor . parameters ()), lr = 1e-3 ) ddpg_continuous_action.py uses --batch-size=256 --tau=0.005 , while (Lillicrap et al., 2016, see Appendix 7 EXPERIMENT DETAILS) 1 uses --batch-size=64 --tau=0.001 ddpg_continuous_action.py also adds support for handling continuous environments where the lower and higher bounds of the action space are not \\([-1,1]\\) , or are asymmetric. The case where the bounds are not \\([-1,1]\\) is handled in DDPG.py (Fujimoto et al., 2018) 2 as follows: class Actor ( nn . Module ): ... def forward ( self , state ): a = F . relu ( self . l1 ( state )) a = F . relu ( self . l2 ( a )) return self . max_action * torch . tanh ( self . l3 ( a )) # Scale from [-1,1] to [-action_high, action_high] On the other hand, in CleanRL's ddpg_continuous_action.py , the mean and the scale of the the action space are computed as action_bias and action_scale respectively. Those scalars are in turn used to scale the output of a tanh activation function in the actor to the original action space range: class Actor ( nn . Module ): def __init__ ( self , env ): ... # action rescaling self . register_buffer ( \"action_scale\" , torch . FloatTensor (( env . action_space . high - env . action_space . low ) / 2.0 )) self . register_buffer ( \"action_bias\" , torch . FloatTensor (( env . action_space . high + env . action_space . low ) / 2.0 )) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = torch . tanh ( self . fc_mu ( x )) return x * self . action_scale + self . action_bias # Scale from [-1,1] to [-action_low, action_high] Additionally, when drawing exploration noise that is added to the actions produced by the actor, CleanRL's ddpg_continuous_action.py centers the distribution the sampled from at action_bias , and the scale of the distribution is set to action_scale * exploration_noise . Info Note that Humanoid-v2 , InvertedPendulum-v2 , Pusher-v2 have action space bounds that are not the standard [-1, 1] . See below. Ant-v2 Observation space: Box(-inf, inf, (111,), float64) Action space: Box(-1.0, 1.0, (8,), float32) HalfCheetah-v2 Observation space: Box(-inf, inf, (17,), float64) Action space: Box(-1.0, 1.0, (6,), float32) Hopper-v2 Observation space: Box(-inf, inf, (11,), float64) Action space: Box(-1.0, 1.0, (3,), float32) Humanoid-v2 Observation space: Box(-inf, inf, (376,), float64) Action space: Box(-0.4, 0.4, (17,), float32) InvertedDoublePendulum-v2 Observation space: Box(-inf, inf, (11,), float64) Action space: Box(-1.0, 1.0, (1,), float32) InvertedPendulum-v2 Observation space: Box(-inf, inf, (4,), float64) Action space: Box(-3.0, 3.0, (1,), float32) Pusher-v2 Observation space: Box(-inf, inf, (23,), float64) Action space: Box(-2.0, 2.0, (7,), float32) Reacher-v2 Observation space: Box(-inf, inf, (11,), float64) Action space: Box(-1.0, 1.0, (2,), float32) Swimmer-v2 Observation space: Box(-inf, inf, (8,), float64) Action space: Box(-1.0, 1.0, (2,), float32) Walker2d-v2 Observation space: Box(-inf, inf, (17,), float64) Action space: Box(-1.0, 1.0, (6,), float32) Experiment results To run benchmark experiments, see benchmark/ddpg.sh . Specifically, execute the following command: Below are the average episodic returns for ddpg_continuous_action.py (3 random seeds). To ensure the quality of the implementation, we compared the results against (Fujimoto et al., 2018) 2 . Environment ddpg_continuous_action.py OurDDPG.py (Fujimoto et al., 2018, Table 1) 2 DDPG.py using settings from (Lillicrap et al., 2016) 1 in (Fujimoto et al., 2018, Table 1) 2 HalfCheetah 9382.32 \u00b1 1395.52 8577.29 3305.60 Walker2d 1598.35 \u00b1 862.66 3098.11 1843.85 Hopper 1313.43 \u00b1 684.46 1860.02 2020.46 Humanoid 897.74 \u00b1 281.87 not available Pusher -34.45 \u00b1 4.47 not available InvertedPendulum 645.67 \u00b1 270.31 1000.00 \u00b1 0.00 Info Note that ddpg_continuous_action.py uses gym MuJoCo v2 environments while OurDDPG.py (Fujimoto et al., 2018) 2 uses the gym MuJoCo v1 environments. According to the openai/gym#834 , gym MuJoCo v2 environments should be equivalent to the gym MuJoCo v1 environments. Also note the performance of our ddpg_continuous_action.py seems to be worse than the reference implementation on Walker2d and Hopper. This is likely due to openai/gym#938 . We would have a hard time reproducing gym MuJoCo v1 environments because they have been long deprecated. One other thing could cause the performance difference: the original code reported the average episodic return using determinisitc evaluation (i.e., without exploration noise), see sfujim/TD3/main.py#L15-L32 , whereas we reported the episodic return during training and the policy gets updated between environments steps. Learning curves: ddpg_continuous_action_jax.py The ddpg_continuous_action_jax.py has the following features: Uses Jax , Flax , and Optax instead of torch . ddpg_continuous_action_jax.py is roughly 2.5-4x faster than ddpg_continuous_action.py For continuous action space Works with the Box observation space of low-level features Works with the Box (continuous) action space Usage poetry install -E \"mujoco jax\" poetry run pip install --upgrade \"jax[cuda]==0.3.14\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html poetry run python -c \"import mujoco_py\" python cleanrl/ddpg_continuous_action_jax.py --help poetry install -E mujoco # only works in Linux python cleanrl/ddpg_continuous_action_jax.py --env-id Hopper-v3 Explanation of the logged metrics See related docs for ddpg_continuous_action.py . Implementation details See related docs for ddpg_continuous_action.py . Experiment results To run benchmark experiments, see benchmark/ddpg.sh . Specifically, execute the following command: Below are the average episodic returns for ddpg_continuous_action_jax.py (3 random seeds). To ensure the quality of the implementation, we compared the results against (Fujimoto et al., 2018) 2 . Environment ddpg_continuous_action_jax.py ddpg_continuous_action.py OurDDPG.py (Fujimoto et al., 2018, Table 1) 2 HalfCheetah 9910.53 \u00b1 673.49 9382.32 \u00b1 1395.52 8577.29 Walker2d 1397.60 \u00b1 677.12 1598.35 \u00b1 862 3098.11 Hopper 1603.5 \u00b1 727.281 1313.43 \u00b1 684.46 1860.02 Info Note that we ran the ddpg_continuous_action_jax.py experiments with RTX 3060 Ti (~810 SPS) and ddpg_continuous_action.py experiments with RTX 2060 (~241 SPS). Using RTX 3060 Ti w/ ddpg_continuous_action.py brings the SPS from 241 to 325, according to this report meaning that under the same hardware, ddpg_continuous_action_jax.py would be roughly 810/241=2.5x faster . However, because of the overhead of --capture-video that both scripts suffer, we suspect ddpg_continuous_action_jax.py would be 3x-4x faster when --capture-video is disabled. Learning curves: Tracked experiments and game play videos: Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N.M., Erez, T., Tassa, Y., Silver, D., & Wierstra, D. (2016). Continuous control with deep reinforcement learning. CoRR, abs/1509.02971. https://arxiv.org/abs/1509.02971 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 Fujimoto, S., Hoof, H.V., & Meger, D. (2018). Addressing Function Approximation Error in Actor-Critic Methods. ArXiv, abs/1802.09477. https://arxiv.org/abs/1802.09477 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9","title":"Deep Deterministic Policy Gradient (DDPG)"},{"location":"rl-algorithms/ddpg/#deep-deterministic-policy-gradient-ddpg","text":"","title":"Deep Deterministic Policy Gradient (DDPG)"},{"location":"rl-algorithms/ddpg/#overview","text":"DDPG is a popular DRL algorithm for continuous control. It extends DQN to work with the continuous action space by introducing a deterministic actor that directly outputs continuous actions. DDPG also combines techniques from DQN, such as the replay buffer and target network. Original paper: Continuous control with deep reinforcement learning Reference resources: sfujim/TD3 Deep Deterministic Policy Gradient | Spinning Up in Deep RL ikostrikov/jaxrl (helpful reference when implemented ddpg_continuous_action_jax.py )","title":"Overview"},{"location":"rl-algorithms/ddpg/#implemented-variants","text":"Variants Implemented Description ddpg_continuous_action.py , docs For continuous action space Below is our single-file implementation of DDPG:","title":"Implemented Variants"},{"location":"rl-algorithms/ddpg/#ddpg_continuous_actionpy","text":"The ddpg_continuous_action.py has the following features: For continuous action space Works with the Box observation space of low-level features Works with the Box (continuous) action space","title":"ddpg_continuous_action.py"},{"location":"rl-algorithms/ddpg/#usage","text":"poetry install poetry install -E pybullet python cleanrl/ddpg_continuous_action.py --help python cleanrl/ddpg_continuous_action.py --env-id HopperBulletEnv-v0 poetry install -E mujoco # only works in Linux python cleanrl/ddpg_continuous_action.py --env-id Hopper-v3","title":"Usage"},{"location":"rl-algorithms/ddpg/#explanation-of-the-logged-metrics","text":"Running python cleanrl/ddpg_continuous_action.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/qf1_loss : the mean squared error (MSE) between the Q values at timestep \\(t\\) and the Bellman update target estimated using the reward \\(r_t\\) and the Q values at timestep \\(t+1\\) , thus minimizing the one-step temporal difference. Formally, it can be expressed by the equation below. $$ J(\\theta^{Q}) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\big[ (Q(s, a) - y)^2 \\big], $$ with the Bellman update target \\(y = r + \\gamma \\, Q^{'}(s', a')\\) , where \\(a' \\sim \\mu^{'}(s')\\) , and the replay buffer \\(\\mathcal{D}\\) . losses/actor_loss : implemented as -qf1(data.observations, actor(data.observations)).mean() ; it is the negative average Q values calculated based on the 1) observations and the 2) actions computed by the actor based on these observations. By minimizing actor_loss , the optimizer updates the actors parameter using the following gradient (Lillicrap et al., 2016, Algorithm 1) 1 : \\[ \\nabla_{\\theta^{\\mu}} J \\approx \\frac{1}{N}\\sum_i\\left.\\left.\\nabla_{a} Q\\left(s, a \\mid \\theta^{Q}\\right)\\right|_{s=s_{i}, a=\\mu\\left(s_{i}\\right)} \\nabla_{\\theta^{\\mu}} \\mu\\left(s \\mid \\theta^{\\mu}\\right)\\right|_{s_{i}} \\] losses/qf1_values : implemented as qf1(data.observations, data.actions).view(-1) , it is the average Q values of the sampled data in the replay buffer; useful when gauging if under or over estimation happens.","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/ddpg/#implementation-details","text":"Our ddpg_continuous_action.py is based on the OurDDPG.py from sfujim/TD3 , which presents the the following implementation difference from (Lillicrap et al., 2016) 1 : ddpg_continuous_action.py uses a gaussian exploration noise \\(\\mathcal{N}(0, 0.1)\\) , while (Lillicrap et al., 2016) 1 uses Ornstein-Uhlenbeck process with \\(\\theta=0.15\\) and \\(\\sigma=0.2\\) . ddpg_continuous_action.py runs the experiments using the openai/gym MuJoCo environments, while (Lillicrap et al., 2016) 1 uses their proprietary MuJoCo environments. ddpg_continuous_action.py uses the following architecture: class QNetwork ( nn . Module ): def __init__ ( self , env ): super ( QNetwork , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod () + np . prod ( env . single_action_space . shape ), 256 ) self . fc2 = nn . Linear ( 256 , 256 ) self . fc3 = nn . Linear ( 256 , 1 ) def forward ( self , x , a ): x = torch . cat ([ x , a ], 1 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x class Actor ( nn . Module ): def __init__ ( self , env ): super ( Actor , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 256 ) self . fc2 = nn . Linear ( 256 , 256 ) self . fc_mu = nn . Linear ( 256 , np . prod ( env . single_action_space . shape )) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) return torch . tanh ( self . fc_mu ( x )) while (Lillicrap et al., 2016, see Appendix 7 EXPERIMENT DETAILS) 1 uses the following architecture (difference highlighted): class QNetwork ( nn . Module ): def __init__ ( self , env ): super ( QNetwork , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 400 ) self . fc2 = nn . Linear ( 400 + np . prod ( env . single_action_space . shape ), 300 ) self . fc3 = nn . Linear ( 300 , 1 ) def forward ( self , x , a ): x = F . relu ( self . fc1 ( x )) x = torch . cat ([ x , a ], 1 ) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x class Actor ( nn . Module ): def __init__ ( self , env ): super ( Actor , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 400 ) self . fc2 = nn . Linear ( 400 , 300 ) self . fc_mu = nn . Linear ( 300 , np . prod ( env . single_action_space . shape )) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) return torch . tanh ( self . fc_mu ( x )) ddpg_continuous_action.py uses the following learning rates: q_optimizer = optim . Adam ( list ( qf1 . parameters ()), lr = 3e-4 ) actor_optimizer = optim . Adam ( list ( actor . parameters ()), lr = 3e-4 ) while (Lillicrap et al., 2016, see Appendix 7 EXPERIMENT DETAILS) 1 uses the following learning rates: q_optimizer = optim . Adam ( list ( qf1 . parameters ()), lr = 1e-4 ) actor_optimizer = optim . Adam ( list ( actor . parameters ()), lr = 1e-3 ) ddpg_continuous_action.py uses --batch-size=256 --tau=0.005 , while (Lillicrap et al., 2016, see Appendix 7 EXPERIMENT DETAILS) 1 uses --batch-size=64 --tau=0.001 ddpg_continuous_action.py also adds support for handling continuous environments where the lower and higher bounds of the action space are not \\([-1,1]\\) , or are asymmetric. The case where the bounds are not \\([-1,1]\\) is handled in DDPG.py (Fujimoto et al., 2018) 2 as follows: class Actor ( nn . Module ): ... def forward ( self , state ): a = F . relu ( self . l1 ( state )) a = F . relu ( self . l2 ( a )) return self . max_action * torch . tanh ( self . l3 ( a )) # Scale from [-1,1] to [-action_high, action_high] On the other hand, in CleanRL's ddpg_continuous_action.py , the mean and the scale of the the action space are computed as action_bias and action_scale respectively. Those scalars are in turn used to scale the output of a tanh activation function in the actor to the original action space range: class Actor ( nn . Module ): def __init__ ( self , env ): ... # action rescaling self . register_buffer ( \"action_scale\" , torch . FloatTensor (( env . action_space . high - env . action_space . low ) / 2.0 )) self . register_buffer ( \"action_bias\" , torch . FloatTensor (( env . action_space . high + env . action_space . low ) / 2.0 )) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = torch . tanh ( self . fc_mu ( x )) return x * self . action_scale + self . action_bias # Scale from [-1,1] to [-action_low, action_high] Additionally, when drawing exploration noise that is added to the actions produced by the actor, CleanRL's ddpg_continuous_action.py centers the distribution the sampled from at action_bias , and the scale of the distribution is set to action_scale * exploration_noise . Info Note that Humanoid-v2 , InvertedPendulum-v2 , Pusher-v2 have action space bounds that are not the standard [-1, 1] . See below. Ant-v2 Observation space: Box(-inf, inf, (111,), float64) Action space: Box(-1.0, 1.0, (8,), float32) HalfCheetah-v2 Observation space: Box(-inf, inf, (17,), float64) Action space: Box(-1.0, 1.0, (6,), float32) Hopper-v2 Observation space: Box(-inf, inf, (11,), float64) Action space: Box(-1.0, 1.0, (3,), float32) Humanoid-v2 Observation space: Box(-inf, inf, (376,), float64) Action space: Box(-0.4, 0.4, (17,), float32) InvertedDoublePendulum-v2 Observation space: Box(-inf, inf, (11,), float64) Action space: Box(-1.0, 1.0, (1,), float32) InvertedPendulum-v2 Observation space: Box(-inf, inf, (4,), float64) Action space: Box(-3.0, 3.0, (1,), float32) Pusher-v2 Observation space: Box(-inf, inf, (23,), float64) Action space: Box(-2.0, 2.0, (7,), float32) Reacher-v2 Observation space: Box(-inf, inf, (11,), float64) Action space: Box(-1.0, 1.0, (2,), float32) Swimmer-v2 Observation space: Box(-inf, inf, (8,), float64) Action space: Box(-1.0, 1.0, (2,), float32) Walker2d-v2 Observation space: Box(-inf, inf, (17,), float64) Action space: Box(-1.0, 1.0, (6,), float32)","title":"Implementation details"},{"location":"rl-algorithms/ddpg/#experiment-results","text":"To run benchmark experiments, see benchmark/ddpg.sh . Specifically, execute the following command: Below are the average episodic returns for ddpg_continuous_action.py (3 random seeds). To ensure the quality of the implementation, we compared the results against (Fujimoto et al., 2018) 2 . Environment ddpg_continuous_action.py OurDDPG.py (Fujimoto et al., 2018, Table 1) 2 DDPG.py using settings from (Lillicrap et al., 2016) 1 in (Fujimoto et al., 2018, Table 1) 2 HalfCheetah 9382.32 \u00b1 1395.52 8577.29 3305.60 Walker2d 1598.35 \u00b1 862.66 3098.11 1843.85 Hopper 1313.43 \u00b1 684.46 1860.02 2020.46 Humanoid 897.74 \u00b1 281.87 not available Pusher -34.45 \u00b1 4.47 not available InvertedPendulum 645.67 \u00b1 270.31 1000.00 \u00b1 0.00 Info Note that ddpg_continuous_action.py uses gym MuJoCo v2 environments while OurDDPG.py (Fujimoto et al., 2018) 2 uses the gym MuJoCo v1 environments. According to the openai/gym#834 , gym MuJoCo v2 environments should be equivalent to the gym MuJoCo v1 environments. Also note the performance of our ddpg_continuous_action.py seems to be worse than the reference implementation on Walker2d and Hopper. This is likely due to openai/gym#938 . We would have a hard time reproducing gym MuJoCo v1 environments because they have been long deprecated. One other thing could cause the performance difference: the original code reported the average episodic return using determinisitc evaluation (i.e., without exploration noise), see sfujim/TD3/main.py#L15-L32 , whereas we reported the episodic return during training and the policy gets updated between environments steps. Learning curves:","title":"Experiment results"},{"location":"rl-algorithms/ddpg/#ddpg_continuous_action_jaxpy","text":"The ddpg_continuous_action_jax.py has the following features: Uses Jax , Flax , and Optax instead of torch . ddpg_continuous_action_jax.py is roughly 2.5-4x faster than ddpg_continuous_action.py For continuous action space Works with the Box observation space of low-level features Works with the Box (continuous) action space","title":"ddpg_continuous_action_jax.py"},{"location":"rl-algorithms/ddpg/#usage_1","text":"poetry install -E \"mujoco jax\" poetry run pip install --upgrade \"jax[cuda]==0.3.14\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html poetry run python -c \"import mujoco_py\" python cleanrl/ddpg_continuous_action_jax.py --help poetry install -E mujoco # only works in Linux python cleanrl/ddpg_continuous_action_jax.py --env-id Hopper-v3","title":"Usage"},{"location":"rl-algorithms/ddpg/#explanation-of-the-logged-metrics_1","text":"See related docs for ddpg_continuous_action.py .","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/ddpg/#implementation-details_1","text":"See related docs for ddpg_continuous_action.py .","title":"Implementation details"},{"location":"rl-algorithms/ddpg/#experiment-results_1","text":"To run benchmark experiments, see benchmark/ddpg.sh . Specifically, execute the following command: Below are the average episodic returns for ddpg_continuous_action_jax.py (3 random seeds). To ensure the quality of the implementation, we compared the results against (Fujimoto et al., 2018) 2 . Environment ddpg_continuous_action_jax.py ddpg_continuous_action.py OurDDPG.py (Fujimoto et al., 2018, Table 1) 2 HalfCheetah 9910.53 \u00b1 673.49 9382.32 \u00b1 1395.52 8577.29 Walker2d 1397.60 \u00b1 677.12 1598.35 \u00b1 862 3098.11 Hopper 1603.5 \u00b1 727.281 1313.43 \u00b1 684.46 1860.02 Info Note that we ran the ddpg_continuous_action_jax.py experiments with RTX 3060 Ti (~810 SPS) and ddpg_continuous_action.py experiments with RTX 2060 (~241 SPS). Using RTX 3060 Ti w/ ddpg_continuous_action.py brings the SPS from 241 to 325, according to this report meaning that under the same hardware, ddpg_continuous_action_jax.py would be roughly 810/241=2.5x faster . However, because of the overhead of --capture-video that both scripts suffer, we suspect ddpg_continuous_action_jax.py would be 3x-4x faster when --capture-video is disabled. Learning curves: Tracked experiments and game play videos: Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N.M., Erez, T., Tassa, Y., Silver, D., & Wierstra, D. (2016). Continuous control with deep reinforcement learning. CoRR, abs/1509.02971. https://arxiv.org/abs/1509.02971 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 Fujimoto, S., Hoof, H.V., & Meger, D. (2018). Addressing Function Approximation Error in Actor-Critic Methods. ArXiv, abs/1802.09477. https://arxiv.org/abs/1802.09477 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9","title":"Experiment results"},{"location":"rl-algorithms/dqn/","text":"Deep Q-Learning (DQN) Overview As an extension of the Q-learning, DQN's main technical contribution is the use of replay buffer and target network, both of which would help improve the stability of the algorithm. Original papers: Human-level control through deep reinforcement learning Implemented Variants Variants Implemented Description dqn_atari.py , docs For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. dqn.py , docs For classic control tasks like CartPole-v1 . Below are our single-file implementations of DQN: dqn_atari.py The dqn_atari.py has the following features: For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space Usage poetry install -E atari python cleanrl/dqn_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/dqn_atari.py --env-id PongNoFrameskip-v4 Explanation of the logged metrics Running python cleanrl/dqn_atari.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/td_loss : the mean squared error (MSE) between the Q values at timestep \\(t\\) and the Bellman update target estimated using the reward \\(r_t\\) and the Q values at timestep \\(t+1\\) , thus minimizing the one-step temporal difference. Formally, it can be expressed by the equation below. $$ J(\\theta^{Q}) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\big[ (Q(s, a) - y)^2 \\big], $$ with the Bellman update target is \\(y = r + \\gamma \\, Q^{'}(s', a')\\) and the replay buffer is \\(\\mathcal{D}\\) . losses/q_values : implemented as qf1(data.observations, data.actions).view(-1) , it is the average Q values of the sampled data in the replay buffer; useful when gauging if under or over estimation happens. Implementation details dqn_atari.py is based on (Mnih et al., 2015) 1 but presents a few implementation differences: dqn_atari.py use slightly different hyperparameters. Specifically, dqn_atari.py uses the more popular Adam Optimizer with the --learning-rate=1e-4 as follows: optim . Adam ( q_network . parameters (), lr = 1e-4 ) whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses the RMSProp optimizer with --learning-rate=2.5e-4 , gradient momentum 0.95 , squared gradient momentum 0.95 , and min squared gradient 0.01 as follows: optim . RMSprop ( q_network . parameters (), lr = 2.5e-4 , momentum = 0.95 , # ... PyTorch's RMSprop does not directly support # squared gradient momentum and min squared gradient # so we are not sure what to put here. ) dqn_atari.py uses --learning-starts=80000 whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --learning-starts=50000 . dqn_atari.py uses --target-network-frequency=1000 whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --learning-starts=10000 . dqn_atari.py uses --total-timesteps=10000000 (i.e., 10M timesteps = 40M frames because of frame-skipping) whereas (Mnih et al., 2015) 1 uses --total-timesteps=50000000 (i.e., 50M timesteps = 200M frames) (See \"Training details\" under \"METHODS\" on page 6 and the related source code run_gpu#L32 , dqn/train_agent.lua#L81-L82 , and dqn/train_agent.lua#L165-L169 ). dqn_atari.py uses --end-e=0.01 (the final exploration epsilon) whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --end-e=0.1 . dqn_atari.py uses --exploration-fraction=0.1 whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --exploration-fraction=0.02 (all corresponds to 250000 steps or 1M frames being the frame that epsilon is annealed to --end-e=0.1 ). dqn_atari.py handles truncation and termination properly like (Mnih et al., 2015) 1 by using SB3's replay buffer's handle_timeout_termination=True . dqn_atari.py use a self-contained evaluation scheme: dqn_atari.py reports the episodic returns obtained throughout training, whereas (Mnih et al., 2015) 1 is trained with --end-e=0.1 but reported episodic returns using a separate evaluation process with --end-e=0.01 (See \"Evaluation procedure\" under \"METHODS\" on page 6). Experiment results To run benchmark experiments, see benchmark/c51.sh . Specifically, execute the following command: Below are the average episodic returns for dqn_atari.py . Environment dqn_atari.py 10M steps (Mnih et al., 2015) 1 50M steps (Hessel et al., 2017, Figure 5) 3 BreakoutNoFrameskip-v4 366.928 \u00b1 39.89 401.2 \u00b1 26.9 ~230 at 10M steps, ~300 at 50M steps PongNoFrameskip-v4 20.25 \u00b1 0.41 18.9 \u00b1 1.3 ~20 10M steps, ~20 at 50M steps BeamRiderNoFrameskip-v4 6673.24 \u00b1 1434.37 6846 \u00b1 1619 ~6000 10M steps, ~7000 at 50M steps Note that we save computational time by reducing timesteps from 50M to 10M, but our dqn_atari.py scores the same or higher than (Mnih et al., 2015) 1 in 10M steps. Learning curves: Tracked experiments and game play videos: dqn.py The dqn.py has the following features: Works with the Box observation space of low-level features Works with the Discrete action space Works with envs like CartPole-v1 Usage python cleanrl/dqn.py --env-id CartPole-v1 Explanation of the logged metrics See related docs for dqn_atari.py . Implementation details The dqn.py shares the same implementation details as dqn_atari.py except the dqn.py runs with different hyperparameters and neural network architecture. Specifically, dqn.py uses a simpler neural network as follows: self . network = nn . Sequential ( nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 120 ), nn . ReLU (), nn . Linear ( 120 , 84 ), nn . ReLU (), nn . Linear ( 84 , env . single_action_space . n ), ) dqn.py runs with different hyperparameters: python dqn.py --total-timesteps 500000 \\ --learning-rate 2 .5e-4 \\ --buffer-size 10000 \\ --gamma 0 .99 \\ --target-network-frequency 500 \\ --max-grad-norm 0 .5 \\ --batch-size 128 \\ --start-e 1 \\ --end-e 0 .05 \\ --exploration-fraction 0 .5 \\ --learning-starts 10000 \\ --train-frequency 10 Experiment results To run benchmark experiments, see benchmark/c51.sh . Specifically, execute the following command: Below are the average episodic returns for dqn.py . Environment dqn.py CartPole-v1 488.69 \u00b1 16.11 Acrobot-v1 -91.54 \u00b1 7.20 MountainCar-v0 -194.95 \u00b1 8.48 Note that the DQN has no official benchmark on classic control environments, so we did not include a comparison. That said, our dqn.py was able to achieve near perfect scores in CartPole-v1 and Acrobot-v1 ; further, it can obtain successful runs in the sparse environment MountainCar-v0 . Learning curves: Tracked experiments and game play videos: Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control through deep reinforcement learning. Nature 518, 529\u2013533 (2015). https://doi.org/10.1038/nature14236 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 [Proposal] Formal API handling of truncation vs termination. https://github.com/openai/gym/issues/2510 \u21a9 Hessel, M., Modayil, J., Hasselt, H.V., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M.G., & Silver, D. (2018). Rainbow: Combining Improvements in Deep Reinforcement Learning. AAAI. \u21a9","title":"Deep Q-Learning (DQN)"},{"location":"rl-algorithms/dqn/#deep-q-learning-dqn","text":"","title":"Deep Q-Learning (DQN)"},{"location":"rl-algorithms/dqn/#overview","text":"As an extension of the Q-learning, DQN's main technical contribution is the use of replay buffer and target network, both of which would help improve the stability of the algorithm. Original papers: Human-level control through deep reinforcement learning","title":"Overview"},{"location":"rl-algorithms/dqn/#implemented-variants","text":"Variants Implemented Description dqn_atari.py , docs For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. dqn.py , docs For classic control tasks like CartPole-v1 . Below are our single-file implementations of DQN:","title":"Implemented Variants"},{"location":"rl-algorithms/dqn/#dqn_ataripy","text":"The dqn_atari.py has the following features: For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space","title":"dqn_atari.py"},{"location":"rl-algorithms/dqn/#usage","text":"poetry install -E atari python cleanrl/dqn_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/dqn_atari.py --env-id PongNoFrameskip-v4","title":"Usage"},{"location":"rl-algorithms/dqn/#explanation-of-the-logged-metrics","text":"Running python cleanrl/dqn_atari.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/td_loss : the mean squared error (MSE) between the Q values at timestep \\(t\\) and the Bellman update target estimated using the reward \\(r_t\\) and the Q values at timestep \\(t+1\\) , thus minimizing the one-step temporal difference. Formally, it can be expressed by the equation below. $$ J(\\theta^{Q}) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\big[ (Q(s, a) - y)^2 \\big], $$ with the Bellman update target is \\(y = r + \\gamma \\, Q^{'}(s', a')\\) and the replay buffer is \\(\\mathcal{D}\\) . losses/q_values : implemented as qf1(data.observations, data.actions).view(-1) , it is the average Q values of the sampled data in the replay buffer; useful when gauging if under or over estimation happens.","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/dqn/#implementation-details","text":"dqn_atari.py is based on (Mnih et al., 2015) 1 but presents a few implementation differences: dqn_atari.py use slightly different hyperparameters. Specifically, dqn_atari.py uses the more popular Adam Optimizer with the --learning-rate=1e-4 as follows: optim . Adam ( q_network . parameters (), lr = 1e-4 ) whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses the RMSProp optimizer with --learning-rate=2.5e-4 , gradient momentum 0.95 , squared gradient momentum 0.95 , and min squared gradient 0.01 as follows: optim . RMSprop ( q_network . parameters (), lr = 2.5e-4 , momentum = 0.95 , # ... PyTorch's RMSprop does not directly support # squared gradient momentum and min squared gradient # so we are not sure what to put here. ) dqn_atari.py uses --learning-starts=80000 whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --learning-starts=50000 . dqn_atari.py uses --target-network-frequency=1000 whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --learning-starts=10000 . dqn_atari.py uses --total-timesteps=10000000 (i.e., 10M timesteps = 40M frames because of frame-skipping) whereas (Mnih et al., 2015) 1 uses --total-timesteps=50000000 (i.e., 50M timesteps = 200M frames) (See \"Training details\" under \"METHODS\" on page 6 and the related source code run_gpu#L32 , dqn/train_agent.lua#L81-L82 , and dqn/train_agent.lua#L165-L169 ). dqn_atari.py uses --end-e=0.01 (the final exploration epsilon) whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --end-e=0.1 . dqn_atari.py uses --exploration-fraction=0.1 whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --exploration-fraction=0.02 (all corresponds to 250000 steps or 1M frames being the frame that epsilon is annealed to --end-e=0.1 ). dqn_atari.py handles truncation and termination properly like (Mnih et al., 2015) 1 by using SB3's replay buffer's handle_timeout_termination=True . dqn_atari.py use a self-contained evaluation scheme: dqn_atari.py reports the episodic returns obtained throughout training, whereas (Mnih et al., 2015) 1 is trained with --end-e=0.1 but reported episodic returns using a separate evaluation process with --end-e=0.01 (See \"Evaluation procedure\" under \"METHODS\" on page 6).","title":"Implementation details"},{"location":"rl-algorithms/dqn/#experiment-results","text":"To run benchmark experiments, see benchmark/c51.sh . Specifically, execute the following command: Below are the average episodic returns for dqn_atari.py . Environment dqn_atari.py 10M steps (Mnih et al., 2015) 1 50M steps (Hessel et al., 2017, Figure 5) 3 BreakoutNoFrameskip-v4 366.928 \u00b1 39.89 401.2 \u00b1 26.9 ~230 at 10M steps, ~300 at 50M steps PongNoFrameskip-v4 20.25 \u00b1 0.41 18.9 \u00b1 1.3 ~20 10M steps, ~20 at 50M steps BeamRiderNoFrameskip-v4 6673.24 \u00b1 1434.37 6846 \u00b1 1619 ~6000 10M steps, ~7000 at 50M steps Note that we save computational time by reducing timesteps from 50M to 10M, but our dqn_atari.py scores the same or higher than (Mnih et al., 2015) 1 in 10M steps. Learning curves: Tracked experiments and game play videos:","title":"Experiment results"},{"location":"rl-algorithms/dqn/#dqnpy","text":"The dqn.py has the following features: Works with the Box observation space of low-level features Works with the Discrete action space Works with envs like CartPole-v1","title":"dqn.py"},{"location":"rl-algorithms/dqn/#usage_1","text":"python cleanrl/dqn.py --env-id CartPole-v1","title":"Usage"},{"location":"rl-algorithms/dqn/#explanation-of-the-logged-metrics_1","text":"See related docs for dqn_atari.py .","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/dqn/#implementation-details_1","text":"The dqn.py shares the same implementation details as dqn_atari.py except the dqn.py runs with different hyperparameters and neural network architecture. Specifically, dqn.py uses a simpler neural network as follows: self . network = nn . Sequential ( nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 120 ), nn . ReLU (), nn . Linear ( 120 , 84 ), nn . ReLU (), nn . Linear ( 84 , env . single_action_space . n ), ) dqn.py runs with different hyperparameters: python dqn.py --total-timesteps 500000 \\ --learning-rate 2 .5e-4 \\ --buffer-size 10000 \\ --gamma 0 .99 \\ --target-network-frequency 500 \\ --max-grad-norm 0 .5 \\ --batch-size 128 \\ --start-e 1 \\ --end-e 0 .05 \\ --exploration-fraction 0 .5 \\ --learning-starts 10000 \\ --train-frequency 10","title":"Implementation details"},{"location":"rl-algorithms/dqn/#experiment-results_1","text":"To run benchmark experiments, see benchmark/c51.sh . Specifically, execute the following command: Below are the average episodic returns for dqn.py . Environment dqn.py CartPole-v1 488.69 \u00b1 16.11 Acrobot-v1 -91.54 \u00b1 7.20 MountainCar-v0 -194.95 \u00b1 8.48 Note that the DQN has no official benchmark on classic control environments, so we did not include a comparison. That said, our dqn.py was able to achieve near perfect scores in CartPole-v1 and Acrobot-v1 ; further, it can obtain successful runs in the sparse environment MountainCar-v0 . Learning curves: Tracked experiments and game play videos: Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control through deep reinforcement learning. Nature 518, 529\u2013533 (2015). https://doi.org/10.1038/nature14236 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 [Proposal] Formal API handling of truncation vs termination. https://github.com/openai/gym/issues/2510 \u21a9 Hessel, M., Modayil, J., Hasselt, H.V., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M.G., & Silver, D. (2018). Rainbow: Combining Improvements in Deep Reinforcement Learning. AAAI. \u21a9","title":"Experiment results"},{"location":"rl-algorithms/overview/","text":"Overview Algorithm Variants Implemented \u2705 Proximal Policy Gradient (PPO) ppo.py , docs ppo_atari.py , docs ppo_continuous_action.py , docs ppo_atari_lstm.py , docs ppo_atari_envpool.py , docs ppo_procgen.py , docs ppo_atari_multigpu.py , docs ppo_pettingzoo_ma_atari.py , docs \u2705 Deep Q-Learning (DQN) dqn.py , docs dqn_atari.py , docs \u2705 Categorical DQN (C51) c51.py , docs c51_atari.py , docs \u2705 Soft Actor-Critic (SAC) sac_continuous_action.py , docs \u2705 Deep Deterministic Policy Gradient (DDPG) ddpg_continuous_action.py , docs \u2705 Twin Delayed Deep Deterministic Policy Gradient (TD3) td3_continuous_action.py , docs \u2705 Phasic Policy Gradient (PPG) ppg_procgen.py , docs","title":"Overview"},{"location":"rl-algorithms/overview/#overview","text":"Algorithm Variants Implemented \u2705 Proximal Policy Gradient (PPO) ppo.py , docs ppo_atari.py , docs ppo_continuous_action.py , docs ppo_atari_lstm.py , docs ppo_atari_envpool.py , docs ppo_procgen.py , docs ppo_atari_multigpu.py , docs ppo_pettingzoo_ma_atari.py , docs \u2705 Deep Q-Learning (DQN) dqn.py , docs dqn_atari.py , docs \u2705 Categorical DQN (C51) c51.py , docs c51_atari.py , docs \u2705 Soft Actor-Critic (SAC) sac_continuous_action.py , docs \u2705 Deep Deterministic Policy Gradient (DDPG) ddpg_continuous_action.py , docs \u2705 Twin Delayed Deep Deterministic Policy Gradient (TD3) td3_continuous_action.py , docs \u2705 Phasic Policy Gradient (PPG) ppg_procgen.py , docs","title":"Overview"},{"location":"rl-algorithms/ppg/","text":"Phasic Policy Gradient (PPG) Overview PPG is a DRL algorithm that separates policy and value function training by introducing an auxiliary phase. The training proceeds by running PPO during the policy phase, saving all the experience in a replay buffer. Then the replay buffer is used to train the value function. This makes the algorithm considerably slower than PPO, but improves sample efficiency on Procgen benchmark. Original paper: Phasic Policy Gradient Reference resources: Code for the paper \"Phasic Policy Gradient\" - by original authors from OpenAI The original code has multiple code level details that are not mentioned in the paper. We found these changes to be important for reproducing the results claimed by the paper. Implemented Variants Variants Implemented Description ppg_procgen.py , docs For classic control tasks like CartPole-v1 . Below are our single-file implementations of PPG: ppg_procgen.py ppg_procgen.py works with the Procgen benchmark, which uses 64x64 RGB image observations, and discrete actions Usage poetry install -E procgen python cleanrl/ppg_procgen.py --help python cleanrl/ppg_procgen.py --env-id \"bigfish\" Explanation of the logged metrics Running python cleanrl/ppg_procgen.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: Same as PPO: charts/episodic_return : episodic return of the game charts/episodic_length : episodic length of the game charts/SPS : number of steps per second (this is initially high but drops off after the auxiliary phase) charts/learning_rate : the current learning rate (annealing is not done by default) losses/value_loss : the mean value loss across all data points losses/policy_loss : the mean policy loss across all data points losses/entropy : the mean entropy value across all data points losses/old_approx_kl : the approximate Kullback\u2013Leibler divergence, measured by (-logratio).mean() , which corresponds to the k1 estimator in John Schulman\u2019s blog post on approximating KL losses/approx_kl : better alternative to olad_approx_kl measured by (logratio.exp() - 1) - logratio , which corresponds to the k3 estimator in approximating KL losses/clipfrac : the fraction of the training data that triggered the clipped objective losses/explained_variance : the explained variance for the value function PPG specific: losses/aux/kl_loss : the mean value of the KL divergence when distilling the latest policy during the auxiliary phase. losses/aux/aux_value_loss : the mean value loss on the auxiliary value head losses/aux/real_value_loss : the mean value loss on the detached value head used to calculate the GAE returns during policy phase Implementation details ppg_procgen.py includes the level implementation details that are different from PPO: Full rollout sampling during auxiliary phase - ( phasic_policy_gradient/ppg.py#L173 ) - Instead of randomly sampling observations over the entire auxiliary buffer, PPG samples full rullouts from the buffer (Sets of 256 steps). This full rollout sampling is only done during the auxiliary phase. Note that the rollouts will still be at random starting points because PPO truncates the rollouts per env. This change gives a decent performance boost. Batch level advantage normalization - PPG normalizes the full batch of advantage values before PPO updates instead of advantage normalization on each minibatch. ( phasic_policy_gradient/ppo.py#L70 ) Normalized network initialization - ( phasic_policy_gradient/impala_cnn.py#L64 ) - PPG uses normalized initialization for all layers, with different scales. Original PPO used orthogonal initialization of only the Policy head and Value heads with scale of 0.01 and 1. respectively. For PPG All weights are initialized with the default torch initialization (Kaiming Uniform) Each layer\u2019s weights are divided by the L2 norm of the weights such that the weights of input_channels axis are individually normalized (axis 1 for linear layers and 1,2,3 for convolutional layers). Then the weights are multiplied by a scale factor. Scale factors for different layers Value head, Policy head, Auxiliary value head - 0.1 Fully connected layer after last conv later - 1.4 Convolutional layers - Approximately 0.638 The Adam Optimizer's Epsilon Parameter -( phasic_policy_gradient/ppg.py#L239 ) - Set to torch default of 1e-8 instead of 1e-5 which is used in PPO. Use the same gamma parameter in the NormalizeReward wrapper. Note that the original implementation from openai/train-procgen uses the default gamma=0.99 in the VecNormalize wrapper but gamma=0.999 as PPO's parameter. The mismatch between the gamma s is technically incorrect. See #209 Here are some additional notes: All the default hyperparameters from the original PPG implementation are used. Except setting 64 for the number of environments. The original PPG paper does not report results on easy environments, hence more hyperparameter tuning can give better results. Skipping every alternate auxiliary phase gives similar performance on easy environments while saving compute. Normalized network initialization scheme seems to matter a lot, but using layernorm with orthogonal initialization also works. Using mixed precision for auxiliary phase also works well to save compute, but using on policy phase makes training unstable. Also, ppg_procgen.py differs from the original openai/phasic-policy-gradient implementation in the following ways. The original PPG code supports LSTM whereas the CleanRL code does not. The original PPG code uses separate optimizers for policy and auxiliary phase, but we do not implement this as we found it to not make too much difference. The original PPG code utilizes multiple GPUs but our implementation does not Experiment results To run benchmark experiments, see benchmark/ppg.sh . Specifically, execute the following command: Below are the average episodic returns for ppg_procgen.py , and comparison with ppg_procgen.py on 25M timesteps. Environment ppg_procgen.py ppo_procgen.py openai/phasic-policy-gradient (easy) Starpilot (easy) 34.82 \u00b1 13.77 32.47 \u00b1 11.21 42.01 \u00b1 9.59 Bossfight (easy) 10.78 \u00b1 1.90 9.63 \u00b1 2.35 10.71 \u00b1 2.05 Bigfish (easy) 24.23 \u00b1 10.73 16.80 \u00b1 9.49 15.94 \u00b1 10.80 Warning Note that we have run the procgen experiments using the easy distribution for reducing the computational cost. However, the original paper's results were condcuted with the hard distribution mode. For convenience, in the learning curves below, we compared the performance of the original code base ( openai/phasic-policy-gradient the purple curve) in the easy distribution. Learning curves: Info Also note that our ppo_procgen.py which closely matches implementation details of openai/baselines ' PPO which might not be the same as openai/phasic-policy-gradient 's PPO. We take the reported results from (Cobbe et al., 2020) 1 and (Cobbe et al., 2021) 2 and compared them in a google sheet (screenshot shown below). As shown, the performance seems to diverge a bit. We also note that (Cobbe et al., 2020) 1 used procgen==0.9.2 and (Cobbe et al., 2021) 2 used procgen==0.10.4 , which also could cause performance difference. It is for this reason, we ran our own openai/phasic-policy-gradient experiments on the easy distribution for comparison, but this does mean it's challenging to compare our results against those in the original PPG paper (Cobbe et al., 2021) 2 . Tracked experiments and game play videos: Cobbe, K., Hesse, C., Hilton, J., & Schulman, J. (2020, November). Leveraging procedural generation to benchmark reinforcement learning. In International conference on machine learning (pp. 2048-2056). PMLR. \u21a9 \u21a9 Cobbe, K. W., Hilton, J., Klimov, O., & Schulman, J. (2021, July). Phasic policy gradient. In International Conference on Machine Learning (pp. 2020-2027). PMLR. \u21a9 \u21a9 \u21a9","title":"Phasic Policy Gradient (PPG)"},{"location":"rl-algorithms/ppg/#phasic-policy-gradient-ppg","text":"","title":"Phasic Policy Gradient (PPG)"},{"location":"rl-algorithms/ppg/#overview","text":"PPG is a DRL algorithm that separates policy and value function training by introducing an auxiliary phase. The training proceeds by running PPO during the policy phase, saving all the experience in a replay buffer. Then the replay buffer is used to train the value function. This makes the algorithm considerably slower than PPO, but improves sample efficiency on Procgen benchmark. Original paper: Phasic Policy Gradient Reference resources: Code for the paper \"Phasic Policy Gradient\" - by original authors from OpenAI The original code has multiple code level details that are not mentioned in the paper. We found these changes to be important for reproducing the results claimed by the paper.","title":"Overview"},{"location":"rl-algorithms/ppg/#implemented-variants","text":"Variants Implemented Description ppg_procgen.py , docs For classic control tasks like CartPole-v1 . Below are our single-file implementations of PPG:","title":"Implemented Variants"},{"location":"rl-algorithms/ppg/#ppg_procgenpy","text":"ppg_procgen.py works with the Procgen benchmark, which uses 64x64 RGB image observations, and discrete actions","title":"ppg_procgen.py"},{"location":"rl-algorithms/ppg/#usage","text":"poetry install -E procgen python cleanrl/ppg_procgen.py --help python cleanrl/ppg_procgen.py --env-id \"bigfish\"","title":"Usage"},{"location":"rl-algorithms/ppg/#explanation-of-the-logged-metrics","text":"Running python cleanrl/ppg_procgen.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: Same as PPO: charts/episodic_return : episodic return of the game charts/episodic_length : episodic length of the game charts/SPS : number of steps per second (this is initially high but drops off after the auxiliary phase) charts/learning_rate : the current learning rate (annealing is not done by default) losses/value_loss : the mean value loss across all data points losses/policy_loss : the mean policy loss across all data points losses/entropy : the mean entropy value across all data points losses/old_approx_kl : the approximate Kullback\u2013Leibler divergence, measured by (-logratio).mean() , which corresponds to the k1 estimator in John Schulman\u2019s blog post on approximating KL losses/approx_kl : better alternative to olad_approx_kl measured by (logratio.exp() - 1) - logratio , which corresponds to the k3 estimator in approximating KL losses/clipfrac : the fraction of the training data that triggered the clipped objective losses/explained_variance : the explained variance for the value function PPG specific: losses/aux/kl_loss : the mean value of the KL divergence when distilling the latest policy during the auxiliary phase. losses/aux/aux_value_loss : the mean value loss on the auxiliary value head losses/aux/real_value_loss : the mean value loss on the detached value head used to calculate the GAE returns during policy phase","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/ppg/#implementation-details","text":"ppg_procgen.py includes the level implementation details that are different from PPO: Full rollout sampling during auxiliary phase - ( phasic_policy_gradient/ppg.py#L173 ) - Instead of randomly sampling observations over the entire auxiliary buffer, PPG samples full rullouts from the buffer (Sets of 256 steps). This full rollout sampling is only done during the auxiliary phase. Note that the rollouts will still be at random starting points because PPO truncates the rollouts per env. This change gives a decent performance boost. Batch level advantage normalization - PPG normalizes the full batch of advantage values before PPO updates instead of advantage normalization on each minibatch. ( phasic_policy_gradient/ppo.py#L70 ) Normalized network initialization - ( phasic_policy_gradient/impala_cnn.py#L64 ) - PPG uses normalized initialization for all layers, with different scales. Original PPO used orthogonal initialization of only the Policy head and Value heads with scale of 0.01 and 1. respectively. For PPG All weights are initialized with the default torch initialization (Kaiming Uniform) Each layer\u2019s weights are divided by the L2 norm of the weights such that the weights of input_channels axis are individually normalized (axis 1 for linear layers and 1,2,3 for convolutional layers). Then the weights are multiplied by a scale factor. Scale factors for different layers Value head, Policy head, Auxiliary value head - 0.1 Fully connected layer after last conv later - 1.4 Convolutional layers - Approximately 0.638 The Adam Optimizer's Epsilon Parameter -( phasic_policy_gradient/ppg.py#L239 ) - Set to torch default of 1e-8 instead of 1e-5 which is used in PPO. Use the same gamma parameter in the NormalizeReward wrapper. Note that the original implementation from openai/train-procgen uses the default gamma=0.99 in the VecNormalize wrapper but gamma=0.999 as PPO's parameter. The mismatch between the gamma s is technically incorrect. See #209 Here are some additional notes: All the default hyperparameters from the original PPG implementation are used. Except setting 64 for the number of environments. The original PPG paper does not report results on easy environments, hence more hyperparameter tuning can give better results. Skipping every alternate auxiliary phase gives similar performance on easy environments while saving compute. Normalized network initialization scheme seems to matter a lot, but using layernorm with orthogonal initialization also works. Using mixed precision for auxiliary phase also works well to save compute, but using on policy phase makes training unstable. Also, ppg_procgen.py differs from the original openai/phasic-policy-gradient implementation in the following ways. The original PPG code supports LSTM whereas the CleanRL code does not. The original PPG code uses separate optimizers for policy and auxiliary phase, but we do not implement this as we found it to not make too much difference. The original PPG code utilizes multiple GPUs but our implementation does not","title":"Implementation details"},{"location":"rl-algorithms/ppg/#experiment-results","text":"To run benchmark experiments, see benchmark/ppg.sh . Specifically, execute the following command: Below are the average episodic returns for ppg_procgen.py , and comparison with ppg_procgen.py on 25M timesteps. Environment ppg_procgen.py ppo_procgen.py openai/phasic-policy-gradient (easy) Starpilot (easy) 34.82 \u00b1 13.77 32.47 \u00b1 11.21 42.01 \u00b1 9.59 Bossfight (easy) 10.78 \u00b1 1.90 9.63 \u00b1 2.35 10.71 \u00b1 2.05 Bigfish (easy) 24.23 \u00b1 10.73 16.80 \u00b1 9.49 15.94 \u00b1 10.80 Warning Note that we have run the procgen experiments using the easy distribution for reducing the computational cost. However, the original paper's results were condcuted with the hard distribution mode. For convenience, in the learning curves below, we compared the performance of the original code base ( openai/phasic-policy-gradient the purple curve) in the easy distribution. Learning curves: Info Also note that our ppo_procgen.py which closely matches implementation details of openai/baselines ' PPO which might not be the same as openai/phasic-policy-gradient 's PPO. We take the reported results from (Cobbe et al., 2020) 1 and (Cobbe et al., 2021) 2 and compared them in a google sheet (screenshot shown below). As shown, the performance seems to diverge a bit. We also note that (Cobbe et al., 2020) 1 used procgen==0.9.2 and (Cobbe et al., 2021) 2 used procgen==0.10.4 , which also could cause performance difference. It is for this reason, we ran our own openai/phasic-policy-gradient experiments on the easy distribution for comparison, but this does mean it's challenging to compare our results against those in the original PPG paper (Cobbe et al., 2021) 2 . Tracked experiments and game play videos: Cobbe, K., Hesse, C., Hilton, J., & Schulman, J. (2020, November). Leveraging procedural generation to benchmark reinforcement learning. In International conference on machine learning (pp. 2048-2056). PMLR. \u21a9 \u21a9 Cobbe, K. W., Hilton, J., Klimov, O., & Schulman, J. (2021, July). Phasic policy gradient. In International Conference on Machine Learning (pp. 2020-2027). PMLR. \u21a9 \u21a9 \u21a9","title":"Experiment results"},{"location":"rl-algorithms/ppo/","text":"Proximal Policy Gradient (PPO) Overview PPO is one of the most popular DRL algorithms. It runs reasonably fast by leveraging vector (parallel) environments and naturally works well with different action spaces, therefore supporting a variety of games. It also has good sample efficiency compared to algorithms such as DQN. Original paper: Proximal Policy Optimization Algorithms Reference resources: Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study \u2b50 The 37 Implementation Details of Proximal Policy Optimization All our PPO implementations below are augmented with the same code-level optimizations presented in openai/baselines 's PPO . To achieve this, see how we matched the implementation details in our blog post The 37 Implementation Details of Proximal Policy Optimization . Implemented Variants Variants Implemented Description ppo.py , docs For classic control tasks like CartPole-v1 . ppo_atari.py , docs For Atari games. It uses convolutional layers and common atari-based pre-processing techniques. ppo_continuous_action.py , docs For continuous action space. Also implemented Mujoco-specific code-level optimizations ppo_atari_lstm.py , docs For Atari games using LSTM without stacked frames. ppo_atari_envpool.py , docs Uses the blazing fast Envpool Atari vectorized environment. ppo_procgen.py , docs For the procgen environments ppo_atari_multigpu.py , docs For Atari environments leveraging multi-GPUs ppo_pettingzoo_ma_atari.py , docs For Pettingzoo's multi-agent Atari environments Below are our single-file implementations of PPO: ppo.py The ppo.py has the following features: Works with the Box observation space of low-level features Works with the Discrete action space Works with envs like CartPole-v1 Usage poetry install python cleanrl/ppo.py --help python cleanrl/ppo.py --env-id CartPole-v1 Explanation of the logged metrics Running python cleanrl/ppo.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : episodic return of the game charts/episodic_length : episodic length of the game charts/SPS : number of steps per second charts/learning_rate : the current learning rate losses/value_loss : the mean value loss across all data points losses/policy_loss : the mean policy loss across all data points losses/entropy : the mean entropy value across all data points losses/old_approx_kl : the approximate Kullback\u2013Leibler divergence, measured by (-logratio).mean() , which corresponds to the k1 estimator in John Schulman\u2019s blog post on approximating KL losses/approx_kl : better alternative to olad_approx_kl measured by (logratio.exp() - 1) - logratio , which corresponds to the k3 estimator in approximating KL losses/clipfrac : the fraction of the training data that triggered the clipped objective losses/explained_variance : the explained variance for the value function Implementation details ppo.py is based on the \"13 core implementation details\" in The 37 Implementation Details of Proximal Policy Optimization , which are as follows: Vectorized architecture ( common/cmd_util.py#L22 ) Orthogonal Initialization of Weights and Constant Initialization of biases ( a2c/utils.py#L58) ) The Adam Optimizer's Epsilon Parameter ( ppo2/model.py#L100 ) Adam Learning Rate Annealing ( ppo2/ppo2.py#L133-L135 ) Generalized Advantage Estimation ( ppo2/runner.py#L56-L65 ) Mini-batch Updates ( ppo2/ppo2.py#L157-L166 ) Normalization of Advantages ( ppo2/model.py#L139 ) Clipped surrogate objective ( ppo2/model.py#L81-L86 ) Value Function Loss Clipping ( ppo2/model.py#L68-L75 ) Overall Loss and Entropy Bonus ( ppo2/model.py#L91 ) Global Gradient Clipping ( ppo2/model.py#L102-L108 ) Debug variables ( ppo2/model.py#L115-L116 ) Separate MLP networks for policy and value functions ( common/policies.py#L156-L160 , baselines/common/models.py#L75-L103 ) Experiment results To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: Below are the average episodic returns for ppo.py . To ensure the quality of the implementation, we compared the results against openai/baselies ' PPO. Environment ppo.py openai/baselies ' PPO (Huang et al., 2022) 1 CartPole-v1 492.40 \u00b1 13.05 497.54 \u00b1 4.02 Acrobot-v1 -89.93 \u00b1 6.34 -81.82 \u00b1 5.58 MountainCar-v0 -200.00 \u00b1 0.00 -200.00 \u00b1 0.00 Learning curves: Tracked experiments and game play videos: Video tutorial If you'd like to learn ppo.py in-depth, consider checking out the following video tutorial: ppo_atari.py The ppo_atari.py has the following features: For Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space Usage poetry install -E atari python cleanrl/ppo_atari.py --help python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 Explanation of the logged metrics See related docs for ppo.py . Implementation details ppo_atari.py is based on the \"9 Atari implementation details\" in The 37 Implementation Details of Proximal Policy Optimization , which are as follows: The Use of NoopResetEnv ( common/atari_wrappers.py#L12 ) The Use of MaxAndSkipEnv ( common/atari_wrappers.py#L97 ) The Use of EpisodicLifeEnv ( common/atari_wrappers.py#L61 ) The Use of FireResetEnv ( common/atari_wrappers.py#L41 ) The Use of WarpFrame (Image transformation) common/atari_wrappers.py#L134 The Use of ClipRewardEnv ( common/atari_wrappers.py#L125 ) The Use of FrameStack ( common/atari_wrappers.py#L188 ) Shared Nature-CNN network for the policy and value functions ( common/policies.py#L157 , common/models.py#L15-L26 ) Scaling the Images to Range [0, 1] ( common/models.py#L19 ) Experiment results To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: Below are the average episodic returns for ppo_atari.py . To ensure the quality of the implementation, we compared the results against openai/baselies ' PPO. Environment ppo_atari.py openai/baselies ' PPO (Huang et al., 2022) 1 BreakoutNoFrameskip-v4 416.31 \u00b1 43.92 406.57 \u00b1 31.554 PongNoFrameskip-v4 20.59 \u00b1 0.35 20.512 \u00b1 0.50 BeamRiderNoFrameskip-v4 2445.38 \u00b1 528.91 2642.97 \u00b1 670.37 Learning curves: Tracked experiments and game play videos: Video tutorial If you'd like to learn ppo_atari.py in-depth, consider checking out the following video tutorial: ppo_continuous_action.py The ppo_continuous_action.py has the following features: For continuous action space. Also implemented Mujoco-specific code-level optimizations Works with the Box observation space of low-level features Works with the Box (continuous) action space Usage poetry install -E atari python cleanrl/ppo_continuous_action.py --help python cleanrl/ppo_continuous_action.py --env-id Hopper-v2 Explanation of the logged metrics See related docs for ppo.py . Implementation details ppo_continuous_action.py is based on the \"9 details for continuous action domains (e.g. Mujoco)\" in The 37 Implementation Details of Proximal Policy Optimization , which are as follows: Continuous actions via normal distributions ( common/distributions.py#L103-L104 ) State-independent log standard deviation ( common/distributions.py#L104 ) Independent action components ( common/distributions.py#L238-L246 ) Separate MLP networks for policy and value functions ( common/policies.py#L160 , baselines/common/models.py#L75-L103 Handling of action clipping to valid range and storage ( common/cmd_util.py#L99-L100 ) Normalization of Observation ( common/vec_env/vec_normalize.py#L4 ) Observation Clipping ( common/vec_env/vec_normalize.py#L39 ) Reward Scaling ( common/vec_env/vec_normalize.py#L28 ) Reward Clipping ( common/vec_env/vec_normalize.py#L32 ) Experiment results To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: Below are the average episodic returns for ppo_continuous_action.py . To ensure the quality of the implementation, we compared the results against openai/baselies ' PPO. Environment ppo_continuous_action.py openai/baselies ' PPO (Huang et al., 2022) 1 Hopper-v2 2231.12 \u00b1 656.72 2518.95 \u00b1 850.46 Walker2d-v2 3050.09 \u00b1 1136.21 3208.08 \u00b1 1264.37 HalfCheetah-v2 1822.82 \u00b1 928.11 2152.26 \u00b1 1159.84 Learning curves: Tracked experiments and game play videos: Video tutorial If you'd like to learn ppo_continuous_action.py in-depth, consider checking out the following video tutorial: ppo_atari_lstm.py The ppo_atari_lstm.py has the following features: For Atari games using LSTM without stacked frames. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space Usage poetry install -E atari python cleanrl/ppo_atari_lstm.py --help python cleanrl/ppo_atari_lstm.py --env-id BreakoutNoFrameskip-v4 Explanation of the logged metrics See related docs for ppo.py . Implementation details ppo_atari_lstm.py is based on the \"5 LSTM implementation details\" in The 37 Implementation Details of Proximal Policy Optimization , which are as follows: Layer initialization for LSTM layers ( a2c/utils.py#L84-L86 ) Initialize the LSTM states to be zeros ( common/models.py#L179 ) Reset LSTM states at the end of the episode ( common/models.py#L141 ) Prepare sequential rollouts in mini-batches ( a2c/utils.py#L81 ) Reconstruct LSTM states during training ( a2c/utils.py#L81 ) To help test out the memory, we remove the 4 stacked frames from the observation (i.e., using env = gym.wrappers.FrameStack(env, 1) instead of env = gym.wrappers.FrameStack(env, 4) like in ppo_atari.py ) Experiment results To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: Below are the average episodic returns for ppo_atari_lstm.py . To ensure the quality of the implementation, we compared the results against openai/baselies ' PPO. Environment ppo_atari_lstm.py openai/baselies ' PPO (Huang et al., 2022) 1 BreakoutNoFrameskip-v4 128.92 \u00b1 31.10 138.98 \u00b1 50.76 PongNoFrameskip-v4 19.78 \u00b1 1.58 19.79 \u00b1 0.67 BeamRiderNoFrameskip-v4 1536.20 \u00b1 612.21 1591.68 \u00b1 372.95 Learning curves: Tracked experiments and game play videos: ppo_atari_envpool.py The ppo_atari_envpool.py has the following features: Uses the blazing fast Envpool vectorized environment. For Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space Warning Note that ppo_atari_envpool.py does not work in Windows and MacOs . See envpool's built wheels here: https://pypi.org/project/envpool/#files Usage poetry install -E envpool python cleanrl/ppo_atari_envpool.py --help python cleanrl/ppo_atari_envpool.py --env-id Breakout-v5 Explanation of the logged metrics See related docs for ppo.py . Implementation details ppo_atari_envpool.py uses a customized RecordEpisodeStatistics to work with envpool but has the same other implementation details as ppo_atari.py (see related docs ). Experiment results To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: Below are the average episodic returns for ppo_atari_envpool.py . Notice it has the same sample efficiency as ppo_atari.py , but runs about 3x faster. Environment ppo_atari_envpool.py (~80 mins) ppo_atari.py (~220 mins) BreakoutNoFrameskip-v4 389.57 \u00b1 29.62 416.31 \u00b1 43.92 PongNoFrameskip-v4 20.55 \u00b1 0.37 20.59 \u00b1 0.35 BeamRiderNoFrameskip-v4 2039.83 \u00b1 1146.62 2445.38 \u00b1 528.91 Learning curves: Tracked experiments and game play videos: ppo_procgen.py The ppo_procgen.py has the following features: For the procgen environments Uses IMPALA-style neural network Works with the Discrete action space Usage poetry install -E procgen python cleanrl/ppo_procgen.py --help python cleanrl/ppo_procgen.py --env-id starpilot Explanation of the logged metrics See related docs for ppo.py . Implementation details ppo_procgen.py is based on the details in \"Appendix\" in The 37 Implementation Details of Proximal Policy Optimization , which are as follows: IMPALA-style Neural Network ( common/models.py#L28 ) Use the same gamma parameter in the NormalizeReward wrapper. Note that the original implementation from openai/train-procgen uses the default gamma=0.99 in the VecNormalize wrapper but gamma=0.999 as PPO's parameter. The mismatch between the gamma s is technically incorrect. See #209 Experiment results To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: We try to match the default setting in openai/train-procgen except that we use the easy distribution mode and total_timesteps=25e6 to save compute. Notice openai/train-procgen has the following settings: Learning rate annealing is turned off by default Reward scaling and reward clipping is used Below are the average episodic returns for ppo_procgen.py . To ensure the quality of the implementation, we compared the results against openai/baselies ' PPO. Environment ppo_procgen.py openai/baselies ' PPO (Huang et al., 2022) 1 StarPilot (easy) 32.47 \u00b1 11.21 33.97 \u00b1 7.86 BossFight (easy) 9.63 \u00b1 2.35 9.35 \u00b1 2.04 BigFish (easy) 16.80 \u00b1 9.49 20.06 \u00b1 5.34 Info Note that we have run the procgen experiments using the easy distribution for reducing the computational cost. Learning curves: Tracked experiments and game play videos: ppo_atari_multigpu.py The ppo_atari_multigpu.py leverages data parallelism to speed up training time at no cost of sample efficiency . ppo_atari_multigpu.py has the following features: Allows the users to use do training leveraging data parallelism For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space Warning Note that ppo_atari_multigpu.py does not work in Windows and MacOs . It will error out with NOTE: Redirects are currently not supported in Windows or MacOs. See pytorch/pytorch#20380 Usage poetry install -E atari python cleanrl/ppo_atari_multigpu.py --help # `--nproc_per_node=2` specifies how many subprocesses we spawn for training with data parallelism # note it is possible to run this with a *single GPU*: each process will simply share the same GPU torchrun --standalone --nnodes = 1 --nproc_per_node = 2 cleanrl/ppo_atari_multigpu.py --env-id BreakoutNoFrameskip-v4 # by default we use the `gloo` backend, but you can use the `nccl` backend for better multi-GPU performance torchrun --standalone --nnodes = 1 --nproc_per_node = 2 cleanrl/ppo_atari_multigpu.py --env-id BreakoutNoFrameskip-v4 --backend nccl # it is possible to spawn more processes than the amount of GPUs you have via `--device-ids` # e.g., the command below spawns two processes using GPU 0 and two processes using GPU 1 torchrun --standalone --nnodes = 1 --nproc_per_node = 2 cleanrl/ppo_atari_multigpu.py --env-id BreakoutNoFrameskip-v4 --device-ids 0 0 1 1 Explanation of the logged metrics See related docs for ppo.py . Implementation details ppo_atari_multigpu.py is based on ppo_atari.py (see its related docs ). We use Pytorch's distributed API to implement the data parallelism paradigm. The basic idea is that the user can spawn \\(N\\) processes each holding a copy of the model, step the environments, and averages their gradients together for the backward pass. Here are a few note-worthy implementation details. Shard the environments : by default, ppo_atari_multigpu.py uses --num-envs=8 . When calling torchrun --standalone --nnodes=1 --nproc_per_node=2 cleanrl/ppo_atari_multigpu.py --env-id BreakoutNoFrameskip-v4 , it spawns \\(N=2\\) (by --nproc_per_node=2 ) subprocesses and shard the environments across these 2 subprocesses. In particular, each subprocess will have 8/2=4 environments. Implementation wise, we do args.num_envs = int(args.num_envs / world_size) . Here world_size=2 refers to the size of the world , which means the group of subprocesses. We also need to adjust various variables as follows: batch size : by default it is (num_envs * num_steps) = 8 * 128 = 1024 and we adjust it to (num_envs / world_size * num_steps) = (4 * 128) = 512 . minibatch size : by default it is (num_envs * num_steps) / num_minibatches = (8 * 128) / 4 = 256 and we adjust it to (num_envs / world_size * num_steps) / num_minibatches = (4 * 128) / 4 = 128 . number of updates : by default it is total_timesteps // batch_size = 10000000 // (8 * 128) = 9765 and we adjust it to total_timesteps // (batch_size * world_size) = 10000000 // (8 * 128 * 2) = 4882 . global step increment : by default it is num_envs and we adjust it to num_envs * world_size . Adjust seed per process : we need be very careful with seeding: we could have used the exact same seed for each subprocess. To ensure this does not happen, we do the following # CRUCIAL: note that we needed to pass a different seed for each data parallelism worker args . seed += local_rank random . seed ( args . seed ) np . random . seed ( args . seed ) torch . manual_seed ( args . seed - local_rank ) torch . backends . cudnn . deterministic = args . torch_deterministic # ... envs = gym . vector . SyncVectorEnv ( [ make_env ( args . env_id , args . seed + i , i , args . capture_video , run_name ) for i in range ( args . num_envs )] ) assert isinstance ( envs . single_action_space , gym . spaces . Discrete ), \"only discrete action space is supported\" agent = Agent ( envs ) . to ( device ) torch . manual_seed ( args . seed ) optimizer = optim . Adam ( agent . parameters (), lr = args . learning_rate , eps = 1e-5 ) Notice that we adjust the seed with args.seed += local_rank (line 2), where local_rank is the index of the subprocesses. This ensures we seed packages and envs with uncorrealted seeds. However, we do need to use the same torch seed for all process to initialize same weights for the agent (line 5), after which we can use a different seed for torch (line 16). 1. Efficient gradient averaging : PyTorch recommends to average the gradient across the whole world via the following (see docs ) for param in agent . parameters (): dist . all_reduce ( param . grad . data , op = dist . ReduceOp . SUM ) param . grad . data /= world_size However, @cswinter introduces a more efficient gradient averaging scheme with proper batching (see entity-neural-network/incubator#220 ), which looks like: all_grads_list = [] for param in agent . parameters (): if param . grad is not None : all_grads_list . append ( param . grad . view ( - 1 )) all_grads = torch . cat ( all_grads_list ) dist . all_reduce ( all_grads , op = dist . ReduceOp . SUM ) offset = 0 for param in agent . parameters (): if param . grad is not None : param . grad . data . copy_ ( all_grads [ offset : offset + param . numel ()] . view_as ( param . grad . data ) / world_size ) offset += param . numel () In our previous empirical testing (see vwxyzjn/cleanrl#162 ), we have found @cswinter 's implementation to be faster, hence we adopt it in our implementation. We can see how ppo_atari_multigpu.py can result in no loss of sample efficiency. In this example, the ppo_atari.py 's minibatch size is 256 and the ppo_atari_multigpu.py 's minibatch size is 128 with world size 2. Because we average gradient across the world, the gradient under ppo_atari_multigpu.py should be virtually the same as the gradient under ppo_atari.py . Experiment results To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: Below are the average episodic returns for ppo_atari_multigpu.py . To ensure no loss of sample efficiency, we compared the results against ppo_atari.py . Environment ppo_atari_multigpu.py (in ~160 mins) ppo_atari.py (in ~215 mins) BreakoutNoFrameskip-v4 429.06 \u00b1 52.09 416.31 \u00b1 43.92 PongNoFrameskip-v4 20.40 \u00b1 0.46 20.59 \u00b1 0.35 BeamRiderNoFrameskip-v4 2454.54 \u00b1 740.49 2445.38 \u00b1 528.91 Learning curves: Under the same hardware, we see that ppo_atari_multigpu.py is about 30% faster than ppo_atari.py with no loss of sample efficiency. Info Although ppo_atari_multigpu.py is 30% faster than ppo_atari.py , ppo_atari_multigpu.py is still slower than ppo_atari_envpool.py , as shown below. This comparison really highlights the different kinds of optimization possible. The purpose of ppo_atari_multigpu.py is not (yet) to achieve the fastest PPO + Atari example. Rather, its purpose is to rigorously validate data paralleism does provide performance benefits . We could do something like ppo_atari_multigpu_envpool.py to possibly obtain the fastest PPO + Atari possible, but that is for another day. Note we may need numba to pin the threads envpool is using in each subprocess to avoid threads fighting each other and lowering the throughput. Tracked experiments and game play videos: ppo_pettingzoo_ma_atari.py ppo_pettingzoo_ma_atari.py trains an agent to learn playing Atari games via selfplay. The selfplay environment is implemented as a vectorized environment from PettingZoo.ml . The basic idea is to create vectorized environment \\(E\\) with num_envs = N , where \\(N\\) is the number of players in the game. Say \\(N = 2\\) , then the 0-th sub environment of \\(E\\) will return the observation for player 0 and 1-th sub environment will return the observation of player 1. Then the two environments takes a batch of 2 actions and execute them for player 0 and player 1, respectively. See \"Vectorized architecture\" in The 37 Implementation Details of Proximal Policy Optimization for more detail. ppo_pettingzoo_ma_atari.py has the following features: For playing the pettingzoo's multi-agent Atari game. Works with the pixel-based observation space Works with the Box action space Warning Note that ppo_pettingzoo_ma_atari.py does not work in Windows . See https://pypi.org/project/multi-agent-ale-py/#files Usage poetry install -E \"pettingzoo atari\" poetry run AutoROM --accept-license python cleanrl/ppo_pettingzoo_ma_atari.py --help python cleanrl/ppo_pettingzoo_ma_atari.py --env-id pong_v3 python cleanrl/ppo_pettingzoo_ma_atari.py --env-id surround_v2 See https://www.pettingzoo.ml/atari for a full-list of supported environments such as basketball_pong_v3 . Notice pettingzoo sometimes introduces breaking changes, so make sure to install the pinned dependencies via poetry . Explanation of the logged metrics Additionally, it logs the following metrics charts/episodic_return-player0 : episodic return of the game for player 0 charts/episodic_return-player1 : episodic return of the game for player 1 charts/episodic_length-player0 : episodic length of the game for player 0 charts/episodic_length-player1 : episodic length of the game for player 1 See other logged metrics in the related docs for ppo.py . Implementation details ppo_pettingzoo_ma_atari.py is based on ppo_atari.py (see its related docs ). ppo_pettingzoo_ma_atari.py additionally has the following implementation details: supersuit wrappers : uses preprocessing wrappers from supersuit instead of from stable_baselines3 , which looks like the following. In particular note that the supersuit does not offer a wrapper similar to NoopResetEnv , and that it uses the agent_indicator_v0 to add two channels indicating the which player the agent controls. -env = gym.make(env_id) -env = NoopResetEnv(env, noop_max=30) -env = MaxAndSkipEnv(env, skip=4) -env = EpisodicLifeEnv(env) -if \"FIRE\" in env.unwrapped.get_action_meanings(): - env = FireResetEnv(env) -env = ClipRewardEnv(env) -env = gym.wrappers.ResizeObservation(env, (84, 84)) -env = gym.wrappers.GrayScaleObservation(env) -env = gym.wrappers.FrameStack(env, 4) +env = importlib.import_module(f\"pettingzoo.atari.{args.env_id}\").parallel_env() +env = ss.max_observation_v0(env, 2) +env = ss.frame_skip_v0(env, 4) +env = ss.clip_reward_v0(env, lower_bound=-1, upper_bound=1) +env = ss.color_reduction_v0(env, mode=\"B\") +env = ss.resize_v1(env, x_size=84, y_size=84) +env = ss.frame_stack_v1(env, 4) +env = ss.agent_indicator_v0(env, type_only=False) +env = ss.pettingzoo_env_to_vec_env_v1(env) +envs = ss.concat_vec_envs_v1(env, args.num_envs // 2, num_cpus=0, base_class=\"gym\") 1. A more detailed note on the agent_indicator_v0 wrapper : let's dig deeper into how agent_indicator_v0 works. We do print(envs.reset(), envs.reset().shape) [ 0. , 0. , 0. , 236. , 1 , 0. ]], [[ 0. , 0. , 0. , 236. , 0. , 1. ], [ 0. , 0. , 0. , 236. , 0. , 1. ], [ 0. , 0. , 0. , 236. , 0. , 1. ], ... , [ 0. , 0. , 0. , 236. , 0. , 1. ], [ 0. , 0. , 0. , 236. , 0. , 1. ], [ 0. , 0. , 0. , 236. , 0. , 1. ]]]]) torch . Size ([ 16 , 84 , 84 , 6 ]) So the agent_indicator_v0 adds the last two columns, where [ 0., 0., 0., 236., 1, 0.]] means this observation is for player 0, and [ 0., 0., 0., 236., 0., 1.] is for player 1. Notice the observation still has the range of \\([0, 255]\\) but the agent indicator channel has the range of \\([0,1]\\) , so we need to be careful when dividing the observation by 255. In particular, we would only divide the first four channels by 255 and leave the agent indicator channels untouched as follows: def get_action_and_value ( self , x , action = None ): x = x . clone () x [:, :, :, [ 0 , 1 , 2 , 3 ]] /= 255.0 hidden = self . network ( x . permute (( 0 , 3 , 1 , 2 ))) Experiment results To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: Info Note that evaluation is usually tricker in in selfplay environments. The usual episodic return is not a good indicator of the agent's performance in zero-sum games because the episodic return converges to zero. To evaluate the agent's ability, an intuitive approach is to take a look at the videos of the agents playing the game (included below), visually inspect the agent's behavior. The best scheme, however, is rating systems like Trueskill or ELO scores . However, they are more difficult to implement and are outside the scode of ppo_pettingzoo_ma_atari.py . For simplicity, we measure the episodic length instead, which in a sense measures how many \"back and forth\" the agent can create. In other words, the longer the agent can play the game, the better the agent can play. Empirically, we have found episodic length to be a good indicator of the agent's skill, especially in pong_v3 and surround_v2 . However, it is not the case for tennis_v3 and we'd need to visually inspect the agents' game play videos. Below are the average episodic length for ppo_pettingzoo_ma_atari.py . To ensure no loss of sample efficiency, we compared the results against ppo_atari.py . Environment ppo_pettingzoo_ma_atari.py pong_v3 4153.60 \u00b1 190.80 surround_v2 3055.33 \u00b1 223.68 tennis_v3 14538.02 \u00b1 7005.54 Learning curves: Tracked experiments and game play videos: Huang, Shengyi; Dossa, Rousslan Fernand Julien; Raffin, Antonin; Kanervisto, Anssi; Wang, Weixun (2022). The 37 Implementation Details of Proximal Policy Optimization. ICLR 2022 Blog Track https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/ \u21a9 \u21a9 \u21a9 \u21a9 \u21a9","title":"Proximal Policy Gradient (PPO)"},{"location":"rl-algorithms/ppo/#proximal-policy-gradient-ppo","text":"","title":"Proximal Policy Gradient (PPO)"},{"location":"rl-algorithms/ppo/#overview","text":"PPO is one of the most popular DRL algorithms. It runs reasonably fast by leveraging vector (parallel) environments and naturally works well with different action spaces, therefore supporting a variety of games. It also has good sample efficiency compared to algorithms such as DQN. Original paper: Proximal Policy Optimization Algorithms Reference resources: Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study \u2b50 The 37 Implementation Details of Proximal Policy Optimization All our PPO implementations below are augmented with the same code-level optimizations presented in openai/baselines 's PPO . To achieve this, see how we matched the implementation details in our blog post The 37 Implementation Details of Proximal Policy Optimization .","title":"Overview"},{"location":"rl-algorithms/ppo/#implemented-variants","text":"Variants Implemented Description ppo.py , docs For classic control tasks like CartPole-v1 . ppo_atari.py , docs For Atari games. It uses convolutional layers and common atari-based pre-processing techniques. ppo_continuous_action.py , docs For continuous action space. Also implemented Mujoco-specific code-level optimizations ppo_atari_lstm.py , docs For Atari games using LSTM without stacked frames. ppo_atari_envpool.py , docs Uses the blazing fast Envpool Atari vectorized environment. ppo_procgen.py , docs For the procgen environments ppo_atari_multigpu.py , docs For Atari environments leveraging multi-GPUs ppo_pettingzoo_ma_atari.py , docs For Pettingzoo's multi-agent Atari environments Below are our single-file implementations of PPO:","title":"Implemented Variants"},{"location":"rl-algorithms/ppo/#ppopy","text":"The ppo.py has the following features: Works with the Box observation space of low-level features Works with the Discrete action space Works with envs like CartPole-v1","title":"ppo.py"},{"location":"rl-algorithms/ppo/#usage","text":"poetry install python cleanrl/ppo.py --help python cleanrl/ppo.py --env-id CartPole-v1","title":"Usage"},{"location":"rl-algorithms/ppo/#explanation-of-the-logged-metrics","text":"Running python cleanrl/ppo.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : episodic return of the game charts/episodic_length : episodic length of the game charts/SPS : number of steps per second charts/learning_rate : the current learning rate losses/value_loss : the mean value loss across all data points losses/policy_loss : the mean policy loss across all data points losses/entropy : the mean entropy value across all data points losses/old_approx_kl : the approximate Kullback\u2013Leibler divergence, measured by (-logratio).mean() , which corresponds to the k1 estimator in John Schulman\u2019s blog post on approximating KL losses/approx_kl : better alternative to olad_approx_kl measured by (logratio.exp() - 1) - logratio , which corresponds to the k3 estimator in approximating KL losses/clipfrac : the fraction of the training data that triggered the clipped objective losses/explained_variance : the explained variance for the value function","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/ppo/#implementation-details","text":"ppo.py is based on the \"13 core implementation details\" in The 37 Implementation Details of Proximal Policy Optimization , which are as follows: Vectorized architecture ( common/cmd_util.py#L22 ) Orthogonal Initialization of Weights and Constant Initialization of biases ( a2c/utils.py#L58) ) The Adam Optimizer's Epsilon Parameter ( ppo2/model.py#L100 ) Adam Learning Rate Annealing ( ppo2/ppo2.py#L133-L135 ) Generalized Advantage Estimation ( ppo2/runner.py#L56-L65 ) Mini-batch Updates ( ppo2/ppo2.py#L157-L166 ) Normalization of Advantages ( ppo2/model.py#L139 ) Clipped surrogate objective ( ppo2/model.py#L81-L86 ) Value Function Loss Clipping ( ppo2/model.py#L68-L75 ) Overall Loss and Entropy Bonus ( ppo2/model.py#L91 ) Global Gradient Clipping ( ppo2/model.py#L102-L108 ) Debug variables ( ppo2/model.py#L115-L116 ) Separate MLP networks for policy and value functions ( common/policies.py#L156-L160 , baselines/common/models.py#L75-L103 )","title":"Implementation details"},{"location":"rl-algorithms/ppo/#experiment-results","text":"To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: Below are the average episodic returns for ppo.py . To ensure the quality of the implementation, we compared the results against openai/baselies ' PPO. Environment ppo.py openai/baselies ' PPO (Huang et al., 2022) 1 CartPole-v1 492.40 \u00b1 13.05 497.54 \u00b1 4.02 Acrobot-v1 -89.93 \u00b1 6.34 -81.82 \u00b1 5.58 MountainCar-v0 -200.00 \u00b1 0.00 -200.00 \u00b1 0.00 Learning curves: Tracked experiments and game play videos:","title":"Experiment results"},{"location":"rl-algorithms/ppo/#video-tutorial","text":"If you'd like to learn ppo.py in-depth, consider checking out the following video tutorial:","title":"Video tutorial"},{"location":"rl-algorithms/ppo/#ppo_ataripy","text":"The ppo_atari.py has the following features: For Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space","title":"ppo_atari.py"},{"location":"rl-algorithms/ppo/#usage_1","text":"poetry install -E atari python cleanrl/ppo_atari.py --help python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4","title":"Usage"},{"location":"rl-algorithms/ppo/#explanation-of-the-logged-metrics_1","text":"See related docs for ppo.py .","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/ppo/#implementation-details_1","text":"ppo_atari.py is based on the \"9 Atari implementation details\" in The 37 Implementation Details of Proximal Policy Optimization , which are as follows: The Use of NoopResetEnv ( common/atari_wrappers.py#L12 ) The Use of MaxAndSkipEnv ( common/atari_wrappers.py#L97 ) The Use of EpisodicLifeEnv ( common/atari_wrappers.py#L61 ) The Use of FireResetEnv ( common/atari_wrappers.py#L41 ) The Use of WarpFrame (Image transformation) common/atari_wrappers.py#L134 The Use of ClipRewardEnv ( common/atari_wrappers.py#L125 ) The Use of FrameStack ( common/atari_wrappers.py#L188 ) Shared Nature-CNN network for the policy and value functions ( common/policies.py#L157 , common/models.py#L15-L26 ) Scaling the Images to Range [0, 1] ( common/models.py#L19 )","title":"Implementation details"},{"location":"rl-algorithms/ppo/#experiment-results_1","text":"To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: Below are the average episodic returns for ppo_atari.py . To ensure the quality of the implementation, we compared the results against openai/baselies ' PPO. Environment ppo_atari.py openai/baselies ' PPO (Huang et al., 2022) 1 BreakoutNoFrameskip-v4 416.31 \u00b1 43.92 406.57 \u00b1 31.554 PongNoFrameskip-v4 20.59 \u00b1 0.35 20.512 \u00b1 0.50 BeamRiderNoFrameskip-v4 2445.38 \u00b1 528.91 2642.97 \u00b1 670.37 Learning curves: Tracked experiments and game play videos:","title":"Experiment results"},{"location":"rl-algorithms/ppo/#video-tutorial_1","text":"If you'd like to learn ppo_atari.py in-depth, consider checking out the following video tutorial:","title":"Video tutorial"},{"location":"rl-algorithms/ppo/#ppo_continuous_actionpy","text":"The ppo_continuous_action.py has the following features: For continuous action space. Also implemented Mujoco-specific code-level optimizations Works with the Box observation space of low-level features Works with the Box (continuous) action space","title":"ppo_continuous_action.py"},{"location":"rl-algorithms/ppo/#usage_2","text":"poetry install -E atari python cleanrl/ppo_continuous_action.py --help python cleanrl/ppo_continuous_action.py --env-id Hopper-v2","title":"Usage"},{"location":"rl-algorithms/ppo/#explanation-of-the-logged-metrics_2","text":"See related docs for ppo.py .","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/ppo/#implementation-details_2","text":"ppo_continuous_action.py is based on the \"9 details for continuous action domains (e.g. Mujoco)\" in The 37 Implementation Details of Proximal Policy Optimization , which are as follows: Continuous actions via normal distributions ( common/distributions.py#L103-L104 ) State-independent log standard deviation ( common/distributions.py#L104 ) Independent action components ( common/distributions.py#L238-L246 ) Separate MLP networks for policy and value functions ( common/policies.py#L160 , baselines/common/models.py#L75-L103 Handling of action clipping to valid range and storage ( common/cmd_util.py#L99-L100 ) Normalization of Observation ( common/vec_env/vec_normalize.py#L4 ) Observation Clipping ( common/vec_env/vec_normalize.py#L39 ) Reward Scaling ( common/vec_env/vec_normalize.py#L28 ) Reward Clipping ( common/vec_env/vec_normalize.py#L32 )","title":"Implementation details"},{"location":"rl-algorithms/ppo/#experiment-results_2","text":"To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: Below are the average episodic returns for ppo_continuous_action.py . To ensure the quality of the implementation, we compared the results against openai/baselies ' PPO. Environment ppo_continuous_action.py openai/baselies ' PPO (Huang et al., 2022) 1 Hopper-v2 2231.12 \u00b1 656.72 2518.95 \u00b1 850.46 Walker2d-v2 3050.09 \u00b1 1136.21 3208.08 \u00b1 1264.37 HalfCheetah-v2 1822.82 \u00b1 928.11 2152.26 \u00b1 1159.84 Learning curves: Tracked experiments and game play videos:","title":"Experiment results"},{"location":"rl-algorithms/ppo/#video-tutorial_2","text":"If you'd like to learn ppo_continuous_action.py in-depth, consider checking out the following video tutorial:","title":"Video tutorial"},{"location":"rl-algorithms/ppo/#ppo_atari_lstmpy","text":"The ppo_atari_lstm.py has the following features: For Atari games using LSTM without stacked frames. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space","title":"ppo_atari_lstm.py"},{"location":"rl-algorithms/ppo/#usage_3","text":"poetry install -E atari python cleanrl/ppo_atari_lstm.py --help python cleanrl/ppo_atari_lstm.py --env-id BreakoutNoFrameskip-v4","title":"Usage"},{"location":"rl-algorithms/ppo/#explanation-of-the-logged-metrics_3","text":"See related docs for ppo.py .","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/ppo/#implementation-details_3","text":"ppo_atari_lstm.py is based on the \"5 LSTM implementation details\" in The 37 Implementation Details of Proximal Policy Optimization , which are as follows: Layer initialization for LSTM layers ( a2c/utils.py#L84-L86 ) Initialize the LSTM states to be zeros ( common/models.py#L179 ) Reset LSTM states at the end of the episode ( common/models.py#L141 ) Prepare sequential rollouts in mini-batches ( a2c/utils.py#L81 ) Reconstruct LSTM states during training ( a2c/utils.py#L81 ) To help test out the memory, we remove the 4 stacked frames from the observation (i.e., using env = gym.wrappers.FrameStack(env, 1) instead of env = gym.wrappers.FrameStack(env, 4) like in ppo_atari.py )","title":"Implementation details"},{"location":"rl-algorithms/ppo/#experiment-results_3","text":"To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: Below are the average episodic returns for ppo_atari_lstm.py . To ensure the quality of the implementation, we compared the results against openai/baselies ' PPO. Environment ppo_atari_lstm.py openai/baselies ' PPO (Huang et al., 2022) 1 BreakoutNoFrameskip-v4 128.92 \u00b1 31.10 138.98 \u00b1 50.76 PongNoFrameskip-v4 19.78 \u00b1 1.58 19.79 \u00b1 0.67 BeamRiderNoFrameskip-v4 1536.20 \u00b1 612.21 1591.68 \u00b1 372.95 Learning curves: Tracked experiments and game play videos:","title":"Experiment results"},{"location":"rl-algorithms/ppo/#ppo_atari_envpoolpy","text":"The ppo_atari_envpool.py has the following features: Uses the blazing fast Envpool vectorized environment. For Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space Warning Note that ppo_atari_envpool.py does not work in Windows and MacOs . See envpool's built wheels here: https://pypi.org/project/envpool/#files","title":"ppo_atari_envpool.py"},{"location":"rl-algorithms/ppo/#usage_4","text":"poetry install -E envpool python cleanrl/ppo_atari_envpool.py --help python cleanrl/ppo_atari_envpool.py --env-id Breakout-v5","title":"Usage"},{"location":"rl-algorithms/ppo/#explanation-of-the-logged-metrics_4","text":"See related docs for ppo.py .","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/ppo/#implementation-details_4","text":"ppo_atari_envpool.py uses a customized RecordEpisodeStatistics to work with envpool but has the same other implementation details as ppo_atari.py (see related docs ).","title":"Implementation details"},{"location":"rl-algorithms/ppo/#experiment-results_4","text":"To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: Below are the average episodic returns for ppo_atari_envpool.py . Notice it has the same sample efficiency as ppo_atari.py , but runs about 3x faster. Environment ppo_atari_envpool.py (~80 mins) ppo_atari.py (~220 mins) BreakoutNoFrameskip-v4 389.57 \u00b1 29.62 416.31 \u00b1 43.92 PongNoFrameskip-v4 20.55 \u00b1 0.37 20.59 \u00b1 0.35 BeamRiderNoFrameskip-v4 2039.83 \u00b1 1146.62 2445.38 \u00b1 528.91 Learning curves: Tracked experiments and game play videos:","title":"Experiment results"},{"location":"rl-algorithms/ppo/#ppo_procgenpy","text":"The ppo_procgen.py has the following features: For the procgen environments Uses IMPALA-style neural network Works with the Discrete action space","title":"ppo_procgen.py"},{"location":"rl-algorithms/ppo/#usage_5","text":"poetry install -E procgen python cleanrl/ppo_procgen.py --help python cleanrl/ppo_procgen.py --env-id starpilot","title":"Usage"},{"location":"rl-algorithms/ppo/#explanation-of-the-logged-metrics_5","text":"See related docs for ppo.py .","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/ppo/#implementation-details_5","text":"ppo_procgen.py is based on the details in \"Appendix\" in The 37 Implementation Details of Proximal Policy Optimization , which are as follows: IMPALA-style Neural Network ( common/models.py#L28 ) Use the same gamma parameter in the NormalizeReward wrapper. Note that the original implementation from openai/train-procgen uses the default gamma=0.99 in the VecNormalize wrapper but gamma=0.999 as PPO's parameter. The mismatch between the gamma s is technically incorrect. See #209","title":"Implementation details"},{"location":"rl-algorithms/ppo/#experiment-results_5","text":"To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: We try to match the default setting in openai/train-procgen except that we use the easy distribution mode and total_timesteps=25e6 to save compute. Notice openai/train-procgen has the following settings: Learning rate annealing is turned off by default Reward scaling and reward clipping is used Below are the average episodic returns for ppo_procgen.py . To ensure the quality of the implementation, we compared the results against openai/baselies ' PPO. Environment ppo_procgen.py openai/baselies ' PPO (Huang et al., 2022) 1 StarPilot (easy) 32.47 \u00b1 11.21 33.97 \u00b1 7.86 BossFight (easy) 9.63 \u00b1 2.35 9.35 \u00b1 2.04 BigFish (easy) 16.80 \u00b1 9.49 20.06 \u00b1 5.34 Info Note that we have run the procgen experiments using the easy distribution for reducing the computational cost. Learning curves: Tracked experiments and game play videos:","title":"Experiment results"},{"location":"rl-algorithms/ppo/#ppo_atari_multigpupy","text":"The ppo_atari_multigpu.py leverages data parallelism to speed up training time at no cost of sample efficiency . ppo_atari_multigpu.py has the following features: Allows the users to use do training leveraging data parallelism For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space Warning Note that ppo_atari_multigpu.py does not work in Windows and MacOs . It will error out with NOTE: Redirects are currently not supported in Windows or MacOs. See pytorch/pytorch#20380","title":"ppo_atari_multigpu.py"},{"location":"rl-algorithms/ppo/#usage_6","text":"poetry install -E atari python cleanrl/ppo_atari_multigpu.py --help # `--nproc_per_node=2` specifies how many subprocesses we spawn for training with data parallelism # note it is possible to run this with a *single GPU*: each process will simply share the same GPU torchrun --standalone --nnodes = 1 --nproc_per_node = 2 cleanrl/ppo_atari_multigpu.py --env-id BreakoutNoFrameskip-v4 # by default we use the `gloo` backend, but you can use the `nccl` backend for better multi-GPU performance torchrun --standalone --nnodes = 1 --nproc_per_node = 2 cleanrl/ppo_atari_multigpu.py --env-id BreakoutNoFrameskip-v4 --backend nccl # it is possible to spawn more processes than the amount of GPUs you have via `--device-ids` # e.g., the command below spawns two processes using GPU 0 and two processes using GPU 1 torchrun --standalone --nnodes = 1 --nproc_per_node = 2 cleanrl/ppo_atari_multigpu.py --env-id BreakoutNoFrameskip-v4 --device-ids 0 0 1 1","title":"Usage"},{"location":"rl-algorithms/ppo/#explanation-of-the-logged-metrics_6","text":"See related docs for ppo.py .","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/ppo/#implementation-details_6","text":"ppo_atari_multigpu.py is based on ppo_atari.py (see its related docs ). We use Pytorch's distributed API to implement the data parallelism paradigm. The basic idea is that the user can spawn \\(N\\) processes each holding a copy of the model, step the environments, and averages their gradients together for the backward pass. Here are a few note-worthy implementation details. Shard the environments : by default, ppo_atari_multigpu.py uses --num-envs=8 . When calling torchrun --standalone --nnodes=1 --nproc_per_node=2 cleanrl/ppo_atari_multigpu.py --env-id BreakoutNoFrameskip-v4 , it spawns \\(N=2\\) (by --nproc_per_node=2 ) subprocesses and shard the environments across these 2 subprocesses. In particular, each subprocess will have 8/2=4 environments. Implementation wise, we do args.num_envs = int(args.num_envs / world_size) . Here world_size=2 refers to the size of the world , which means the group of subprocesses. We also need to adjust various variables as follows: batch size : by default it is (num_envs * num_steps) = 8 * 128 = 1024 and we adjust it to (num_envs / world_size * num_steps) = (4 * 128) = 512 . minibatch size : by default it is (num_envs * num_steps) / num_minibatches = (8 * 128) / 4 = 256 and we adjust it to (num_envs / world_size * num_steps) / num_minibatches = (4 * 128) / 4 = 128 . number of updates : by default it is total_timesteps // batch_size = 10000000 // (8 * 128) = 9765 and we adjust it to total_timesteps // (batch_size * world_size) = 10000000 // (8 * 128 * 2) = 4882 . global step increment : by default it is num_envs and we adjust it to num_envs * world_size . Adjust seed per process : we need be very careful with seeding: we could have used the exact same seed for each subprocess. To ensure this does not happen, we do the following # CRUCIAL: note that we needed to pass a different seed for each data parallelism worker args . seed += local_rank random . seed ( args . seed ) np . random . seed ( args . seed ) torch . manual_seed ( args . seed - local_rank ) torch . backends . cudnn . deterministic = args . torch_deterministic # ... envs = gym . vector . SyncVectorEnv ( [ make_env ( args . env_id , args . seed + i , i , args . capture_video , run_name ) for i in range ( args . num_envs )] ) assert isinstance ( envs . single_action_space , gym . spaces . Discrete ), \"only discrete action space is supported\" agent = Agent ( envs ) . to ( device ) torch . manual_seed ( args . seed ) optimizer = optim . Adam ( agent . parameters (), lr = args . learning_rate , eps = 1e-5 ) Notice that we adjust the seed with args.seed += local_rank (line 2), where local_rank is the index of the subprocesses. This ensures we seed packages and envs with uncorrealted seeds. However, we do need to use the same torch seed for all process to initialize same weights for the agent (line 5), after which we can use a different seed for torch (line 16). 1. Efficient gradient averaging : PyTorch recommends to average the gradient across the whole world via the following (see docs ) for param in agent . parameters (): dist . all_reduce ( param . grad . data , op = dist . ReduceOp . SUM ) param . grad . data /= world_size However, @cswinter introduces a more efficient gradient averaging scheme with proper batching (see entity-neural-network/incubator#220 ), which looks like: all_grads_list = [] for param in agent . parameters (): if param . grad is not None : all_grads_list . append ( param . grad . view ( - 1 )) all_grads = torch . cat ( all_grads_list ) dist . all_reduce ( all_grads , op = dist . ReduceOp . SUM ) offset = 0 for param in agent . parameters (): if param . grad is not None : param . grad . data . copy_ ( all_grads [ offset : offset + param . numel ()] . view_as ( param . grad . data ) / world_size ) offset += param . numel () In our previous empirical testing (see vwxyzjn/cleanrl#162 ), we have found @cswinter 's implementation to be faster, hence we adopt it in our implementation. We can see how ppo_atari_multigpu.py can result in no loss of sample efficiency. In this example, the ppo_atari.py 's minibatch size is 256 and the ppo_atari_multigpu.py 's minibatch size is 128 with world size 2. Because we average gradient across the world, the gradient under ppo_atari_multigpu.py should be virtually the same as the gradient under ppo_atari.py .","title":"Implementation details"},{"location":"rl-algorithms/ppo/#experiment-results_6","text":"To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: Below are the average episodic returns for ppo_atari_multigpu.py . To ensure no loss of sample efficiency, we compared the results against ppo_atari.py . Environment ppo_atari_multigpu.py (in ~160 mins) ppo_atari.py (in ~215 mins) BreakoutNoFrameskip-v4 429.06 \u00b1 52.09 416.31 \u00b1 43.92 PongNoFrameskip-v4 20.40 \u00b1 0.46 20.59 \u00b1 0.35 BeamRiderNoFrameskip-v4 2454.54 \u00b1 740.49 2445.38 \u00b1 528.91 Learning curves: Under the same hardware, we see that ppo_atari_multigpu.py is about 30% faster than ppo_atari.py with no loss of sample efficiency. Info Although ppo_atari_multigpu.py is 30% faster than ppo_atari.py , ppo_atari_multigpu.py is still slower than ppo_atari_envpool.py , as shown below. This comparison really highlights the different kinds of optimization possible. The purpose of ppo_atari_multigpu.py is not (yet) to achieve the fastest PPO + Atari example. Rather, its purpose is to rigorously validate data paralleism does provide performance benefits . We could do something like ppo_atari_multigpu_envpool.py to possibly obtain the fastest PPO + Atari possible, but that is for another day. Note we may need numba to pin the threads envpool is using in each subprocess to avoid threads fighting each other and lowering the throughput. Tracked experiments and game play videos:","title":"Experiment results"},{"location":"rl-algorithms/ppo/#ppo_pettingzoo_ma_ataripy","text":"ppo_pettingzoo_ma_atari.py trains an agent to learn playing Atari games via selfplay. The selfplay environment is implemented as a vectorized environment from PettingZoo.ml . The basic idea is to create vectorized environment \\(E\\) with num_envs = N , where \\(N\\) is the number of players in the game. Say \\(N = 2\\) , then the 0-th sub environment of \\(E\\) will return the observation for player 0 and 1-th sub environment will return the observation of player 1. Then the two environments takes a batch of 2 actions and execute them for player 0 and player 1, respectively. See \"Vectorized architecture\" in The 37 Implementation Details of Proximal Policy Optimization for more detail. ppo_pettingzoo_ma_atari.py has the following features: For playing the pettingzoo's multi-agent Atari game. Works with the pixel-based observation space Works with the Box action space Warning Note that ppo_pettingzoo_ma_atari.py does not work in Windows . See https://pypi.org/project/multi-agent-ale-py/#files","title":"ppo_pettingzoo_ma_atari.py"},{"location":"rl-algorithms/ppo/#usage_7","text":"poetry install -E \"pettingzoo atari\" poetry run AutoROM --accept-license python cleanrl/ppo_pettingzoo_ma_atari.py --help python cleanrl/ppo_pettingzoo_ma_atari.py --env-id pong_v3 python cleanrl/ppo_pettingzoo_ma_atari.py --env-id surround_v2 See https://www.pettingzoo.ml/atari for a full-list of supported environments such as basketball_pong_v3 . Notice pettingzoo sometimes introduces breaking changes, so make sure to install the pinned dependencies via poetry .","title":"Usage"},{"location":"rl-algorithms/ppo/#explanation-of-the-logged-metrics_7","text":"Additionally, it logs the following metrics charts/episodic_return-player0 : episodic return of the game for player 0 charts/episodic_return-player1 : episodic return of the game for player 1 charts/episodic_length-player0 : episodic length of the game for player 0 charts/episodic_length-player1 : episodic length of the game for player 1 See other logged metrics in the related docs for ppo.py .","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/ppo/#implementation-details_7","text":"ppo_pettingzoo_ma_atari.py is based on ppo_atari.py (see its related docs ). ppo_pettingzoo_ma_atari.py additionally has the following implementation details: supersuit wrappers : uses preprocessing wrappers from supersuit instead of from stable_baselines3 , which looks like the following. In particular note that the supersuit does not offer a wrapper similar to NoopResetEnv , and that it uses the agent_indicator_v0 to add two channels indicating the which player the agent controls. -env = gym.make(env_id) -env = NoopResetEnv(env, noop_max=30) -env = MaxAndSkipEnv(env, skip=4) -env = EpisodicLifeEnv(env) -if \"FIRE\" in env.unwrapped.get_action_meanings(): - env = FireResetEnv(env) -env = ClipRewardEnv(env) -env = gym.wrappers.ResizeObservation(env, (84, 84)) -env = gym.wrappers.GrayScaleObservation(env) -env = gym.wrappers.FrameStack(env, 4) +env = importlib.import_module(f\"pettingzoo.atari.{args.env_id}\").parallel_env() +env = ss.max_observation_v0(env, 2) +env = ss.frame_skip_v0(env, 4) +env = ss.clip_reward_v0(env, lower_bound=-1, upper_bound=1) +env = ss.color_reduction_v0(env, mode=\"B\") +env = ss.resize_v1(env, x_size=84, y_size=84) +env = ss.frame_stack_v1(env, 4) +env = ss.agent_indicator_v0(env, type_only=False) +env = ss.pettingzoo_env_to_vec_env_v1(env) +envs = ss.concat_vec_envs_v1(env, args.num_envs // 2, num_cpus=0, base_class=\"gym\") 1. A more detailed note on the agent_indicator_v0 wrapper : let's dig deeper into how agent_indicator_v0 works. We do print(envs.reset(), envs.reset().shape) [ 0. , 0. , 0. , 236. , 1 , 0. ]], [[ 0. , 0. , 0. , 236. , 0. , 1. ], [ 0. , 0. , 0. , 236. , 0. , 1. ], [ 0. , 0. , 0. , 236. , 0. , 1. ], ... , [ 0. , 0. , 0. , 236. , 0. , 1. ], [ 0. , 0. , 0. , 236. , 0. , 1. ], [ 0. , 0. , 0. , 236. , 0. , 1. ]]]]) torch . Size ([ 16 , 84 , 84 , 6 ]) So the agent_indicator_v0 adds the last two columns, where [ 0., 0., 0., 236., 1, 0.]] means this observation is for player 0, and [ 0., 0., 0., 236., 0., 1.] is for player 1. Notice the observation still has the range of \\([0, 255]\\) but the agent indicator channel has the range of \\([0,1]\\) , so we need to be careful when dividing the observation by 255. In particular, we would only divide the first four channels by 255 and leave the agent indicator channels untouched as follows: def get_action_and_value ( self , x , action = None ): x = x . clone () x [:, :, :, [ 0 , 1 , 2 , 3 ]] /= 255.0 hidden = self . network ( x . permute (( 0 , 3 , 1 , 2 )))","title":"Implementation details"},{"location":"rl-algorithms/ppo/#experiment-results_7","text":"To run benchmark experiments, see benchmark/ppo.sh . Specifically, execute the following command: Info Note that evaluation is usually tricker in in selfplay environments. The usual episodic return is not a good indicator of the agent's performance in zero-sum games because the episodic return converges to zero. To evaluate the agent's ability, an intuitive approach is to take a look at the videos of the agents playing the game (included below), visually inspect the agent's behavior. The best scheme, however, is rating systems like Trueskill or ELO scores . However, they are more difficult to implement and are outside the scode of ppo_pettingzoo_ma_atari.py . For simplicity, we measure the episodic length instead, which in a sense measures how many \"back and forth\" the agent can create. In other words, the longer the agent can play the game, the better the agent can play. Empirically, we have found episodic length to be a good indicator of the agent's skill, especially in pong_v3 and surround_v2 . However, it is not the case for tennis_v3 and we'd need to visually inspect the agents' game play videos. Below are the average episodic length for ppo_pettingzoo_ma_atari.py . To ensure no loss of sample efficiency, we compared the results against ppo_atari.py . Environment ppo_pettingzoo_ma_atari.py pong_v3 4153.60 \u00b1 190.80 surround_v2 3055.33 \u00b1 223.68 tennis_v3 14538.02 \u00b1 7005.54 Learning curves: Tracked experiments and game play videos: Huang, Shengyi; Dossa, Rousslan Fernand Julien; Raffin, Antonin; Kanervisto, Anssi; Wang, Weixun (2022). The 37 Implementation Details of Proximal Policy Optimization. ICLR 2022 Blog Track https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/ \u21a9 \u21a9 \u21a9 \u21a9 \u21a9","title":"Experiment results"},{"location":"rl-algorithms/sac/","text":"Soft Actor-Critic (SAC) Overview The Soft Actor-Critic (SAC) algorithm extends the DDPG algorithm by 1) using a stochastic policy, which in theory can express multi-modal optimal policies. This also enables the use of 2) entropy regularization based on the stochastic policy's entropy. It serves as a built-in, state-dependent exploration heuristic for the agent, instead of relying on non-correlated noise processes as in DDPG , or TD3 Additionally, it incorporates the 3) usage of two Soft Q-network to reduce the overestimation bias issue in Q-network-based methods. Original papers: The SAC algorithm's initial proposal, and later updates and improvements can be chronologically traced through the following publications: Reinforcement Learning with Deep Energy-Based Policies Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor Composable Deep Reinforcement Learning for Robotic Manipulation Soft Actor-Critic Algorithms and Applications Reference resources: haarnoja/sac openai/spinningup pranz24/pytorch-soft-actor-critic DLR-RM/stable-baselines3 denisyarats/pytorch_sac haarnoja/softqlearning rail-berkeley/softlearning Variants Implemented Description sac_continuous_actions.py , docs For continuous action space Below is our single-file implementations of SAC: sac_continuous_action.py The sac_continuous_action.py has the following features: For continuous action space. Works with the Box observation space of low-level features. Works with the Box (continuous) action space. Numerically stable stochastic policy based on openai/spinningup and pranz24/pytorch-soft-actor-critic implementations. Supports automatic entropy coefficient \\(\\alpha\\) tuning, enabled by default. Usage poetry install # Pybullet poetry install -E pybullet ## Default python cleanrl/sac_continuous_action.py --env-id HopperBulletEnv-v0 ## Without Automatic entropy coef. tuning python cleanrl/sac_continuous_action.py --env-id HopperBulletEnv-v0 --autotune False --alpha 0 .2 Explanation of the logged metrics Running python cleanrl/ddpg_continuous_action.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : the episodic return of the game during training charts/SPS : number of steps per second losses/qf1_loss , losses/qf2_loss : for each Soft Q-value network \\(Q_{\\theta_i}\\) , \\(i \\in \\{1,2\\}\\) , this metric holds the mean squared error (MSE) between the soft Q-value estimate \\(Q_{\\theta_i}(s, a)\\) and the entropy regularized Bellman update target estimated as \\(r_t + \\gamma \\, Q_{\\theta_{i}^{'}}(s', a') + \\alpha \\, \\mathcal{H} \\big[ \\pi(a' \\vert s') \\big]\\) . More formally, the Soft Q-value loss for the \\(i\\) -th network is obtained by: \\[ J(\\theta^{Q}_{i}) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\big[ (Q_{\\theta_i}(s, a) - y)^2 \\big] \\] with the entropy regularized , Soft Bellman update target : $$ y = r(s, a) + \\gamma ({\\color{orange} \\min_{\\theta_{1,2}}Q_{\\theta_i^{'}}(s',a')} - \\alpha \\, \\text{log} \\pi( \\cdot \\vert s')) $$ where \\(a' \\sim \\pi( \\cdot \\vert s')\\) , \\(\\text{log} \\pi( \\cdot \\vert s')\\) approximates the entropy of the policy, and \\(\\mathcal{D}\\) is the replay buffer storing samples of the agent during training. Here, \\(\\min_{\\theta_{1,2}}Q_{\\theta_i^{'}}(s',a')\\) takes the minimum Soft Q-value network estimate between the two target Q-value networks \\(Q_{\\theta_1^{'}}\\) and \\(Q_{\\theta_2^{'}}\\) for the next state and action pair, so as to reduce over-estimation bias. losses/qf_loss : averages losses/qf1_loss and losses/qf2_loss for comparison with algorithms using a single Q-value network. losses/actor_loss : Given the stochastic nature of the policy in SAC, the actor (or policy) objective is formulated so as to maximize the likelihood of actions \\(a \\sim \\pi( \\cdot \\vert s)\\) that would result in high Q-value estimate \\(Q(s, a)\\) . Additionally, the policy objective encourages the policy to maintain its entropy high enough to help explore, discover, and capture multi-modal optimal policies. The policy's objective function can thus be defined as: \\[ \\text{max}_{\\phi} \\, J_{\\pi}(\\phi) = \\mathbb{E}_{s \\sim \\mathcal{D}} \\Big[ \\text{min}_{i=1,2} Q_{\\theta_i}(s, a) - \\alpha \\, \\text{log}\\pi_{\\phi}(a \\vert s) \\Big] \\] where the action is sampled using the reparameterization trick 1 : \\(a = \\mu_{\\phi}(s) + \\epsilon \\, \\sigma_{\\phi}(s)\\) with \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\) , \\(\\text{log} \\pi_{\\phi}( \\cdot \\vert s')\\) approximates the entropy of the policy, and \\(\\mathcal{D}\\) is the replay buffer storing samples of the agent during training. losses/alpha : \\(\\alpha\\) coefficient for entropy regularization of the policy. losses/alpha_loss : In the policy's objective defined above, the coefficient of the entropy bonus \\(\\alpha\\) is kept fixed all across the training. As suggested by the authors in Section 5 of the Soft Actor-Critic And Applications paper, the original purpose of augmenting the standard reward with the entropy of the policy is to encourage exploration of not well enough explored states (thus high entropy). Conversely, for states where the policy has already learned a near-optimal policy, it would be preferable to reduce the entropy bonus of the policy, so that it does not become sub-optimal due to the entropy maximization incentive . Therefore, having a fixed value for \\(\\alpha\\) does not fit this desideratum of matching the entropy bonus with the knowledge of the policy at an arbitrary state during its training. To mitigate this, the authors proposed a method to dynamically adjust \\(\\alpha\\) as the policy is trained, which is as follows: \\[ \\alpha^{*}_t = \\text{argmin}_{\\alpha_t} \\mathbb{E}_{a_t \\sim \\pi^{*}_t} \\big[ -\\alpha_t \\, \\text{log}\\pi^{*}_t(a_t \\vert s_t; \\alpha_t) - \\alpha_t \\mathcal{H} \\big], \\] where \\(\\mathcal{H}\\) represents the target entropy , the desired lower bound for the expected entropy of the policy over the trajectory distribution induced by the latter. As a heuristic for the target entropy , the authors use the dimension of the action space of the task. Implementation details CleanRL's sac_continuous_action.py implementation is based on openai/spinningup . sac_continuous_action.py uses a numerically stable estimation method for the standard deviation \\(\\sigma\\) of the policy, which squashes it into a range of reasonable values for a standard deviation: LOG_STD_MAX = 2 LOG_STD_MIN = - 5 class Actor ( nn . Module ): def __init__ ( self , env ): super ( Actor , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 256 ) self . fc2 = nn . Linear ( 256 , 256 ) self . fc_mean = nn . Linear ( 256 , np . prod ( env . single_action_space . shape )) self . fc_logstd = nn . Linear ( 256 , np . prod ( env . single_action_space . shape )) # action rescaling self . action_scale = torch . FloatTensor (( env . action_space . high - env . action_space . low ) / 2.0 ) self . action_bias = torch . FloatTensor (( env . action_space . high + env . action_space . low ) / 2.0 ) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) mean = self . fc_mean ( x ) log_std = self . fc_logstd ( x ) log_std = torch . tanh ( log_std ) log_std = LOG_STD_MIN + 0.5 * ( LOG_STD_MAX - LOG_STD_MIN ) * ( log_std + 1 ) # From SpinUp / Denis Yarats return mean , log_std def get_action ( self , x ): mean , log_std = self ( x ) std = log_std . exp () normal = torch . distributions . Normal ( mean , std ) x_t = normal . rsample () # for reparameterization trick (mean + std * N(0,1)) y_t = torch . tanh ( x_t ) action = y_t * self . action_scale + self . action_bias log_prob = normal . log_prob ( x_t ) # Enforcing Action Bound log_prob -= torch . log ( self . action_scale * ( 1 - y_t . pow ( 2 )) + 1e-6 ) log_prob = log_prob . sum ( 1 , keepdim = True ) mean = torch . tanh ( mean ) * self . action_scale + self . action_bias return action , log_prob , mean def to ( self , device ): self . action_scale = self . action_scale . to ( device ) self . action_bias = self . action_bias . to ( device ) return super ( Actor , self ) . to ( device ) Note that unlike openai/spinningup 's implementation which uses LOG_STD_MIN = -20 , CleanRL's uses LOG_STD_MIN = -5 instead. sac_continuous_action.py uses different learning rates for the policy and the Soft Q-value networks optimization. parser . add_argument ( \"--policy-lr\" , type = float , default = 3e-4 , help = \"the learning rate of the policy network optimizer\" ) parser . add_argument ( \"--q-lr\" , type = float , default = 1e-3 , help = \"the learning rate of the Q network network optimizer\" ) while openai/spinningup 's uses a single learning rate of lr=1e-3 for both components. Note that in case it is used, the automatic entropy coefficient \\(\\alpha\\) 's tuning shares the q-lr learning rate: # Automatic entropy tuning if args . autotune : target_entropy = - torch . prod ( torch . Tensor ( envs . single_action_space . shape ) . to ( device )) . item () log_alpha = torch . zeros ( 1 , requires_grad = True , device = device ) alpha = log_alpha . exp () . item () a_optimizer = optim . Adam ([ log_alpha ], lr = args . q_lr ) else : alpha = args . alpha sac_continuous_action.py uses --batch-size=256 while openai/spinningup 's uses --batch-size=100 by default. Experiment results To run benchmark experiments, see benchmark/sac.sh . Specifically, execute the following command: The table below compares the results of CleanRL's sac_continuous_action.py with the latest published results by the original authors of the SAC algorithm. Info Note that the results table above references the training episodic return for sac_continuous_action.py , the results of Soft Actor-Critic Algorithms and Applications reference evaluation episodic return obtained by running the policy in the deterministic mode. Environment sac_continuous_action.py SAC: Algorithms and Applications @ 1M steps HalfCheetah-v2 10310.37 \u00b1 1873.21 ~11,250 Walker2d-v2 4418.15 \u00b1 592.82 ~4,800 Hopper-v2 2685.76 \u00b1 762.16 ~3,250 Learning curves: Tracked experiments and game play videos: Diederik P Kingma, Max Welling (2016). Auto-Encoding Variational Bayes. ArXiv, abs/1312.6114. https://arxiv.org/abs/1312.6114 \u21a9","title":"Soft Actor-Critic (SAC)"},{"location":"rl-algorithms/sac/#soft-actor-critic-sac","text":"","title":"Soft Actor-Critic (SAC)"},{"location":"rl-algorithms/sac/#overview","text":"The Soft Actor-Critic (SAC) algorithm extends the DDPG algorithm by 1) using a stochastic policy, which in theory can express multi-modal optimal policies. This also enables the use of 2) entropy regularization based on the stochastic policy's entropy. It serves as a built-in, state-dependent exploration heuristic for the agent, instead of relying on non-correlated noise processes as in DDPG , or TD3 Additionally, it incorporates the 3) usage of two Soft Q-network to reduce the overestimation bias issue in Q-network-based methods. Original papers: The SAC algorithm's initial proposal, and later updates and improvements can be chronologically traced through the following publications: Reinforcement Learning with Deep Energy-Based Policies Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor Composable Deep Reinforcement Learning for Robotic Manipulation Soft Actor-Critic Algorithms and Applications Reference resources: haarnoja/sac openai/spinningup pranz24/pytorch-soft-actor-critic DLR-RM/stable-baselines3 denisyarats/pytorch_sac haarnoja/softqlearning rail-berkeley/softlearning Variants Implemented Description sac_continuous_actions.py , docs For continuous action space Below is our single-file implementations of SAC:","title":"Overview"},{"location":"rl-algorithms/sac/#sac_continuous_actionpy","text":"The sac_continuous_action.py has the following features: For continuous action space. Works with the Box observation space of low-level features. Works with the Box (continuous) action space. Numerically stable stochastic policy based on openai/spinningup and pranz24/pytorch-soft-actor-critic implementations. Supports automatic entropy coefficient \\(\\alpha\\) tuning, enabled by default.","title":"sac_continuous_action.py"},{"location":"rl-algorithms/sac/#usage","text":"poetry install # Pybullet poetry install -E pybullet ## Default python cleanrl/sac_continuous_action.py --env-id HopperBulletEnv-v0 ## Without Automatic entropy coef. tuning python cleanrl/sac_continuous_action.py --env-id HopperBulletEnv-v0 --autotune False --alpha 0 .2","title":"Usage"},{"location":"rl-algorithms/sac/#explanation-of-the-logged-metrics","text":"Running python cleanrl/ddpg_continuous_action.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : the episodic return of the game during training charts/SPS : number of steps per second losses/qf1_loss , losses/qf2_loss : for each Soft Q-value network \\(Q_{\\theta_i}\\) , \\(i \\in \\{1,2\\}\\) , this metric holds the mean squared error (MSE) between the soft Q-value estimate \\(Q_{\\theta_i}(s, a)\\) and the entropy regularized Bellman update target estimated as \\(r_t + \\gamma \\, Q_{\\theta_{i}^{'}}(s', a') + \\alpha \\, \\mathcal{H} \\big[ \\pi(a' \\vert s') \\big]\\) . More formally, the Soft Q-value loss for the \\(i\\) -th network is obtained by: \\[ J(\\theta^{Q}_{i}) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\big[ (Q_{\\theta_i}(s, a) - y)^2 \\big] \\] with the entropy regularized , Soft Bellman update target : $$ y = r(s, a) + \\gamma ({\\color{orange} \\min_{\\theta_{1,2}}Q_{\\theta_i^{'}}(s',a')} - \\alpha \\, \\text{log} \\pi( \\cdot \\vert s')) $$ where \\(a' \\sim \\pi( \\cdot \\vert s')\\) , \\(\\text{log} \\pi( \\cdot \\vert s')\\) approximates the entropy of the policy, and \\(\\mathcal{D}\\) is the replay buffer storing samples of the agent during training. Here, \\(\\min_{\\theta_{1,2}}Q_{\\theta_i^{'}}(s',a')\\) takes the minimum Soft Q-value network estimate between the two target Q-value networks \\(Q_{\\theta_1^{'}}\\) and \\(Q_{\\theta_2^{'}}\\) for the next state and action pair, so as to reduce over-estimation bias. losses/qf_loss : averages losses/qf1_loss and losses/qf2_loss for comparison with algorithms using a single Q-value network. losses/actor_loss : Given the stochastic nature of the policy in SAC, the actor (or policy) objective is formulated so as to maximize the likelihood of actions \\(a \\sim \\pi( \\cdot \\vert s)\\) that would result in high Q-value estimate \\(Q(s, a)\\) . Additionally, the policy objective encourages the policy to maintain its entropy high enough to help explore, discover, and capture multi-modal optimal policies. The policy's objective function can thus be defined as: \\[ \\text{max}_{\\phi} \\, J_{\\pi}(\\phi) = \\mathbb{E}_{s \\sim \\mathcal{D}} \\Big[ \\text{min}_{i=1,2} Q_{\\theta_i}(s, a) - \\alpha \\, \\text{log}\\pi_{\\phi}(a \\vert s) \\Big] \\] where the action is sampled using the reparameterization trick 1 : \\(a = \\mu_{\\phi}(s) + \\epsilon \\, \\sigma_{\\phi}(s)\\) with \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\) , \\(\\text{log} \\pi_{\\phi}( \\cdot \\vert s')\\) approximates the entropy of the policy, and \\(\\mathcal{D}\\) is the replay buffer storing samples of the agent during training. losses/alpha : \\(\\alpha\\) coefficient for entropy regularization of the policy. losses/alpha_loss : In the policy's objective defined above, the coefficient of the entropy bonus \\(\\alpha\\) is kept fixed all across the training. As suggested by the authors in Section 5 of the Soft Actor-Critic And Applications paper, the original purpose of augmenting the standard reward with the entropy of the policy is to encourage exploration of not well enough explored states (thus high entropy). Conversely, for states where the policy has already learned a near-optimal policy, it would be preferable to reduce the entropy bonus of the policy, so that it does not become sub-optimal due to the entropy maximization incentive . Therefore, having a fixed value for \\(\\alpha\\) does not fit this desideratum of matching the entropy bonus with the knowledge of the policy at an arbitrary state during its training. To mitigate this, the authors proposed a method to dynamically adjust \\(\\alpha\\) as the policy is trained, which is as follows: \\[ \\alpha^{*}_t = \\text{argmin}_{\\alpha_t} \\mathbb{E}_{a_t \\sim \\pi^{*}_t} \\big[ -\\alpha_t \\, \\text{log}\\pi^{*}_t(a_t \\vert s_t; \\alpha_t) - \\alpha_t \\mathcal{H} \\big], \\] where \\(\\mathcal{H}\\) represents the target entropy , the desired lower bound for the expected entropy of the policy over the trajectory distribution induced by the latter. As a heuristic for the target entropy , the authors use the dimension of the action space of the task.","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/sac/#implementation-details","text":"CleanRL's sac_continuous_action.py implementation is based on openai/spinningup . sac_continuous_action.py uses a numerically stable estimation method for the standard deviation \\(\\sigma\\) of the policy, which squashes it into a range of reasonable values for a standard deviation: LOG_STD_MAX = 2 LOG_STD_MIN = - 5 class Actor ( nn . Module ): def __init__ ( self , env ): super ( Actor , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 256 ) self . fc2 = nn . Linear ( 256 , 256 ) self . fc_mean = nn . Linear ( 256 , np . prod ( env . single_action_space . shape )) self . fc_logstd = nn . Linear ( 256 , np . prod ( env . single_action_space . shape )) # action rescaling self . action_scale = torch . FloatTensor (( env . action_space . high - env . action_space . low ) / 2.0 ) self . action_bias = torch . FloatTensor (( env . action_space . high + env . action_space . low ) / 2.0 ) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) mean = self . fc_mean ( x ) log_std = self . fc_logstd ( x ) log_std = torch . tanh ( log_std ) log_std = LOG_STD_MIN + 0.5 * ( LOG_STD_MAX - LOG_STD_MIN ) * ( log_std + 1 ) # From SpinUp / Denis Yarats return mean , log_std def get_action ( self , x ): mean , log_std = self ( x ) std = log_std . exp () normal = torch . distributions . Normal ( mean , std ) x_t = normal . rsample () # for reparameterization trick (mean + std * N(0,1)) y_t = torch . tanh ( x_t ) action = y_t * self . action_scale + self . action_bias log_prob = normal . log_prob ( x_t ) # Enforcing Action Bound log_prob -= torch . log ( self . action_scale * ( 1 - y_t . pow ( 2 )) + 1e-6 ) log_prob = log_prob . sum ( 1 , keepdim = True ) mean = torch . tanh ( mean ) * self . action_scale + self . action_bias return action , log_prob , mean def to ( self , device ): self . action_scale = self . action_scale . to ( device ) self . action_bias = self . action_bias . to ( device ) return super ( Actor , self ) . to ( device ) Note that unlike openai/spinningup 's implementation which uses LOG_STD_MIN = -20 , CleanRL's uses LOG_STD_MIN = -5 instead. sac_continuous_action.py uses different learning rates for the policy and the Soft Q-value networks optimization. parser . add_argument ( \"--policy-lr\" , type = float , default = 3e-4 , help = \"the learning rate of the policy network optimizer\" ) parser . add_argument ( \"--q-lr\" , type = float , default = 1e-3 , help = \"the learning rate of the Q network network optimizer\" ) while openai/spinningup 's uses a single learning rate of lr=1e-3 for both components. Note that in case it is used, the automatic entropy coefficient \\(\\alpha\\) 's tuning shares the q-lr learning rate: # Automatic entropy tuning if args . autotune : target_entropy = - torch . prod ( torch . Tensor ( envs . single_action_space . shape ) . to ( device )) . item () log_alpha = torch . zeros ( 1 , requires_grad = True , device = device ) alpha = log_alpha . exp () . item () a_optimizer = optim . Adam ([ log_alpha ], lr = args . q_lr ) else : alpha = args . alpha sac_continuous_action.py uses --batch-size=256 while openai/spinningup 's uses --batch-size=100 by default.","title":"Implementation details"},{"location":"rl-algorithms/sac/#experiment-results","text":"To run benchmark experiments, see benchmark/sac.sh . Specifically, execute the following command: The table below compares the results of CleanRL's sac_continuous_action.py with the latest published results by the original authors of the SAC algorithm. Info Note that the results table above references the training episodic return for sac_continuous_action.py , the results of Soft Actor-Critic Algorithms and Applications reference evaluation episodic return obtained by running the policy in the deterministic mode. Environment sac_continuous_action.py SAC: Algorithms and Applications @ 1M steps HalfCheetah-v2 10310.37 \u00b1 1873.21 ~11,250 Walker2d-v2 4418.15 \u00b1 592.82 ~4,800 Hopper-v2 2685.76 \u00b1 762.16 ~3,250 Learning curves: Tracked experiments and game play videos: Diederik P Kingma, Max Welling (2016). Auto-Encoding Variational Bayes. ArXiv, abs/1312.6114. https://arxiv.org/abs/1312.6114 \u21a9","title":"Experiment results"},{"location":"rl-algorithms/td3/","text":"Twin Delayed Deep Deterministic Policy Gradient (TD3) Overview TD3 is a popular DRL algorithm for continuous control. It extends DDPG with three techniques: 1) Clipped Double Q-Learning, 2) Delayed Policy Updates, and 3) Target Policy Smoothing Regularization. With these three techniques TD3 shows significantly better performance compared to DDPG. Original paper: Addressing Function Approximation Error in Actor-Critic Methods Reference resources: sfujim/TD3 Twin Delayed DDPG | Spinning Up in Deep RL Implemented Variants Variants Implemented Description td3_continuous_action.py , docs For continuous action space Below are our single-file implementations of TD3: td3_continuous_action.py The td3_continuous_action.py has the following features: For continuous action space Works with the Box observation space of low-level features Works with the Box (continuous) action space Usage poetry install poetry install -E pybullet python cleanrl/td3_continuous_action.py --help python cleanrl/td3_continuous_action.py --env-id HopperBulletEnv-v0 poetry install -E mujoco # only works in Linux python cleanrl/td3_continuous_action.py --env-id Hopper-v3 Explanation of the logged metrics Running python cleanrl/td3_continuous_action.py will automatically record various metrics such as various losses in Tensorboard. Below are the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/qf1_loss : the MSE between the Q values at timestep \\(t\\) and the target Q values at timestep \\(t+1\\) , which minimizes temporal difference. losses/actor_loss : implemented as -qf1(data.observations, actor(data.observations)).mean() ; it is the negative average Q values calculated based on the 1) observations and the 2) actions computed by the actor based on these observations. By minimizing actor_loss , the optimizer updates the actors parameter using the following gradient (Fujimoto et al., 2018, Algorithm 1) 2 : \\[ \\nabla_{\\phi} J(\\phi)=\\left.N^{-1} \\sum \\nabla_{a} Q_{\\theta_{1}}(s, a)\\right|_{a=\\pi_{\\phi}(s)} \\nabla_{\\phi} \\pi_{\\phi}(s) \\] losses/qf1_values : implemented as `qf1(data.observations, data.actions).view(-1); it is the average Q values of the sampled data in the replay buffer; useful when gauging if under or over esitmations happen Implementation details Our td3_continuous_action.py is based on the TD3.py from sfujim/TD3 . Our td3_continuous_action.py presents the following implementation differences. td3_continuous_action.py uses a two separate objects qf1 and qf2 to represents the two Q functions in the Clipped Double Q-learning architecture, whereas TD3.py (Fujimoto et al., 2018) 2 uses a single Critic class that contains both Q networks. That said, these two implementations are virtually the same. td3_continuous_action.py also adds support for handling continuous environments where the lower and higher bounds of the action space are not \\([-1,1]\\) , or are asymmetric. The case where the bounds are not \\([-1,1]\\) is handled in TD3.py (Fujimoto et al., 2018) 2 as follows: class Actor ( nn . Module ): ... def forward ( self , state ): a = F . relu ( self . l1 ( state )) a = F . relu ( self . l2 ( a )) return self . max_action * torch . tanh ( self . l3 ( a )) # Scale from [-1,1] to [-action_high, action_high] On the other hand, in CleanRL's td3_continuous_action.py , the mean and the scale of the action space are computed as action_bias and action_scale respectively. Those scalars are in turn used to scale the output of a tanh activation function in the actor to the original action space range: class Actor ( nn . Module ): def __init__ ( self , env ): ... # action rescaling self . register_buffer ( \"action_scale\" , torch . FloatTensor (( env . action_space . high - env . action_space . low ) / 2.0 )) self . register_buffer ( \"action_bias\" , torch . FloatTensor (( env . action_space . high + env . action_space . low ) / 2.0 )) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = torch . tanh ( self . fc_mu ( x )) return x * self . action_scale + self . action_bias # Scale from [-1,1] to [-action_low, action_high] Additionally, when drawing exploration noise that is added to the actions produced by the actor, CleanRL's td3_continuous_action.py centers the distribution the sampled from at action_bias , and the scale of the distribution is set to action_scale * exploration_noise . Info Note that Humanoid-v2 , InvertedPendulum-v2 , Pusher-v2 have action space bounds that are not the standard [-1, 1] . See below and PR #196 Ant-v2 Observation space: Box(-inf, inf, (111,), float64) Action space: Box(-1.0, 1.0, (8,), float32) HalfCheetah-v2 Observation space: Box(-inf, inf, (17,), float64) Action space: Box(-1.0, 1.0, (6,), float32) Hopper-v2 Observation space: Box(-inf, inf, (11,), float64) Action space: Box(-1.0, 1.0, (3,), float32) Humanoid-v2 Observation space: Box(-inf, inf, (376,), float64) Action space: Box(-0.4, 0.4, (17,), float32) InvertedDoublePendulum-v2 Observation space: Box(-inf, inf, (11,), float64) Action space: Box(-1.0, 1.0, (1,), float32) InvertedPendulum-v2 Observation space: Box(-inf, inf, (4,), float64) Action space: Box(-3.0, 3.0, (1,), float32) Pusher-v2 Observation space: Box(-inf, inf, (23,), float64) Action space: Box(-2.0, 2.0, (7,), float32) Reacher-v2 Observation space: Box(-inf, inf, (11,), float64) Action space: Box(-1.0, 1.0, (2,), float32) Swimmer-v2 Observation space: Box(-inf, inf, (8,), float64) Action space: Box(-1.0, 1.0, (2,), float32) Walker2d-v2 Observation space: Box(-inf, inf, (17,), float64) Action space: Box(-1.0, 1.0, (6,), float32) Experiment results To run benchmark experiments, see benchmark/td3.sh . Specifically, execute the following command: Below are the average episodic returns for td3_continuous_action.py (3 random seeds). To ensure the quality of the implementation, we compared the results against (Fujimoto et al., 2018) 2 . Environment td3_continuous_action.py TD3.py (Fujimoto et al., 2018, Table 1) 2 HalfCheetah 9018.31 \u00b1 1078.31 9636.95 \u00b1 859.065 Walker2d 4246.07 \u00b1 1210.84 4682.82 \u00b1 539.64 Hopper 3391.78 \u00b1 232.21 3564.07 \u00b1 114.74 Humanoid 4822.64 \u00b1 321.85 not available Pusher -42.24 \u00b1 6.74 not available InvertedPendulum 964.59 \u00b1 43.91 1000.00 \u00b1 0.00 Info Note that td3_continuous_action.py uses gym MuJoCo v2 environments while TD3.py (Fujimoto et al., 2018) 2 uses the gym MuJoCo v1 environments. According to the openai/gym#834 , gym MuJoCo v2 environments should be equivalent to the gym MuJoCo v1 environments. Also note the performance of our td3_continuous_action.py seems to be worse than the reference implementation on Walker2d. This is likely due to openai/gym#938 . We would have a hard time reproducing gym MuJoCo v1 environments because they have been long deprecated. One other thing could cause the performance difference: the original code reported the average episodic return using determinisitc evaluation (i.e., without exploration noise), see sfujim/TD3/main.py#L15-L32 , whereas we reported the episodic return during training and the policy gets updated between environments steps. Learning curves: Tracked experiments and game play videos: Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N.M., Erez, T., Tassa, Y., Silver, D., & Wierstra, D. (2016). Continuous control with deep reinforcement learning. CoRR, abs/1509.02971. https://arxiv.org/abs/1509.02971 \u21a9 Fujimoto, S., Hoof, H.V., & Meger, D. (2018). Addressing Function Approximation Error in Actor-Critic Methods. ArXiv, abs/1802.09477. https://arxiv.org/abs/1802.09477 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9","title":"Twin Delayed Deep Deterministic Policy Gradient (TD3)"},{"location":"rl-algorithms/td3/#twin-delayed-deep-deterministic-policy-gradient-td3","text":"","title":"Twin Delayed Deep Deterministic Policy Gradient (TD3)"},{"location":"rl-algorithms/td3/#overview","text":"TD3 is a popular DRL algorithm for continuous control. It extends DDPG with three techniques: 1) Clipped Double Q-Learning, 2) Delayed Policy Updates, and 3) Target Policy Smoothing Regularization. With these three techniques TD3 shows significantly better performance compared to DDPG. Original paper: Addressing Function Approximation Error in Actor-Critic Methods Reference resources: sfujim/TD3 Twin Delayed DDPG | Spinning Up in Deep RL","title":"Overview"},{"location":"rl-algorithms/td3/#implemented-variants","text":"Variants Implemented Description td3_continuous_action.py , docs For continuous action space Below are our single-file implementations of TD3:","title":"Implemented Variants"},{"location":"rl-algorithms/td3/#td3_continuous_actionpy","text":"The td3_continuous_action.py has the following features: For continuous action space Works with the Box observation space of low-level features Works with the Box (continuous) action space","title":"td3_continuous_action.py"},{"location":"rl-algorithms/td3/#usage","text":"poetry install poetry install -E pybullet python cleanrl/td3_continuous_action.py --help python cleanrl/td3_continuous_action.py --env-id HopperBulletEnv-v0 poetry install -E mujoco # only works in Linux python cleanrl/td3_continuous_action.py --env-id Hopper-v3","title":"Usage"},{"location":"rl-algorithms/td3/#explanation-of-the-logged-metrics","text":"Running python cleanrl/td3_continuous_action.py will automatically record various metrics such as various losses in Tensorboard. Below are the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/qf1_loss : the MSE between the Q values at timestep \\(t\\) and the target Q values at timestep \\(t+1\\) , which minimizes temporal difference. losses/actor_loss : implemented as -qf1(data.observations, actor(data.observations)).mean() ; it is the negative average Q values calculated based on the 1) observations and the 2) actions computed by the actor based on these observations. By minimizing actor_loss , the optimizer updates the actors parameter using the following gradient (Fujimoto et al., 2018, Algorithm 1) 2 : \\[ \\nabla_{\\phi} J(\\phi)=\\left.N^{-1} \\sum \\nabla_{a} Q_{\\theta_{1}}(s, a)\\right|_{a=\\pi_{\\phi}(s)} \\nabla_{\\phi} \\pi_{\\phi}(s) \\] losses/qf1_values : implemented as `qf1(data.observations, data.actions).view(-1); it is the average Q values of the sampled data in the replay buffer; useful when gauging if under or over esitmations happen","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/td3/#implementation-details","text":"Our td3_continuous_action.py is based on the TD3.py from sfujim/TD3 . Our td3_continuous_action.py presents the following implementation differences. td3_continuous_action.py uses a two separate objects qf1 and qf2 to represents the two Q functions in the Clipped Double Q-learning architecture, whereas TD3.py (Fujimoto et al., 2018) 2 uses a single Critic class that contains both Q networks. That said, these two implementations are virtually the same. td3_continuous_action.py also adds support for handling continuous environments where the lower and higher bounds of the action space are not \\([-1,1]\\) , or are asymmetric. The case where the bounds are not \\([-1,1]\\) is handled in TD3.py (Fujimoto et al., 2018) 2 as follows: class Actor ( nn . Module ): ... def forward ( self , state ): a = F . relu ( self . l1 ( state )) a = F . relu ( self . l2 ( a )) return self . max_action * torch . tanh ( self . l3 ( a )) # Scale from [-1,1] to [-action_high, action_high] On the other hand, in CleanRL's td3_continuous_action.py , the mean and the scale of the action space are computed as action_bias and action_scale respectively. Those scalars are in turn used to scale the output of a tanh activation function in the actor to the original action space range: class Actor ( nn . Module ): def __init__ ( self , env ): ... # action rescaling self . register_buffer ( \"action_scale\" , torch . FloatTensor (( env . action_space . high - env . action_space . low ) / 2.0 )) self . register_buffer ( \"action_bias\" , torch . FloatTensor (( env . action_space . high + env . action_space . low ) / 2.0 )) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = torch . tanh ( self . fc_mu ( x )) return x * self . action_scale + self . action_bias # Scale from [-1,1] to [-action_low, action_high] Additionally, when drawing exploration noise that is added to the actions produced by the actor, CleanRL's td3_continuous_action.py centers the distribution the sampled from at action_bias , and the scale of the distribution is set to action_scale * exploration_noise . Info Note that Humanoid-v2 , InvertedPendulum-v2 , Pusher-v2 have action space bounds that are not the standard [-1, 1] . See below and PR #196 Ant-v2 Observation space: Box(-inf, inf, (111,), float64) Action space: Box(-1.0, 1.0, (8,), float32) HalfCheetah-v2 Observation space: Box(-inf, inf, (17,), float64) Action space: Box(-1.0, 1.0, (6,), float32) Hopper-v2 Observation space: Box(-inf, inf, (11,), float64) Action space: Box(-1.0, 1.0, (3,), float32) Humanoid-v2 Observation space: Box(-inf, inf, (376,), float64) Action space: Box(-0.4, 0.4, (17,), float32) InvertedDoublePendulum-v2 Observation space: Box(-inf, inf, (11,), float64) Action space: Box(-1.0, 1.0, (1,), float32) InvertedPendulum-v2 Observation space: Box(-inf, inf, (4,), float64) Action space: Box(-3.0, 3.0, (1,), float32) Pusher-v2 Observation space: Box(-inf, inf, (23,), float64) Action space: Box(-2.0, 2.0, (7,), float32) Reacher-v2 Observation space: Box(-inf, inf, (11,), float64) Action space: Box(-1.0, 1.0, (2,), float32) Swimmer-v2 Observation space: Box(-inf, inf, (8,), float64) Action space: Box(-1.0, 1.0, (2,), float32) Walker2d-v2 Observation space: Box(-inf, inf, (17,), float64) Action space: Box(-1.0, 1.0, (6,), float32)","title":"Implementation details"},{"location":"rl-algorithms/td3/#experiment-results","text":"To run benchmark experiments, see benchmark/td3.sh . Specifically, execute the following command: Below are the average episodic returns for td3_continuous_action.py (3 random seeds). To ensure the quality of the implementation, we compared the results against (Fujimoto et al., 2018) 2 . Environment td3_continuous_action.py TD3.py (Fujimoto et al., 2018, Table 1) 2 HalfCheetah 9018.31 \u00b1 1078.31 9636.95 \u00b1 859.065 Walker2d 4246.07 \u00b1 1210.84 4682.82 \u00b1 539.64 Hopper 3391.78 \u00b1 232.21 3564.07 \u00b1 114.74 Humanoid 4822.64 \u00b1 321.85 not available Pusher -42.24 \u00b1 6.74 not available InvertedPendulum 964.59 \u00b1 43.91 1000.00 \u00b1 0.00 Info Note that td3_continuous_action.py uses gym MuJoCo v2 environments while TD3.py (Fujimoto et al., 2018) 2 uses the gym MuJoCo v1 environments. According to the openai/gym#834 , gym MuJoCo v2 environments should be equivalent to the gym MuJoCo v1 environments. Also note the performance of our td3_continuous_action.py seems to be worse than the reference implementation on Walker2d. This is likely due to openai/gym#938 . We would have a hard time reproducing gym MuJoCo v1 environments because they have been long deprecated. One other thing could cause the performance difference: the original code reported the average episodic return using determinisitc evaluation (i.e., without exploration noise), see sfujim/TD3/main.py#L15-L32 , whereas we reported the episodic return during training and the policy gets updated between environments steps. Learning curves: Tracked experiments and game play videos: Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N.M., Erez, T., Tassa, Y., Silver, D., & Wierstra, D. (2016). Continuous control with deep reinforcement learning. CoRR, abs/1509.02971. https://arxiv.org/abs/1509.02971 \u21a9 Fujimoto, S., Hoof, H.V., & Meger, D. (2018). Addressing Function Approximation Error in Actor-Critic Methods. ArXiv, abs/1802.09477. https://arxiv.org/abs/1802.09477 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9","title":"Experiment results"}]}